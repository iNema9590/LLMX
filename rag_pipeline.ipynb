{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import concurrent.futures\n",
    "import json\n",
    "\n",
    "# API endpoint\n",
    "URL = \"http://localhost:11434/api/generate\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Different queries to test load balancing\n",
    "PROMPTS = [\n",
    "    \"What is Reinforcement Learning?\",\n",
    "    \"Explain the concept of Transformers in AI.\",\n",
    "    \"How does backpropagation work in neural networks?\",\n",
    "    \"What are the differences between supervised and unsupervised learning?\",\n",
    "    # \"Explain the importance of attention mechanisms in deep learning.\",\n",
    "    # \"What is gradient descent and why is it used?\",\n",
    "    # \"How does transfer learning work in machine learning?\",\n",
    "    # \"What are large language models and how are they trained?\",\n",
    "    # \"Explain the concept of embeddings in NLP.\",\n",
    "    # \"Describe the role of GPUs in deep learning.\"\n",
    "]\n",
    "\n",
    "def send_request(prompt):\n",
    "    \"\"\"Send a request to the Ollama API with a given prompt.\"\"\"\n",
    "    data = {\n",
    "        \"model\": \"llama3.3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(URL, headers=HEADERS, json=data)\n",
    "    return { \"prompt\": prompt, \"response\": response.json() }\n",
    "\n",
    "# Run parallel requests\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#     results = list(executor.map(send_request, PROMPTS))\n",
    "\n",
    "# # Print results\n",
    "# for i, res in enumerate(results):\n",
    "#     print(f\"\\nQuery {i+1}: {res['prompt']}\")\n",
    "#     print(f\"Response: {res['response']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        \"model\": \"llama2\",\n",
    "        \"prompt\": \"who got the first nobel proce in physics?\",\n",
    "        \"stream\": False\n",
    "    }\n",
    "response = requests.post(URL, json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Röntgen for his discovery of X-rays. Röntgen was a German physicist and engineer who discovered X-rays in 1895 while working at the University of Würzburg. He was awarded the Nobel Prize in Physics in 1901 \"in recognition of the services he has rendered by his discovery of the Ray which enables X-ray photographs to be taken.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "# Allow nested asyncio loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def query_ollama(prompt):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\"model\": \"llama3.3\", \"prompt\": prompt}\n",
    "        )\n",
    "        return response.json()\n",
    "\n",
    "async def main():\n",
    "    prompts = [\n",
    "        \"Explain quantum computing.\",\n",
    "        \"What is reinforcement learning?\",\n",
    "        \"Tell me a joke about AI.\",\n",
    "    ]\n",
    "    \n",
    "    tasks = [query_ollama(prompt) for prompt in prompts]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for i, res in enumerate(responses):\n",
    "        print(f\"Response {i}: {res}\")\n",
    "\n",
    "# Run asyncio directly\n",
    "await main()  # Instead of asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4  \n",
    "response = generate_response(query, documents)\n",
    "\n",
    "# Display the response\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4 \n",
    "response = generate_response(query, documents)\n",
    "# Display the response\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute exact Shapley values with parallel LLM calls\n",
    "shapley_values = compute_exact_shapley_values(query, documents, num_workers=num_workers)\n",
    "\n",
    "# Display the Shapley values\n",
    "for i, value in enumerate(shapley_values):\n",
    "    print(f\"Document {i+1} Shapley Value: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Shapley values\n",
    "visualize_shapley_values(shapley_values, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"http://localhost:15000/api/generate\"\n",
    "data = {\n",
    "    \"model\": \"llama3.3\",\n",
    "    \"prompt\": \"How about wrong data?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data).json()\n",
    "print(response[\"response\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST http://localhost:11434/api/generate      -H \"Content-Type: application/json\"      -d '{ \"model\": \"llama3.3\", \"prompt\": \"Explain the concept of Reinforcement Learning.\",\"stream\": true}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
