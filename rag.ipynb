{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs.aau.dk/em63by/anaconda3/envs/llmx/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import faiss\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/recipes.csv')\n",
    "features_drop = ['recipe_url', 'input_db_index', 'food_kg_locator', 'food_com_unique_id', 'submit_date', 'last_changed_date', 'author_id', 'rating', 'recipe_id', 'serves', 'units' ]\n",
    "data.drop(features_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"directions\"].iloc[1].strip('{'\"\"'}').replace('\",\"', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredients_list(ingredients:str) -> list: \n",
    "    \"\"\"Clean the ingredients string to get a list. Each element of a list is a specific ingredient with the quantity\n",
    "\n",
    "    Args:\n",
    "        ingredients (str): string of ingredients\n",
    "\n",
    "    Returns:\n",
    "        list: list of ingredients (each elem in the lsit is an ingredient.)\n",
    "    \"\"\"a\n",
    "    sample_list = ingredients.split(')')\n",
    "    clean_sample_list = ''\n",
    "    pattern = r'[^A-Za-z0-9/ -]'\n",
    "    for elem in sample_list : \n",
    "        cleaned_elem = re.sub(pattern, ' ', elem)\n",
    "        cleaned_elem = re.sub(r'\\s+', ' ', cleaned_elem).strip(' ')\n",
    "        if len(cleaned_elem) > 1: \n",
    "            clean_sample_list=clean_sample_list+\" \"+cleaned_elem\n",
    "\n",
    "    return clean_sample_list\n",
    "\n",
    "def get_directions_list(directions:str): \n",
    "    \"\"\"Clean the directions string to get a list of directions\n",
    "\n",
    "    Args:\n",
    "        directions (str): badly formated directions string\n",
    "\n",
    "    Returns:\n",
    "        list: contains the different directions to follow for a given recipe. \n",
    "    \"\"\"\n",
    "    return directions.strip('{'\"\"'}').replace('\",\"', ' ')\n",
    "\n",
    "data['new_ingredients'] = ''\n",
    "data['new_directions'] = ''\n",
    "data.new_ingredients = data.ingredients.apply(lambda x : get_ingredients_list(x))\n",
    "data.new_directions = data.directions.apply(lambda x : get_directions_list(x))\n",
    "data.drop(['ingredients', 'directions'], axis = 1, inplace = True)\n",
    "data.rename(columns = {'new_ingredients': 'ingredients', 'new_directions' : 'directions'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] =data[\"title\"] +\". Ingredients:\" + data[\"ingredients\"] + \". Instructions:\" +data[\"directions\"]\n",
    "documents = data[\"text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# embeddings = embedding_model.encode(documents[:1000], show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507335"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved.\n"
     ]
    }
   ],
   "source": [
    "### 1️⃣ Load Data and Create FAISS Index ###\n",
    "with open(\"data/documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "with open(\"data/embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "embeddings = normalize_embeddings(embeddings)\n",
    "\n",
    "def index_documents(method=\"faiss\", index_name=\"recipes\", es_host=\"http://localhost:9200\"):\n",
    "    if method == \"faiss\":\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, \"data/recipe_faiss.index\")\n",
    "        print(\"FAISS index saved.\")\n",
    "        return index\n",
    "    elif method == \"elasticsearch\":\n",
    "        es = Elasticsearch(es_host)\n",
    "        mapping = {\"mappings\": {\"properties\": {\"text\": {\"type\": \"text\"}, \"vector\": {\"type\": \"dense_vector\", \"dims\": embeddings.shape[1]}}}}\n",
    "        es.indices.create(index=index_name, body=mapping, ignore=400)\n",
    "        for i, (text, vector) in enumerate(zip(documents, embeddings)):\n",
    "            es.index(index=index_name, id=i, body={\"text\": text, \"vector\": vector.tolist()})\n",
    "        print(\"Elasticsearch index created.\")\n",
    "        return es\n",
    "\n",
    "# Choose indexing method\n",
    "\n",
    "### 2️⃣ Retrieval Function ###\n",
    "faiss_index = faiss.read_index(\"data/recipe_faiss.index\")\n",
    "\n",
    "# Load the same embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve_documents(query, k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = normalize_embeddings(query_embedding.reshape(1, -1))\n",
    "    scores, indices = faiss_index.search(query_embedding, k)\n",
    "    return [documents[i] for i in indices[0]], scores\n",
    "\n",
    "### 3️⃣ Query RAG Pipeline ###\n",
    "def query_rag(query, retrieved_docs, k):\n",
    "    if k:\n",
    "        retrieved_text = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        prompt = f\"Using only the following list of recipes, answer the question about a recipe. \\nList of recipes:{retrieved_text} \\n If you can not find a recipe from the documents provided, then just answer -I do not have this recipe. Do not skip the details in the instruction.\\n Question: {query}. Answer:\"\n",
    "    else:\n",
    "        prompt= f'You are a foodchat bot who gives recipes. Given the query provide a recipe. Query: {query}. Answer:'\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\"model\": \"llama3.3\", \"prompt\": prompt}\n",
    "    \n",
    "    response = requests.post(url, json=data, stream=True)\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                json_data = json.loads(line.decode(\"utf-8\"))\n",
    "                full_response += json_data.get(\"response\", \"\")\n",
    "                \n",
    "                # If done, exit early\n",
    "                if json_data.get(\"done\", False):\n",
    "                    break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON Decode Error:\", e)\n",
    "                continue\n",
    "\n",
    "    if k:\n",
    "        return full_response\n",
    "    else:\n",
    "        return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Give a recipe with beef or lamb which takes minimal effort\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp, docs, scores=query_rag(query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6258053183555603\tLamb (Or Beef) Entree. Ingredients: 1 lb lamb stew meat or beef 3 roma tomatoes 4 ounces portabella mushrooms 14 ounces quartered artichokes 2 garlic cloves chopped 2 tablespoons olive oil 6 ounces fe\n",
      "0.6486079692840576\tEasy Meat Patties. Ingredients: 1 lb ground lamb 1 small onion finely chopped 3 green chilies finely chopped 1 teaspoon gingerroot grated 6 -7 garlic cloves crushed 1/4 cup coriander leaves chopped 1/\n",
      "0.6753540635108948\tOld-Fashioned Irish Stew. Ingredients: 3 lbs lamb necks slices fat trimmed and reserved 1-inch thick 4 medium onions thinly sliced 1 medium onion chopped 1 lb medium carrot peeled halved crosswise and\n",
      "0.6849710941314697\tLamb Stew. Ingredients: 2 lbs lamb shoulder cut into cubes 2 teaspoons flour 3 teaspoons shortening salt and pepper 1 cup chopped onion hot water 3 carrots cut into 1 inch chunks 6 potatoes cut into 1\n",
      "0.6867395639419556\tLamb Stew. Ingredients: 6 medium peeled potatoes cut in chunks 4 carrots peeled and cut in chunks 2 large onions cut in chunks 1 lb lean lamb stew meat 1 teaspoon salt pepper 1/4 teaspoon rosemary 1/2\n",
      "0.687942624092102\tBraised Lamb (Shoulder) With Garlic and Cinnamon. Ingredients: 1 tablespoon extra virgin olive oil 2 lbs lamb preferably from the shoulder cut into 1-to-2-inch chunks salt and pepper 5 -6 garlic clove\n",
      "0.692775547504425\tLamb Burgers. Ingredients: 1/4 cup minced fresh cilantro 3 tablespoons crumbled feta cheese 2 teaspoons minced red onions 1/4 teaspoon salt 1/4 teaspoon ground coriander 1/4 teaspoon ground red pepper\n",
      "0.693062961101532\tLamb and Green Bean Ragout. Ingredients: 2 tablespoons olive oil 1 lb lamb stew meat cut into 1/2 cubes 1 large onion chopped 1 lb fresh green beans quartered 2 15 ounce cans Hunts tomato sauce 1/2 te\n",
      "0.6944193243980408\tEconomical Baked Lamb With Rice. Ingredients: 500 g lean lamb I buy a leg and get the butcher to take out the bone 1 medium carrot 1 stalk celery or 1 tsp dried celery 4 stalks parsley broad-leaf only\n",
      "0.694670557975769\tLamb Stew. Ingredients: 2 lbs lamb cubed 1/2 teaspoon sugar 2 tablespoons oil 2 teaspoons salt 1/4 teaspoon pepper 1/4 cup flour 2 cups water 3/4 cup red wine 1/4 teaspoon garlic powder 2 teaspoons Wo\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(scores[0], docs):\n",
    "    print(f\"{a}\\t{b[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided recipes, I found one that requires minimal effort and uses lamb. Here is the recipe:\n",
      "\n",
      "**Lamb Burgers**\n",
      "\n",
      "Ingredients:\n",
      "- 1/4 cup minced fresh cilantro\n",
      "- 3 tablespoons crumbled feta cheese\n",
      "- 2 teaspoons minced red onions\n",
      "- 1/4 teaspoon salt\n",
      "- 1/4 teaspoon ground coriander\n",
      "- 1/4 teaspoon ground red pepper\n",
      "- 1/4 teaspoon black pepper\n",
      "- 1 lb lean ground lamb\n",
      "\n",
      "Instructions:\n",
      "1. Prepare broiler OR grill pan.\n",
      "2. Combine all 8 ingredients.\n",
      "3. Divide lamb mixture into 4 equal portions, shaping each into a 3/4-inch thick patty.\n",
      "4. Place patties on broiler or grillpan coated with cooking spray.\n",
      "5. Cook 4 minutes on each side or until done.\n",
      "6. Serve it on a bun with your favorites (tomato, lettuce).\n",
      "\n",
      "This recipe requires minimal effort as it involves only mixing the ingredients, shaping the patties, and cooking them. It's a simple and quick recipe that can be prepared in about 20-30 minutes.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_shapley_values(retrieved_docs):\n",
    "    \"\"\"\n",
    "    Compute exact Shapley values for retrieved documents based on cosine similarity.\n",
    "    \"\"\"\n",
    "    n = len(retrieved_docs)\n",
    "    shapley_values = np.zeros(n)\n",
    "    \n",
    "    # Generate full response\n",
    "    full_response = query_rag(query, k=n)\n",
    "    full_embedding = normalize_embeddings(model.encode(full_response).reshape(1, -1))\n",
    "    \n",
    "\n",
    "    # Iterate over all subsets\n",
    "    for subset in itertools.chain.from_iterable(itertools.combinations(range(n), r) for r in range(n)):\n",
    "        if not subset:\n",
    "            continue\n",
    "        \n",
    "        subset_docs = [retrieved_docs[i] for i in subset]\n",
    "        subset_response = query_rag(\"\\n\".join(subset_docs))\n",
    "        subset_embedding = model.encode(subset_response).reshape(1, -1)\n",
    "        \n",
    "        for i in subset:\n",
    "            subset_minus_i = [retrieved_docs[j] for j in subset if j != i]\n",
    "            if not subset_minus_i:\n",
    "                continue\n",
    "            subset_minus_i_response = query_rag(\"\\n\".join(subset_minus_i))\n",
    "            subset_minus_i_embedding = model.encode(subset_minus_i_response).reshape(1, -1)\n",
    "            \n",
    "            # Compute cosine similarities\n",
    "            cos_full_subset = cosine_similarity(full_embedding, subset_embedding)[0, 0]\n",
    "            cos_full_subset_minus_i = cosine_similarity(full_embedding, subset_minus_i_embedding)[0, 0]\n",
    "            \n",
    "            # Marginal contribution\n",
    "            marginal_contrib = cos_full_subset - cos_full_subset_minus_i\n",
    "            shapley_values[i] += marginal_contrib / len(subset)\n",
    "    \n",
    "    # Normalize by number of permutations\n",
    "    shapley_values /= n\n",
    "    \n",
    "    return shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m shv\u001b[38;5;241m=\u001b[39m\u001b[43mcompute_shapley_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[128], line 16\u001b[0m, in \u001b[0;36mcompute_shapley_values\u001b[0;34m(retrieved_docs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Generate full response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m full_response \u001b[38;5;241m=\u001b[39m query_rag(query, k\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m---> 16\u001b[0m full_embedding \u001b[38;5;241m=\u001b[39m normalize_embeddings(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_response\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Iterate over all subsets\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(itertools\u001b[38;5;241m.\u001b[39mcombinations(\u001b[38;5;28mrange\u001b[39m(n), r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m~/anaconda3/envs/llmx/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:591\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    590\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m--> 591\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmx/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1056\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts.\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m            \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmx/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:495\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m    494\u001b[0m         batch1\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 495\u001b[0m         batch2\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    496\u001b[0m     to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# strip\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "shv=compute_shapley_values(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
