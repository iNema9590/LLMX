{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs.aau.dk/em63by/anaconda3/envs/llmx/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import faiss\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/recipes.csv')\n",
    "features_drop = ['recipe_url', 'input_db_index', 'food_kg_locator', 'food_com_unique_id', 'submit_date', 'last_changed_date', 'author_id', 'rating', 'recipe_id', 'serves', 'units' ]\n",
    "data.drop(features_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"directions\"].iloc[1].strip('{'\"\"'}').replace('\",\"', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredients_list(ingredients:str) -> list: \n",
    "    \"\"\"Clean the ingredients string to get a list. Each element of a list is a specific ingredient with the quantity\n",
    "\n",
    "    Args:\n",
    "        ingredients (str): string of ingredients\n",
    "\n",
    "    Returns:\n",
    "        list: list of ingredients (each elem in the lsit is an ingredient.)\n",
    "    \"\"\"\n",
    "    sample_list = ingredients.split(')')\n",
    "    clean_sample_list = ''\n",
    "    pattern = r'[^A-Za-z0-9/ -]'\n",
    "    for elem in sample_list : \n",
    "        cleaned_elem = re.sub(pattern, ' ', elem)\n",
    "        cleaned_elem = re.sub(r'\\s+', ' ', cleaned_elem).strip(' ')\n",
    "        if len(cleaned_elem) > 1: \n",
    "            clean_sample_list=clean_sample_list+\" \"+cleaned_elem\n",
    "\n",
    "    return clean_sample_list\n",
    "\n",
    "def get_directions_list(directions:str): \n",
    "    \"\"\"Clean the directions string to get a list of directions\n",
    "\n",
    "    Args:\n",
    "        directions (str): badly formated directions string\n",
    "\n",
    "    Returns:\n",
    "        list: contains the different directions to follow for a given recipe. \n",
    "    \"\"\"\n",
    "    return directions.strip('{'\"\"'}').replace('\",\"', ' ')\n",
    "\n",
    "data['new_ingredients'] = ''\n",
    "data['new_directions'] = ''\n",
    "data.new_ingredients = data.ingredients.apply(lambda x : get_ingredients_list(x))\n",
    "data.new_directions = data.directions.apply(lambda x : get_directions_list(x))\n",
    "data.drop(['ingredients', 'directions'], axis = 1, inplace = True)\n",
    "data.rename(columns = {'new_ingredients': 'ingredients', 'new_directions' : 'directions'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] =data[\"title\"] +\". Ingredients:\" + data[\"ingredients\"] + \". Instructions:\" +data[\"directions\"]\n",
    "documents = data[\"text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# embeddings = embedding_model.encode(documents[:1000], show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1️⃣ Load Data and Create FAISS Index ###\n",
    "with open(\"data/documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "with open(\"data/embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "\n",
    "def normalize_embeddings(embeddings):\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "embeddings = normalize_embeddings(embeddings)\n",
    "\n",
    "def index_documents(method=\"faiss\", index_name=\"recipes\", es_host=\"http://localhost:9200\"):\n",
    "    if method == \"faiss\":\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, \"data/recipe_faiss.index\")\n",
    "        print(\"FAISS index saved.\")\n",
    "        return index\n",
    "    elif method == \"elasticsearch\":\n",
    "        es = Elasticsearch(es_host)\n",
    "        mapping = {\"mappings\": {\"properties\": {\"text\": {\"type\": \"text\"}, \"vector\": {\"type\": \"dense_vector\", \"dims\": embeddings.shape[1]}}}}\n",
    "        es.indices.create(index=index_name, body=mapping, ignore=400)\n",
    "        for i, (text, vector) in enumerate(zip(documents, embeddings)):\n",
    "            es.index(index=index_name, id=i, body={\"text\": text, \"vector\": vector.tolist()})\n",
    "        print(\"Elasticsearch index created.\")\n",
    "        return es\n",
    "\n",
    "# Choose indexing method\n",
    "\n",
    "### 2️⃣ Retrieval Function ###\n",
    "faiss_index = faiss.read_index(\"data/recipe_faiss.index\")\n",
    "\n",
    "# Load the same embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve_documents(query, k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = normalize_embeddings(query_embedding.reshape(1, -1))\n",
    "    scores, indices = faiss_index.search(query_embedding, k)\n",
    "    return [documents[i] for i in indices[0]], scores\n",
    "\n",
    "### 3️⃣ Query RAG Pipeline ###\n",
    "def query_rag(query, retrieved_docs=None):\n",
    "    if retrieved_docs:\n",
    "        retrieved_text = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        prompt = f\"Using only the following list of recipes, answer the question about a recipe. \\nList of recipes:{retrieved_text} \\n If you can not find a recipe from the documents provided, then just answer -I do not have this recipe. Do not skip the details in the instruction.\\n Question: {query}. Answer:\"\n",
    "    else:\n",
    "        prompt= f'You are a foodchat bot who gives recipes. Given the query provide a recipe. Query: {query}. Answer:'\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\"model\": \"llama3.3\", \"prompt\": prompt}\n",
    "    \n",
    "    response = requests.post(url, json=data, stream=True)\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                json_data = json.loads(line.decode(\"utf-8\"))\n",
    "                full_response += json_data.get(\"response\", \"\")\n",
    "                \n",
    "                # If done, exit early\n",
    "                if json_data.get(\"done\", False):\n",
    "                    break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"JSON Decode Error:\", e)\n",
    "                continue\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"I have some chicken and vegetables, what can i cook which is not that hard?\"\n",
    "docs, scores=retrieve_documents(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5773535966873169\tVeggie-Chicken Toss With Lemon Herb Sauce. Ingredients: 2 chicken breasts salt and pepper 1 teaspoon dried rosemary 1 teaspoon dried basil 1/3 cup chopped onion 1 chopped garlic clove 1 red potatoes 3\n",
      "0.5913329720497131\tSuper Easy Chicken. Ingredients: 4 pieces chicken drumsticks actually any 6 piece of chicken works 2 onions sliced 4 potatoes cut into 4 2 red peppers sliced 2 green peppers sliced 2 tablespoons olive\n",
      "0.6129103302955627\tChicken With Chickpeas and Olives. Ingredients: 1 roasting chicken divided into 8 pieces salt and pepper oil for frying 2 green peppers cut into small cubes 2 small onions chopped 1 small hot pepper c\n",
      "0.6155734062194824\tGolden Chicken & Autumn Vegetables. Ingredients: 4 boneless skinless chicken breasts 1 15 ounce can Swanson chicken broth 1 tablespoon chopped fresh parsley I used dried 1/2 teaspoon garlic powder 1/2\n",
      "0.619018018245697\tChicken and Vegetable Stir -Fry. Ingredients: 2 tablespoons sesame oil 1 lb boneless skinless chicken breast cut into thin strips 1 red bell pepper cleaned and cut into strips 1 cup sliced fresh mushr\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(scores[0], docs):\n",
    "    print(f\"{a}\\t{b[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=query_rag(query, retrieved_docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can try cooking \"Super Easy Chicken\". This recipe seems to be quite simple and requires minimal effort. All you need to do is place the chicken and your desired vegetables (except tomatoes) in a shallow saucepan, add some olive oil, salt, and pepper, and cook on low heat until everything is cooked through. The instructions are straightforward and don't require any complicated techniques or ingredient preparations. Give it a try!\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_shapley_values(retrieved_docs):\n",
    "    \"\"\"\n",
    "    Compute exact Shapley values for retrieved documents based on cosine similarity.\n",
    "    \"\"\"\n",
    "    n = len(retrieved_docs)\n",
    "    shapley_values = np.zeros(n)\n",
    "    \n",
    "    # Generate full response\n",
    "    full_response = query_rag(query, retrieved_docs=retrieved_docs)\n",
    "    full_embedding = normalize_embeddings(model.encode(full_response).reshape(1, -1))\n",
    "    \n",
    "\n",
    "    # Iterate over all subsets\n",
    "    for subset in itertools.chain.from_iterable(itertools.combinations(range(n), r) for r in range(n)):\n",
    "        if not subset:\n",
    "            continue\n",
    "        \n",
    "        subset_docs = [retrieved_docs[i] for i in subset]\n",
    "        subset_response = query_rag(subset_docs)\n",
    "        subset_embedding = model.encode(subset_response).reshape(1, -1)\n",
    "        \n",
    "        for i in subset:\n",
    "            subset_minus_i = [retrieved_docs[j] for j in subset if j != i]\n",
    "            if not subset_minus_i:\n",
    "                continue\n",
    "            subset_minus_i_response = query_rag(subset_minus_i)\n",
    "            subset_minus_i_embedding = model.encode(subset_minus_i_response).reshape(1, -1)\n",
    "            \n",
    "            # Compute cosine similarities\n",
    "            cos_full_subset = cosine_similarity(full_embedding, subset_embedding)[0, 0]\n",
    "            cos_full_subset_minus_i = cosine_similarity(full_embedding, subset_minus_i_embedding)[0, 0]\n",
    "            \n",
    "            # Marginal contribution\n",
    "            marginal_contrib = cos_full_subset - cos_full_subset_minus_i\n",
    "            shapley_values[i] += marginal_contrib\n",
    "    \n",
    "    \n",
    "    return shapley_values/pow(2, (n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def compute_shapley_values_parallel(retrieved_docs):\n",
    "    \"\"\"\n",
    "    Compute exact Shapley values for retrieved documents using cosine similarity.\n",
    "    Parallelized using ThreadPoolExecutor to optimize API requests to Ollama.\n",
    "    \"\"\"\n",
    "    n = len(retrieved_docs)\n",
    "    shapley_values = np.zeros(n)\n",
    "\n",
    "    # Generate full response\n",
    "    full_response = query_rag(query, retrieved_docs=retrieved_docs)\n",
    "    full_embedding = normalize_embeddings(model.encode(full_response).reshape(1, -1))\n",
    "\n",
    "    # Function to process a subset\n",
    "    def process_subset(subset):\n",
    "        if not subset:\n",
    "            return []\n",
    "\n",
    "        subset_docs = [retrieved_docs[i] for i in subset]\n",
    "        subset_response = query_rag(query, retrieved_docs=subset_docs)\n",
    "        subset_embedding = model.encode(subset_response).reshape(1, -1)\n",
    "\n",
    "        results = []\n",
    "        for i in subset:\n",
    "            subset_minus_i = [retrieved_docs[j] for j in subset if j != i]\n",
    "            if not subset_minus_i:\n",
    "                subset_minus_i_response = query_rag(query)\n",
    "            else:\n",
    "                subset_minus_i_response = query_rag(query, retrieved_docs=subset_minus_i)\n",
    "            subset_minus_i_embedding = model.encode(subset_minus_i_response).reshape(1, -1)\n",
    "\n",
    "            # Compute cosine similarities\n",
    "            cos_full_subset = cosine_similarity(full_embedding, subset_embedding)[0, 0]\n",
    "            cos_full_subset_minus_i = cosine_similarity(full_embedding, subset_minus_i_embedding)[0, 0]\n",
    "\n",
    "            # Marginal contribution\n",
    "            marginal_contrib = cos_full_subset - cos_full_subset_minus_i\n",
    "            results.append((i, marginal_contrib))\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Use ThreadPoolExecutor to process subsets in parallel\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        subsets = itertools.chain.from_iterable(itertools.combinations(range(n), r) for r in range(n+1))\n",
    "        futures = [executor.submit(process_subset, subset) for subset in subsets]\n",
    "\n",
    "        # Collect results\n",
    "        for future in futures:\n",
    "            for i, contrib in future.result():\n",
    "                shapley_values[i] += contrib\n",
    "\n",
    "    return shapley_values / pow(2, (n-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "shv=compute_shapley_values_parallel(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0018289 ,  0.23371889, -0.00851012,  0.0081025 , -0.04131911])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "shv=compute_shapley_values(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07867983, 0.00879009, 0.00241612])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
