{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/globalsc/users/e/k/ekuzmenk/LLMX/shaprag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ulb/code_wit/ekuzmenk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)\n",
    "df= pd.read_csv(\"../scripts/nq_2_positives.csv\",index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/globalsc/users/e/k/ekuzmenk/LLMX/shaprag/lib/python3.11/site-packages/accelerate/accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = []\n",
    "for i in range(len(df.question)):\n",
    "    n = 0\n",
    "    docs=ast.literal_eval(df.context[i])\n",
    "    doc_sents = []\n",
    "    for j in range(len(docs)):\n",
    "        sents = nltk.sent_tokenize(docs[j])\n",
    "        new_sents = []\n",
    "        for s in range(len(sents)):\n",
    "            #new_sents.append(str(n + s) + '-' + str(j) + '-' + sents[s])\n",
    "            new_sents.append(sents[s])\n",
    "        n += len(sents)\n",
    "        doc_sents.append(new_sents)\n",
    "    flat_doc_sents = [\n",
    "    x\n",
    "    for xs in doc_sents\n",
    "    for x in xs\n",
    "]\n",
    "    all_sents.append(flat_doc_sents)\n",
    "df['Sentences'] = all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on death row in the United States on January 1, 2013.',\n",
       " 'Since 1977, the states of Texas (464), Virginia (108) and Oklahoma (94) have executed the most death row inmates.',\n",
       " ', California (683), Florida (390), Texas (330) and Pennsylvania (218) housed more than half of all inmates pending on death row.',\n",
       " ', the longest-serving prisoner on death row in the US who has been executed was Jack Alderman who served over 33 years.',\n",
       " 'He was executed in Georgia in 2008.',\n",
       " 'However, Alderman only holds the distinction of being the longest-serving \"executed\" inmate so far.',\n",
       " 'A Florida inmate, Gary Alvord, arrived',\n",
       " 'punishable by death penalty.',\n",
       " 'But perhaps the best indicator that this law is not a deterrent to criminality is the ever-increasing number of death convicts.',\n",
       " 'From 1994 to 1995 the number of persons on death row increased from 12 to 104.',\n",
       " 'From 1995 to 1996 it increased to 182.',\n",
       " 'In 1997 the total death convicts was at 520 and in 1998 the inmates in death row was at 781.',\n",
       " 'As of November 1999 there are a total of 956 death convicts at the National Bilibid Prisons and at the Correctional Institute for Women.',\n",
       " 'As of December 31, 1999, based on',\n",
       " 'Gospodor Monument Park Gospodor Monument Park is a roadside attraction along Interstate 5 near Toledo, Washington, in the United States.',\n",
       " 'It features four sculptures, collectively known as the Gospodor monuments, created in 2002.',\n",
       " 'The tallest, standing at more than 100 feet, commemorates Mother Teresa and features a gold painted wooden statue of Jesus.',\n",
       " 'Next to this monument is one featuring Mother Teresa herself.',\n",
       " 'Another honors victims of The Holocaust and features an eternal electric flame.',\n",
       " 'The park also includes a 100-foot tall monument depicting Chief Seattle to commemorate Native Americans and other indigenous tribes.',\n",
       " \"After their installation in 2002, the'\",\n",
       " \"Attorney General of India The Attorney General for India is the Indian government's chief legal advisor, and is primary lawyer in the Supreme Court of India.\",\n",
       " \"He can be said to be the lawyer from government's side.\",\n",
       " 'He is appointed by the President of India under Article 76(1) of the Constitution and holds office during the pleasure of the President.',\n",
       " 'He must be a person qualified to be appointed as a Judge of the Supreme Court (He must have been a judge of some high court for five years or an advocate of some high court for ten years or\"',\n",
       " 'The Supreme Court is presided over by the Chief Justice of Nigeria and thirteen associate justices, who are appointed by the President of Nigeria on the recommendation of the National Judicial Council.',\n",
       " 'These justices are subject to confirmation by the Senate.',\n",
       " 'Nigeria is made up of 36 states and 1 territory.',\n",
       " \"They are: the Federal Capital Territory, Abia, Adamawa, Akwa Ibom, Anambra, Bauchi, Bayelsa, Benue, Borno, Cross River, Delta, Ebonyi, Edo, Ekiti, Enugu, Gombe, Imo, Jigawa, Kaduna, Kano, Katsina, Kebbi, Kogi, Kwara, Lagos, Nasarawa, Niger, Ogun, Ondo, Osun, Oyo, Plateau, Rivers, Sokoto, Taraba, Yobe, and Zamfara Each state is further'\",\n",
       " 'an eminent jurist, in the opinion of the President and must be a citizen of India.)',\n",
       " 'The 15th and current Attorney General is K. K. Venugopal.',\n",
       " 'He was appointed by Pranab Mukherjee, the President of India at that time.',\n",
       " 'He was formally appointed as with effect from 30 June 2017 and shall have a tenure of 3 years.',\n",
       " 'The Attorney General is necessary for giving advice to the Government of India in legal matters referred to him.',\n",
       " 'He also performs other legal duties assigned to him by the President.',\n",
       " \"The Attorney General has the right of audience in all Courts'\",\n",
       " 'White House The White House is the official residence and workplace of the President of the United States.',\n",
       " 'It is located at 1600 Pennsylvania Avenue NW in Washington, D.C. and has been the residence of every U.S. President since John Adams in 1800.',\n",
       " 'The term, \"White House\", is often used as a metonym for the president and his advisers.',\n",
       " 'The residence was designed by Irish-born architect James Hoban in the neoclassical style.',\n",
       " 'Hoban modelled the building on Leinster House in Dublin, a building which today houses the Oireachtas, the Irish legislature.',\n",
       " \"Construction took place between 1792 and 1800 using Aquia'\",\n",
       " 'the OAR (Olympic Athletes from Russia) designation.',\n",
       " 'The official sanctions imposed by the IOC included: the exclusion of Russian government officials from the Games; the use of the Olympic flag and Olympic Anthem in place of the Russian flag and anthem; and the submission of a replacement logo for the OAR uniforms.',\n",
       " 'By early January 2018, the IOC had banned 43 Russian athletes from competing in the 2018 Winter Olympics and all future Olympic Games (as part of the Oswald Commission).',\n",
       " \"Of those athletes, 42 appealed against their bans to the Court of Arbitration for Sport (CAS) and 28 of'\",\n",
       " 'record is 73, set by Barry Bonds in 2001.',\n",
       " 'Other notable single season records were achieved by Babe Ruth who hit 60 in 1927, Roger Maris, with 61 home runs in 1961, and Mark McGwire, who hit 70 in 1998.',\n",
       " 'Negro League slugger Josh Gibson\\\\\\'s Baseball Hall of Fame plaque says he hit \"almost 800\" home runs in his career.',\n",
       " 'The \"Guinness Book of World Records\" lists Gibson\\\\\\'s lifetime home run total at 800.',\n",
       " 'Ken Burns\\\\\\' award-winning series, \"Baseball\", states that his actual total may have been as high as 950.',\n",
       " \"Gibson\\\\'s true total is not known, in part due'\",\n",
       " 'resigned from the post of Additional Solicitor General as a result.',\n",
       " \"The Attorneys General for India since independence are listed below: Attorney General of India The Attorney General for India is the Indian government's chief legal advisor, and is primary lawyer in the Supreme Court of India.\",\n",
       " \"He can be said to be the lawyer from government's side.\",\n",
       " 'He is appointed by the President of India under Article 76(1) of the Constitution and holds office during the pleasure of the President.',\n",
       " 'He must be a person qualified to be appointed as a Judge of the Supreme Court (He must have\"']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1/100: total number of death row inmates in the us... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx0.pkl...\n",
      "Successfully loaded 1034 cached utilities.\n",
      "Response: As of November 1999, there were 956 death convicts at the National Bilibid Prisons and at the Correctional Institute for Women.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                       | 1/100 [00:50<1:23:35, 50.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 2/100: big little lies season 2 how many episodes... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx1.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                       | 2/100 [01:41<1:22:46, 50.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 3/100: who sang waiting for a girl like you... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx2.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Foreigner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                      | 3/100 [02:32<1:22:27, 51.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 4/100: where do you cross the arctic circle in norway... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx3.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Saltfjellet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                      | 4/100 [03:23<1:21:17, 50.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 5/100: who is the main character in green eggs and ham... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx4.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: The main character in Green Eggs and Ham is a strange creature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                      | 5/100 [04:16<1:21:43, 51.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 6/100: do veins carry blood to the heart or away... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx5.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Veins carry blood toward the heart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                     | 6/100 [05:06<1:20:07, 51.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 7/100: who played charlie bucket in the original charlie and the ch... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx6.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Peter Ostrum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                     | 7/100 [06:01<1:20:57, 52.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 8/100: what is 1 radian in terms of pi... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx7.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: 1 radian is equal to π/180.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                    | 8/100 [06:53<1:20:08, 52.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 9/100: when does season 5 of bates motel come out... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx8.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: September 19, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▌                                    | 9/100 [07:46<1:19:49, 52.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 10/100: how many episodes are in series 7 game of thrones... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx9.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▉                                   | 10/100 [08:38<1:18:39, 52.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 11/100: who is next in line to be the monarch of england... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx10.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Prince William, Duke of Cambridge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▎                                  | 11/100 [09:31<1:17:52, 52.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 12/100: who is in charge of enforcing the pendleton act of 1883... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx11.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: The Civil Service Commission.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▋                                  | 12/100 [10:21<1:15:47, 51.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 13/100: what is the name of latest version of android... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx12.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Android Pie.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████                                  | 13/100 [11:14<1:15:43, 52.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 14/100: why was there so much interest in cuba both before and after... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx13.pkl...\n",
      "Successfully loaded 756 cached utilities.\n",
      "Response: Historians have debated America's intentions in Cuba, with some initially believing it was due to humanitarian interest in the Cuban people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▍                                 | 14/100 [12:08<1:15:28, 52.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 15/100: when did veterans day start being called veterans day... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx14.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: May 26, 1954.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▊                                 | 15/100 [13:01<1:14:47, 52.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 16/100: when did big air snowboarding become an olympic sport... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx15.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: 2018.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▏                                | 16/100 [13:52<1:13:05, 52.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 17/100: who played in the most world series games... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx16.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: The New York Yankees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▋                                | 17/100 [14:45<1:12:45, 52.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 18/100: who sings i can't stop this feeling anymore... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx17.pkl...\n",
      "Successfully loaded 848 cached utilities.\n",
      "Response: Justin Timberlake.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████                                | 18/100 [15:37<1:11:35, 52.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 19/100: who is the month of may named after... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx18.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: The month of May is named after the goddess Maia, a Greek and Roman goddess of fertility.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▍                               | 19/100 [16:30<1:11:00, 52.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 20/100: who has the most petroleum in the world... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx19.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Venezuela.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▊                               | 20/100 [17:22<1:09:38, 52.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 21/100: who is the sister of for king and country... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx20.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Rebecca St. James.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▏                              | 21/100 [18:15<1:09:12, 52.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 22/100: who developed the first periodic table with 8 columns... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx21.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Gilbert N. Lewis and Irving Langmuir.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▌                              | 22/100 [19:08<1:08:37, 52.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 23/100: who plays skyler on lab rats elite force... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx22.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Paris Berelc plays Skylar Storm on Lab Rats: Elite Force.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████▉                              | 23/100 [20:02<1:07:55, 52.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 24/100: when is season seven of game of thrones coming out... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx23.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: July 16, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▎                             | 24/100 [20:54<1:06:46, 52.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 25/100: who went home on rupaul's drag race season 10 episode 4... ---\n",
      "Loading existing utility cache from ../Experiment_data/NQ_sents/utilities_q_idx24.pkl...\n",
      "Successfully loaded 847 cached utilities.\n",
      "Response: Dusty Ray Bottoms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▎                             | 24/100 [21:16<1:07:22, 53.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m         actual_samples = num_s\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m actual_samples > \u001b[32m0\u001b[39m: \n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         results_for_query[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mContextCite\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m], model_cc = \u001b[43mharness\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_contextcite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactual_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m#results_for_query[f\"FM_Shap{actual_samples}\"], _, F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\", k=4)\u001b[39;00m\n\u001b[32m     53\u001b[39m         \u001b[38;5;66;03m#results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=16, beta_b=1, max_unique_lookups=actual_samples, seed=SEED)\u001b[39;00m\n\u001b[32m     54\u001b[39m         \u001b[38;5;66;03m#results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[38;5;66;03m#                                    utility_type=\"divergence\"\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m#                                )\u001b[39;00m\n\u001b[32m     70\u001b[39m LDS = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/globalsc/users/e/k/ekuzmenk/LLMX/SHapRAG/rag_shap.py:313\u001b[39m, in \u001b[36mContextAttribution.compute_contextcite\u001b[39m\u001b[34m(self, num_samples, seed)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Compute utilities on-demand for the sampled subsets\u001b[39;00m\n\u001b[32m    312\u001b[39m pbar = tqdm(sampled_tuples, desc=\u001b[33m\"\u001b[39m\u001b[33mComputing utilities for ContextCite\u001b[39m\u001b[33m\"\u001b[39m, disable=\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m utilities_for_samples = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_utility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_tuple\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv_tuple\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# Filter out any samples where utility computation failed\u001b[39;00m\n\u001b[32m    316\u001b[39m valid_indices = [i \u001b[38;5;28;01mfor\u001b[39;00m i, u \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(utilities_for_samples) \u001b[38;5;28;01mif\u001b[39;00m u != -\u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/globalsc/users/e/k/ekuzmenk/LLMX/SHapRAG/rag_shap.py:313\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Compute utilities on-demand for the sampled subsets\u001b[39;00m\n\u001b[32m    312\u001b[39m pbar = tqdm(sampled_tuples, desc=\u001b[33m\"\u001b[39m\u001b[33mComputing utilities for ContextCite\u001b[39m\u001b[33m\"\u001b[39m, disable=\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m utilities_for_samples = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_utility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_tuple\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v_tuple \u001b[38;5;129;01min\u001b[39;00m pbar]\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# Filter out any samples where utility computation failed\u001b[39;00m\n\u001b[32m    316\u001b[39m valid_indices = [i \u001b[38;5;28;01mfor\u001b[39;00m i, u \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(utilities_for_samples) \u001b[38;5;28;01mif\u001b[39;00m u != -\u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/globalsc/users/e/k/ekuzmenk/LLMX/SHapRAG/rag_shap.py:143\u001b[39m, in \u001b[36mContextAttribution.get_utility\u001b[39m\u001b[34m(self, subset_tuple)\u001b[39m\n\u001b[32m    141\u001b[39m     v_np = np.array(subset_tuple)\n\u001b[32m    142\u001b[39m     context_str = \u001b[38;5;28mself\u001b[39m._get_ablated_context_from_vector(v_np)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     utility = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_llm_compute_logprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m.utility_cache[subset_tuple] = utility\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m utility\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/globalsc/users/e/k/ekuzmenk/LLMX/SHapRAG/rag_shap.py:237\u001b[39m, in \u001b[36mContextAttribution._llm_compute_logprob\u001b[39m\u001b[34m(self, context_str, response)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m log_probs_tokens_with, log_probs_tokens_empty, answer_log_probs_with, answer_log_probs_empty\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m total_log_prob_with, total_log_prob_empty, prob_with, prob_empty, logit_with, logit_empty\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logit_gain_total.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/globalsc/users/e/k/ekuzmenk/LLMX/shaprag/lib/python3.11/site-packages/torch/cuda/memory.py:222\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# SENTENCE LEVEL\n",
    "num_questions_to_run = 100\n",
    "k_values = [1, 2]\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "LDSs=[]\n",
    "r2s = []\n",
    "RMSEs = []\n",
    "\n",
    "for i in tqdm(range(num_questions_to_run), disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    #docs=df.paragraphs[i]\n",
    "    #docs=ast.literal_eval(df.context[i])\n",
    "    #docs = [sent[4:] for sent in df.Sentences[i]][:20]\n",
    "    docs = [sent for sent in df.Sentences[i]]\n",
    "\n",
    "    utility_cache_base_dir = \"../Experiment_data/NQ_sents\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "\n",
    "    print(f'Response: {harness.target_response}')\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "    if accelerator_main.is_main_process:\n",
    "        m_samples_map = {\"L\": 128, \"XL\": 256}\n",
    "        T_iterations_map = {\"L\":40} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0: \n",
    "                results_for_query[f\"ContextCite{actual_samples}\"], model_cc = harness.compute_contextcite(num_samples=actual_samples, seed=SEED)\n",
    "                #results_for_query[f\"FM_Shap{actual_samples}\"], _, F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\", k=4)\n",
    "                #results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=16, beta_b=1, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                #results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        #results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "        #results_for_query[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "\n",
    "        #prob_topk = harness.evaluate_topk_performance(\n",
    "        #                                        results_for_query, \n",
    "        #                                        k_values, \n",
    "        #                                        utility_type=\"probability\"\n",
    "        #                                    )\n",
    "    \n",
    "        #div_topk = harness.evaluate_topk_performance(\n",
    "        #                                    results_for_query, \n",
    "        #                                    k_values, \n",
    "        #                                    utility_type=\"divergence\"\n",
    "        #                                )\n",
    "        LDS = []\n",
    "        r2 = []\n",
    "        RMSE = []\n",
    "        for i in results_for_query:\n",
    "            if \"FM_Shap\" in i:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30, utl=True, model=modelfm)}\n",
    "                calculate_r2 = {i:harness.r2(30, model=modelfm)}\n",
    "                calculate_RMSE = {i:harness.RMSE(30, model=modelfm)}\n",
    "                LDS.append(calculate_LDS)\n",
    "                r2.append(calculate_r2)\n",
    "                RMSE.append(calculate_RMSE)\n",
    "            elif \"ContextCite\" in i:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30)}\n",
    "                calculate_r2 = {i:harness.r2(30, model=model_cc, method=\"notfm\")}\n",
    "                calculate_RMSE = {i:harness.RMSE(30, model=model_cc, method=\"notfm\")}\n",
    "                LDS.append(calculate_LDS)\n",
    "                r2.append(calculate_r2)\n",
    "                RMSE.append(calculate_RMSE)\n",
    "        #LDS = [{i:harness.lds(results_for_query[i], 30)} for i in results_for_query]\n",
    "        LDSs.append(LDS)\n",
    "        r2s.append(r2)\n",
    "        RMSEs.append(RMSE)\n",
    "\n",
    "        #results_for_query[\"topk_probability\"] = prob_topk\n",
    "        #results_for_query[\"topk_divergence\"] = div_topk\n",
    "        harness.save_utility_cache(current_utility_path)\n",
    "        \n",
    "        all_results.append(results_for_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOC LEVEL\n",
    "num_questions_to_run = 100\n",
    "k_values = [1, 2]\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "LDSs=[]\n",
    "\n",
    "for i in tqdm(range(num_questions_to_run), disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    #docs=df.paragraphs[i]\n",
    "    docs=ast.literal_eval(df.context[i])\n",
    "\n",
    "    utility_cache_base_dir = \"../Experiment_data/NQ\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "\n",
    "    print(f'Response: {harness.target_response}')\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "    if accelerator_main.is_main_process:\n",
    "        m_samples_map = {\"L\": 100} \n",
    "        T_iterations_map = {\"L\":40} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0: \n",
    "                results_for_query[f\"ContextCite{actual_samples}\"], model_cc = harness.compute_contextcite(num_samples=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"FM_Shap{actual_samples}\"], _, F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\")\n",
    "                #results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=16, beta_b=1, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                #results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "        results_for_query[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "\n",
    "        prob_topk = harness.evaluate_topk_performance(\n",
    "                                                results_for_query, \n",
    "                                                k_values, \n",
    "                                                utility_type=\"probability\"\n",
    "                                            )\n",
    "    \n",
    "        div_topk = harness.evaluate_topk_performance(\n",
    "                                            results_for_query, \n",
    "                                            k_values, \n",
    "                                            utility_type=\"divergence\"\n",
    "                                        )\n",
    "        LDS = []\n",
    "        for i in results_for_query:\n",
    "            if \"FM_Shap\" in i:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30, utl=True, model=modelfm)}\n",
    "                LDS.append(calculate_LDS)\n",
    "            else:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30)}\n",
    "                LDS.append(calculate_LDS)\n",
    "        #LDS = [{i:harness.lds(results_for_query[i], 30)} for i in results_for_query]\n",
    "        LDSs.append(LDS)\n",
    "\n",
    "        results_for_query[\"topk_probability\"] = prob_topk\n",
    "        results_for_query[\"topk_divergence\"] = div_topk\n",
    "        harness.save_utility_cache(current_utility_path)\n",
    "        \n",
    "        all_results.append(results_for_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "all_results_json = copy.deepcopy(all_results)\n",
    "\n",
    "    # Recursively convert NumPy arrays to lists\n",
    "def convert_numpy_to_list(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.floating, np.bool_)):\n",
    "        # Convert NumPy scalar types to Python native types\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy_to_list(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "            return [convert_numpy_to_list(elem) for elem in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "converted_all_results = convert_numpy_to_list(all_results_json)\n",
    "\n",
    "with open('NQ_sents_50_FM20_128.json', 'w') as f:\n",
    "    json.dump(converted_all_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NQ_sents_50_FM5_128.json', 'r') as f:\n",
    "    reloaded_all_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Averaged Results ---\n",
      "{'ContextCite128': 0.7727334949984583, 'FM_Shap128': 0.49181042131650154}\n",
      "\n",
      "--- Averaged Results ---\n",
      "{'ContextCite128': 0.6258846358556795, 'FM_Shap128': -31.20602288561671}\n",
      "\n",
      "--- Averaged Results ---\n",
      "{'ContextCite128': 1.8267026093929886, 'FM_Shap128': 4.762287752280098}\n"
     ]
    }
   ],
   "source": [
    "consolidated_LDS = [{key: value for d in inner_list for key, value in d.items()} for inner_list in LDSs]\n",
    "consolidated_R2 = [{key: value for d in inner_list for key, value in d.items()} for inner_list in r2s]\n",
    "consolidated_RMSE = [{key: value for d in inner_list for key, value in d.items()} for inner_list in RMSEs]\n",
    "import collections\n",
    "import math\n",
    "\n",
    "def average_list_of_dicts(list_of_dicts):\n",
    "    \"\"\"\n",
    "    Averages numeric values across dictionaries in a list, based on common keys.\n",
    "    Handles NaN values by skipping them.\n",
    "\n",
    "    Args:\n",
    "        list_of_dicts (list): A list where each element is a dictionary.\n",
    "                              All dictionaries are expected to have the same keys.\n",
    "                              Values may include NaNs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the original keys and values are\n",
    "              the averages of the corresponding numeric values from the input dictionaries.\n",
    "              NaN values are ignored for averaging.\n",
    "              Returns an empty dictionary if the input list is empty.\n",
    "    \"\"\"\n",
    "    if not list_of_dicts:\n",
    "        return {} # Return empty dict for empty input, no print statement as requested\n",
    "\n",
    "    sum_values = collections.defaultdict(float)\n",
    "    count_values = collections.defaultdict(int)\n",
    "\n",
    "    for d in list_of_dicts:\n",
    "        for key, value in d.items():\n",
    "            # Check if the value is numeric (int or float) AND not NaN\n",
    "            if isinstance(value, (int, float)) and not math.isnan(value):\n",
    "                sum_values[key] += value\n",
    "                count_values[key] += 1\n",
    "            # Other types (strings, bools, etc.) and NaNs are simply skipped\n",
    "\n",
    "    averaged_dict = {}\n",
    "    for key in list_of_dicts[0].keys(): # Iterate through all expected keys from the first dict\n",
    "        if count_values[key] > 0:\n",
    "            averaged_dict[key] = sum_values[key] / count_values[key]\n",
    "        else:\n",
    "            # If no valid numeric values were found for a key (e.g., all were NaN or non-numeric)\n",
    "            averaged_dict[key] = float('nan') # Or 0.0, or None, depending on desired output\n",
    "\n",
    "    return averaged_dict\n",
    "\n",
    "import pprint\n",
    "averaged_LDS = average_list_of_dicts(consolidated_LDS)\n",
    "print(\"\\n--- Averaged Results ---\")\n",
    "pprint.pprint(averaged_LDS)\n",
    "\n",
    "averaged_R2 = average_list_of_dicts(consolidated_R2)\n",
    "print(\"\\n--- Averaged Results ---\")\n",
    "pprint.pprint(averaged_R2)\n",
    "\n",
    "averaged_RMSE = average_list_of_dicts(consolidated_RMSE)\n",
    "print(\"\\n--- Averaged Results ---\")\n",
    "pprint.pprint(averaged_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_bar_chart(data_dict, title=\"Averaged R2 for NQ sent-level, FM20\",\n",
    "                   x_label=\"Methods\", y_label=\"R2\"):\n",
    "\n",
    "    plot_data = []\n",
    "    for key, value in data_dict.items():\n",
    "        if not math.isnan(value):\n",
    "            plot_data.append((key, value))\n",
    "        else:\n",
    "            print(f\"Skipping '{key}' for plotting as its value is NaN.\")\n",
    "\n",
    "    if not plot_data:\n",
    "        print(\"No valid numeric data to plot after filtering NaNs.\")\n",
    "        return\n",
    "\n",
    "    # --- NEW: Sort the data ---\n",
    "    # Sort by the value (index 1 of the tuple), in descending order\n",
    "    plot_data.sort(key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Unpack sorted data into separate lists for plotting\n",
    "    labels = [item[0] for item in plot_data]\n",
    "    values = [item[1] for item in plot_data]\n",
    "    \n",
    "    # --- NEW: Define a list of colors ---\n",
    "    # You can customize this list with any valid matplotlib color names or hex codes.\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "    # If you have more bars than colors, you can use matplotlib.cm for a colormap:\n",
    "    # import matplotlib.cm as cm\n",
    "    # colors = cm.viridis(np.linspace(0, 1, len(labels)))\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    # --- NEW: Pass the list of colors to plt.bar ---\n",
    "    bars = plt.bar(labels, values, color=colors[:len(labels)]) # Slice to match number of bars\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "\n",
    "    # Rotate x-axis labels if they are long to prevent overlap\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10) # 'ha' is horizontal alignment\n",
    "\n",
    "    # Add value labels on top of the bars\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, # Position text slightly above bar\n",
    "                 round(yval, 4), # Format value to 4 decimal places\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Add a grid for easier reading of values\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Adjust layout to prevent labels from being cut off\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    #plt.show()\n",
    "    plt.savefig(f'nq_sents_100_fm20_R2_2307.png')\n",
    "\n",
    "plot_bar_chart(averaged_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability-based results\n",
    "print(\"\\nProbability-based Top-k Performance:\")\n",
    "for q in reloaded_all_results:\n",
    "    for method, drops in q['topk_probability'].items():\n",
    "        print(f\"  {method}:\")\n",
    "        for k, drop in drops.items():\n",
    "            print(f\"    k={k}: Drop = {drop:.4f}\")\n",
    "\n",
    "# Divergence-based results\n",
    "print(\"\\nDivergence-based Top-k Performance:\")\n",
    "for q in reloaded_all_results:\n",
    "    for method, jsds in q['topk_divergence'].items():\n",
    "        print(f\"  {method}:\")\n",
    "        for k, jsd in jsds.items():\n",
    "            print(f\"    k={k}: JSD = {jsd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContextCite128': {'1': 270.12265995144844, '2': 392.120825111866},\n",
       " 'FM_Shap128': {'1': 278.2387535870075, '2': 416.97775742411613},\n",
       " 'ContextCite256': {},\n",
       " 'FM_Shap256': {},\n",
       " 'LOO': {'1': 278.2387535870075, '2': 328.6395903378725},\n",
       " 'ARC-JSD': {'1': 270.42414382100105, '2': 302.5687276571989}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_drops = {'ContextCite128': {}, 'FM_Shap128': {}, 'ContextCite256': {}, 'FM_Shap256': {}, 'LOO': {}, 'ARC-JSD': {}}\n",
    "div_drops = {'ContextCite128': {}, 'FM_Shap128': {}, 'ContextCite256': {}, 'FM_Shap256': {}, 'LOO': {}, 'ARC-JSD': {}}\n",
    "count = 0\n",
    "for q in reloaded_all_results:\n",
    "    for method in q:\n",
    "        if method == 'topk_probability':\n",
    "            drops = q[method]\n",
    "            for m in drops:\n",
    "                if m not in ('TMC100', 'BetaShap100'):\n",
    "                    values = drops[m]\n",
    "                    try:\n",
    "                        topk_drops[m]['1'] += values['1']\n",
    "                    except:\n",
    "                        topk_drops[m]['1'] = values['1']\n",
    "                    try:\n",
    "                        topk_drops[m]['2'] += values['2']\n",
    "                    except:\n",
    "                        topk_drops[m]['2'] = values['2']\n",
    "        elif method == 'topk_divergence':\n",
    "            drops = q[method]\n",
    "            for m in drops:\n",
    "                if m not in ('TMC100', 'BetaShap100'):\n",
    "                    values = drops[m]\n",
    "                    try:\n",
    "                        div_drops[m]['1'] += values['1']\n",
    "                    except:\n",
    "                        div_drops[m]['1'] = values['1']\n",
    "                    try:\n",
    "                        div_drops[m]['2'] += values['2']\n",
    "                    except:\n",
    "                        div_drops[m]['2'] = values['2']\n",
    "    count += 1\n",
    "topk_drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in topk_drops:\n",
    "    for k in topk_drops[m]:\n",
    "        topk_drops[m][k] = topk_drops[m][k]/count\n",
    "for m in div_drops:\n",
    "    for k in div_drops[m]:\n",
    "        div_drops[m][k] = div_drops[m][k]/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContextCite128': {'1': 5.402453199028969, '2': 7.84241650223732},\n",
       " 'FM_Shap128': {'1': 5.564775071740151, '2': 8.339555148482322},\n",
       " 'ContextCite256': {},\n",
       " 'FM_Shap256': {},\n",
       " 'LOO': {'1': 5.564775071740151, '2': 6.57279180675745},\n",
       " 'ARC-JSD': {'1': 5.408482876420021, '2': 6.051374553143978}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContextCite128': {'1': 0.7963868528121247, '2': 1.180995022549329},\n",
       " 'FM_Shap128': {'1': 0.8192394025804025, '2': 1.2128547834401404},\n",
       " 'ContextCite256': {},\n",
       " 'FM_Shap256': {},\n",
       " 'LOO': {'1': 0.8137818653616875, '2': 1.0525908829749526},\n",
       " 'ARC-JSD': {'1': 1.0260680706004086, '2': 1.2847744130069372}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math # For handling potential NaN values if they were in the input\n",
    "\n",
    "# --- Plotting Function for Grouped Bar Chart ---\n",
    "def plot_grouped_bar_chart(data_for_plotting, title=\"Divergence Climb\",\n",
    "                           x_label=\"Methods\", y_label=\"Averaged Divergence Climb\"):\n",
    "    \"\"\"\n",
    "    Draws a grouped bar chart for the given nested dictionary data.\n",
    "\n",
    "    Args:\n",
    "        data_for_plotting (dict): A dictionary where keys are metric names,\n",
    "                                  and values are dictionaries containing numeric\n",
    "                                  values for categories like '1' and '2'.\n",
    "                                  e.g., {'MetricA': {'1': 0.1, '2': 0.2}}\n",
    "        title (str): The title of the plot.\n",
    "        x_label (str): Label for the x-axis.\n",
    "        y_label (str): Label for the y-axis.\n",
    "    \"\"\"\n",
    "    if not data_for_plotting:\n",
    "        print(\"No data to plot. The input dictionary is empty.\")\n",
    "        return\n",
    "\n",
    "    # Extract metric names and ensure a consistent order (alphabetical for clarity)\n",
    "    metric_names = sorted(data_for_plotting.keys())\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    values_cat1 = []\n",
    "    values_cat2 = []\n",
    "    \n",
    "    # Store labels for the actual metrics being plotted (in case some have NaNs)\n",
    "    plot_metric_labels = []\n",
    "\n",
    "    for metric in metric_names:\n",
    "        cat_data = data_for_plotting[metric]\n",
    "        val1 = cat_data.get('1', float('nan')) # Use .get() to handle missing keys\n",
    "        val2 = cat_data.get('2', float('nan'))\n",
    "\n",
    "        # Only include metrics where at least one category has a valid number\n",
    "        if (isinstance(val1, (int, float)) and not math.isnan(val1)) or \\\n",
    "           (isinstance(val2, (int, float)) and not math.isnan(val2)):\n",
    "            values_cat1.append(val1 if (isinstance(val1, (int, float)) and not math.isnan(val1)) else 0)\n",
    "            values_cat2.append(val2 if (isinstance(val2, (int, float)) and not math.isnan(val2)) else 0)\n",
    "            plot_metric_labels.append(metric)\n",
    "        else:\n",
    "            print(f\"Skipping metric '{metric}' as both '1' and '2' values are NaN or non-numeric.\")\n",
    "\n",
    "\n",
    "    if not plot_metric_labels:\n",
    "        print(\"No valid data points found to plot after processing categories.\")\n",
    "        return\n",
    "\n",
    "    # Set up positions for the bars\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(plot_metric_labels)) # The x locations for the groups\n",
    "\n",
    "    plt.figure(figsize=(14, 8)) # Adjust figure size\n",
    "\n",
    "    # Plotting the bars\n",
    "    bar1 = plt.bar(index - bar_width/2, values_cat1, bar_width, label='1', color='skyblue')\n",
    "    bar2 = plt.bar(index + bar_width/2, values_cat2, bar_width, label='2', color='lightcoral')\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel(x_label, fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(index, plot_metric_labels, rotation=45, ha='right', fontsize=12) # Set metric labels at group center\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Add value labels on top of the bars\n",
    "    def autolabel(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, height + 0.005, # Position text slightly above bar\n",
    "                     f'{height:.4f}', # Format value to 4 decimal places\n",
    "                     ha='center', va='bottom', fontsize=9, rotation=0)\n",
    "\n",
    "    autolabel(bar1)\n",
    "    autolabel(bar2)\n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from being cut off\n",
    "    #plt.show()\n",
    "    plt.savefig(f'nq_sents_100_fm20_div_drop_2307.png')\n",
    "\n",
    "plot_grouped_bar_chart(div_drops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ContextCite100',\n",
       " 'FM_Shap100',\n",
       " 'FM_Weights100',\n",
       " 'BetaShap100',\n",
       " 'TMC100',\n",
       " 'LOO',\n",
       " 'ARC-JSD',\n",
       " 'topk_probability',\n",
       " 'topk_divergence']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in results_for_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how many episodes are in series 7 game of thrones'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.question[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness._generate_sampled_ablations(4, sampling_method='uniform', seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'Game of Thrones (season 7) The seventh and penultimate season of the fantasy drama television series \"Game of Thrones\" premiered on HBO on July 16, 2017, and concluded on August 27, 2017. Unlike previous seasons that consisted of ten episodes each, the seventh season consisted of only seven. Like the previous season, it largely consisted of original content not found in George R. R. Martin\\\\\\'s \"A Song of Ice and Fire\" series, while also incorporating material Martin revealed to showrunners about the upcoming novels in the series. The series was adapted for television by David Benioff and D. B. Weiss.\\', \\'Bender, who worked on the show\\\\\\'s sixth season, said that the seventh season would consist of seven episodes. Benioff and Weiss stated that they were unable to produce 10 episodes in the show\\\\\\'s usual 12 to 14 month time frame, as Weiss said \"It\\\\\\'s crossing out of a television schedule into more of a mid-range movie schedule.\" HBO confirmed on July 18, 2016, that the seventh season would consist of seven episodes, and would premiere later than usual in mid-2017 because of the later filming schedule. Later it was confirmed that the season would debut on July 16. The seventh\\', \\'\"Battlestar Galactica\" show bought the show\\\\\\\\\\\\\\'s sets. They were redesigned the next year and used for scenes on the Battlestar \"Pegasus\". Dick Tufeld reprised his role as voice of the robot for the third time. On October 10, 2014, it was announced that Legendary TV was developing a new reboot of \"Lost in Space\" for Netflix with \"Dracula Untold\" screenwriters Matt Sazama and Burk Sharpless attached to write. On June 29, 2016, Netflix ordered the series with 10 episodes. The series debuted on Netflix on April 13, 2018. It was renewed for a second season on May 13, 2018. The\\\\\\'\\', \"series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsgård, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsgård also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley\\'\", \\'Britain\\\\\\\\\\\\\\'s Got Talent Britain\\\\\\\\\\\\\\'s Got Talent (often abbreviated to BGT) is a televised British talent show competition, broadcast on ITV. It is a part of the global \"Got Talent\" franchise created by Simon Cowell, and is produced by both Thames (formerly Talkback Thames) and Syco Entertainment production, with its distribution handled by Fremantle. Since its premiere in June 2007, each series has been aired in late Spring/early Summer, and hosted by Ant & Dec. To accompany each series since it first began, a sister show is run on ITV2 entitled Britain\\\\\\\\\\\\\\'s Got More Talent presented by Stephen Mulhern. Initially planned\\\\\\'\\', \\'Factor\" (also created by Cowell) and \"The Voice UK\". On average, it draws viewing figures of 9.9 million viewers per series - the show\\\\\\\\\\\\\\'s live final in the third series attracted a record 17.3 million viewers, obtaining a 64.6% audience share at the time of its broadcast. At present, the programme is contracted to run until 2022. The show\\\\\\\\\\\\\\'s format was devised by \"X Factor\" creator and Sony Music executive, Simon Cowell, who was involved in the creation of other \"Got Talent\" programmes across several different countries. To showcase his idea, a pilot episode was filmed in September 2005, with\\\\\\'\\', \\'genuine, though very obscure, saying, \"only fools and horses work for a living\", which had its origins in 19th-century American vaudeville. \"Only Fools and Horses\" had also been the title of an episode of \"Citizen Smith\", and Sullivan liked the expression and thought it was suited to the new sitcom. He also thought longer titles would attract attention. He was first overruled on the grounds that the audience would not understand the title, but he eventually got his way. Filming of the first series began in May 1981, and the first episode, \"Big Brother\", was transmitted on BBC One at\\\\\\'\\', \\'Grey\\\\\\\\\\\\\\'s Anatomy (season 14) The fourteenth season of the American television medical drama \"Grey\\\\\\\\\\\\\\'s Anatomy\" was ordered on February 10, 2017, by American Broadcasting Company (ABC), and premiered on September 28, 2017 with a special two-hour premiere. The season consists of 24 episodes, with the season\\\\\\\\\\\\\\'s seventh episode marking the 300th episode for the series overall. The season is produced by ABC Studios, in association with Shondaland Production Company and The Mark Gordon Company; the showrunners being Krista Vernoff and William Harper. The fourteenth season is the first not to feature Jerrika Hinton as Dr. Stephanie Edwards since her introduction\\\\\\'\\', \\'Only Fools and Horses Only Fools and Horses is a British sitcom created and written by John Sullivan. Seven series were originally broadcast on BBC One in the United Kingdom from 1981 to 1991, with sixteen sporadic Christmas specials aired until the end of the show in 2003. Episodes are regularly repeated on UKTV comedy channel Gold and occasionally repeated on Yesterday and BBC One. Set in Peckham in south-east London, it stars David Jason as ambitious market trader Derek \"Del Boy\" Trotter, Nicholas Lyndhurst as his younger brother Rodney Trotter, and Lennard Pearce as their elderly Grandad. After Pearce\\\\\\\\\\\\\\'s\\\\\\'\\', \\'the Imperial War Museum, London. A showing Del, Rodney and Albert making an appeal for donations was shown on 14 March 1997, with 10.6 million viewers. A Sport Relief special was aired on 21 March 2014. \"Only Fools and Horses\" had two producers: Ray Butt from 1981 to 1987, and Gareth Gwenlan thereafter. Seven directors were used: Martin Shardlow directed all episodes in series one, Bernard Thompson directed the 1981 Christmas special, Susan Belbin series four, and Mandie Fletcher series five. Butt directed series three and five, as well as the 1985, 1986 and 1987 Christmas specials. Tony Dow became\\\\\\'\\']'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genuine, though very obscure, saying, \"only fools and horses work for a living\", which had its origins in 19th-century American vaudeville. \"Only Fools and Horses\" had also been the title of an episode of \"Citizen Smith\", and Sullivan liked the expression and thought it was suited to the new sitcom. He also thought longer titles would attract attention. He was first overruled on the grounds that the audience would not understand the title, but he eventually got his way. Filming of the first series began in May 1981, and the first episode, \"Big Brother\", was transmitted on BBC One at\\''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContextCite100': array([ 9.42799874e+00,  4.95333848e+00,  1.54510382e-01, -1.18733185e+00,\n",
       "        -1.97362536e-01, -1.63601493e-03, -4.20977587e-01, -1.71711992e+00,\n",
       "         5.13743546e-01, -3.61054749e-01]),\n",
       " 'FM_Shap100': array([10.32068284,  5.69851447,  0.11906173, -0.44636073, -0.88331575,\n",
       "        -1.00641049, -0.22007863, -0.79765982,  0.29558806, -0.81720759]),\n",
       " 'FM_Weights100': array([10.32068284,  5.69851447,  0.11906173, -0.44636073, -0.88331575,\n",
       "        -1.00641049, -0.22007863, -0.79765982,  0.29558806, -0.81720759]),\n",
       " 'BetaShap100': array([12.18496471,  6.09588892,  0.22350156,  0.12756782, -0.06228456,\n",
       "         0.05961801,  0.48444461, -0.42519495,  0.84019263, -0.43572434]),\n",
       " 'TMC100': array([ 9.96529419,  5.98631053, -0.16527917, -0.53574083, -0.08729146,\n",
       "        -0.09466684,  0.01608808, -0.76350622,  0.2977114 , -0.34037049]),\n",
       " 'LOO': array([12.21575832,  8.61024857,  1.24591446,  0.25884056, -0.06205273,\n",
       "         0.06636238,  0.56908131, -0.40280247,  0.83529949, -0.49150467]),\n",
       " 'ARC-JSD': [0.5446134370340587,\n",
       "  1.0898773498838636,\n",
       "  0.044462880339779076,\n",
       "  0.01866064508612908,\n",
       "  0.002239784375689169,\n",
       "  0.007842083208885242,\n",
       "  0.01821326307367599,\n",
       "  0.2748797760945081,\n",
       "  0.025274369537008567,\n",
       "  0.033064027917419025]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -2.71718673,  2.64412498,  0.42945254,  0.83703511,\n",
       "         0.81296934,  2.79770344, -0.70547725, -1.78606628, -1.45755979],\n",
       "       [-2.71718673,  0.        , -1.55238252,  0.47399397,  0.30587813,\n",
       "        -1.0472584 , -3.32381804, -0.22556415,  1.52053671,  2.70219414],\n",
       "       [ 2.64412498, -1.55238252,  0.        , -0.05849546,  0.24429583,\n",
       "         1.2087218 , -1.96988449,  0.43088249,  1.2081562 ,  1.73971653],\n",
       "       [ 0.42945254,  0.47399397, -0.05849546,  0.        ,  0.45773645,\n",
       "         0.56765182,  0.01962721, -0.01922445,  0.25747797, -0.84770228],\n",
       "       [ 0.83703511,  0.30587813,  0.24429583,  0.45773645,  0.        ,\n",
       "         0.23349889,  0.07346147, -0.04195382,  0.03793556, -0.31164835],\n",
       "       [ 0.81296934, -1.0472584 ,  1.2087218 ,  0.56765182,  0.23349889,\n",
       "         0.        ,  0.17034947,  0.01708509,  0.06055724, -0.52206366],\n",
       "       [ 2.79770344, -3.32381804, -1.96988449,  0.01962721,  0.07346147,\n",
       "         0.17034947,  0.        , -0.15618031, -0.78104942, -1.01976868],\n",
       "       [-0.70547725, -0.22556415,  0.43088249, -0.01922445, -0.04195382,\n",
       "         0.01708509, -0.15618031,  0.        ,  0.11437202,  0.07063006],\n",
       "       [-1.78606628,  1.52053671,  1.2081562 ,  0.25747797,  0.03793556,\n",
       "         0.06055724, -0.78104942,  0.11437202,  0.        ,  0.33895081],\n",
       "       [-1.45755979,  2.70219414,  1.73971653, -0.84770228, -0.31164835,\n",
       "        -0.52206366, -1.01976868,  0.07063006,  0.33895081,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_subsets = list(itertools.product([0, 1], repeat=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sampled_tuples = harness._generate_sampled_ablations(10, sampling_method='uniform', seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContextCite100 [ 9.42799874e+00  4.95333848e+00  1.54510382e-01 -1.18733185e+00\n",
      " -1.97362536e-01 -1.63601493e-03 -4.20977587e-01 -1.71711992e+00\n",
      "  5.13743546e-01 -3.61054749e-01]\n",
      "FM_Shap100 [10.32068284  5.69851447  0.11906173 -0.44636073 -0.88331575 -1.00641049\n",
      " -0.22007863 -0.79765982  0.29558806 -0.81720759]\n",
      "FM_Weights100 [10.32068284  5.69851447  0.11906173 -0.44636073 -0.88331575 -1.00641049\n",
      " -0.22007863 -0.79765982  0.29558806 -0.81720759]\n",
      "BetaShap100 [12.18172271  7.73213164  0.78913852  0.1642193  -0.06000717  0.05865822\n",
      "  0.56100273 -0.43111163  0.83715638 -0.46782496]\n",
      "TMC100 [ 9.96529419  5.98631053 -0.16527917 -0.53574083 -0.08729146 -0.09466684\n",
      "  0.01608808 -0.76350622  0.2977114  -0.34037049]\n",
      "LOO [12.21575832  8.61024857  1.24591446  0.25884056 -0.06205273  0.06636238\n",
      "  0.56908131 -0.40280247  0.83529949 -0.49150467]\n",
      "ARC-JSD [ 0.34588716 10.75895722  0.03822068  0.01981782  0.03414572  0.03501194\n",
      "  0.05906658  0.02726513  0.04695243  0.04338138]\n",
      "ContextCite100 [ 0.53418168 12.16175294 -0.         -0.24774383  0.07068216 -0.15795876\n",
      " -0.33347372 -0.75802752 -0.08757905  0.43004518]\n",
      "FM_Shap100 [ 1.15737413 11.18100285  0.95781058 -0.32263266 -0.08514776 -0.18217726\n",
      " -1.15921022 -0.97505184 -0.85258222  0.99982655]\n",
      "FM_Weights100 [ 1.15737413 11.18100285  0.95781058 -0.32263266 -0.08514776 -0.18217726\n",
      " -1.15921022 -0.97505184 -0.85258222  0.99982655]\n",
      "BetaShap100 [ 0.21676057 11.50381064 -0.09319942  0.0297979   0.15657951 -0.02019246\n",
      "  0.09096223  0.06806707  0.14693925  0.2230877 ]\n",
      "TMC100 [ 1.40377991e+00  1.16928111e+01  3.30199075e-01  2.81167603e-01\n",
      " -5.76844215e-02 -1.53967810e-01 -4.03205872e-01 -1.12489018e+00\n",
      " -6.06763196e-01  1.18422508e-04]\n",
      "LOO [ 0.20963287 11.43750668 -0.05697346  0.03973675  0.14608097 -0.02613449\n",
      "  0.1030159   0.0685215   0.14507675  0.20595169]\n",
      "ARC-JSD [0.14539488 0.03024971 0.01218375 0.00362713 0.04913212 0.07869485\n",
      " 0.39515209 0.09537129 0.45822141 0.04900802]\n",
      "ContextCite100 [14.91676144  1.30346552 -0.13807412  0.13872938 -0.17465006  0.\n",
      "  0.          0.22240864 -0.18441365 -0.39862788]\n",
      "FM_Shap100 [14.04915143  2.64288548 -0.13482286 -0.11445386  0.06647309  0.24001701\n",
      " -0.0462553  -0.33132172 -0.51126746 -0.21880319]\n",
      "FM_Weights100 [14.04915143  2.64288548 -0.13482286 -0.11445386  0.06647309  0.24001701\n",
      " -0.0462553  -0.33132172 -0.51126746 -0.21880319]\n",
      "BetaShap100 [16.72602342  5.60220246  0.28342357  0.85604533  0.41220628  0.54943228\n",
      "  0.68906237  0.47209979  0.42072394 -0.27974994]\n",
      "TMC100 [14.93326917  1.54164892 -0.38235347 -0.08974192 -0.02827799  0.21508722\n",
      " -0.04979339 -0.08708606 -0.55046264 -0.70356853]\n",
      "LOO [16.75804234  6.87147236  0.73386097  1.01523876  0.47528744  0.66949558\n",
      "  0.74391079  0.63510132  0.51623726 -0.05023003]\n",
      "ARC-JSD [0.11826901 0.00497185 0.01089974 0.00588869 0.00380878 0.00715541\n",
      " 0.00271674 0.00526729 0.00446857 0.00606334]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localsc/ekuzmenk/61547/ipykernel_3201918/1415687491.py:11: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 4))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for result in range(len(all_results)):\n",
    "    method_scores = {}\n",
    "    for method, scores in all_results[result].items():\n",
    "        if scores is not None and type(scores) is not dict:\n",
    "            print(method, scores)\n",
    "            method_scores[method] = np.round(scores, 4)\n",
    "\n",
    "    for method, scores in method_scores.items():\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.bar(range(len(scores)), scores, color='skyblue')\n",
    "        plt.title(f\"Approximate Scores: {method}\")\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.xticks(range(len(scores)))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'nq_doc_plots/{result}_{method}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
