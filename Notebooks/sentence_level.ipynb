{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dee4df1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea562b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9cdc9",
   "metadata": {},
   "source": [
    "# Data Loading & Preparation\n",
    "\n",
    "For tidyQA, there are indexes provided for the possible answer passage holding the answer. If it is \"-1\" it does  NOT mean that the answer is in the last passage chunk but simply that there is no passage holding the answer for that question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1939a",
   "metadata": {},
   "source": [
    "##  Long Version Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "57ab8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('../data/tydi.jsonl', 'r') as f: \n",
    "    for line in f: \n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4a5df248",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data = []\n",
    "for i in range(len(data)): \n",
    "    if data[i]['language'] == 'english': \n",
    "        english_data.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd88b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zebra finch'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data[0]['question_text'] # question\n",
    "english_data[0]['document_plaintext'] # context\n",
    "english_data[0]['document_title'] # title\n",
    "# english_data[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e921500",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = {'title' : [], 'context' : [],\n",
    "                 'question' : []}\n",
    "for i in range(len(english_data)) :\n",
    "    filtered_data['title'].append(english_data[i]['document_title'])\n",
    "    filtered_data['context'].append(english_data[i]['document_plaintext'])\n",
    "    filtered_data['question'].append(english_data[i]['question_text'])\n",
    "\n",
    "df = pd.DataFrame.from_dict(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224ead1",
   "metadata": {},
   "source": [
    "## Short Version Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6dde80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/tydiqa.json', 'r')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a6bff72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wound care'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]['title'] # get title\n",
    "data['data'][0]['paragraphs'][0]['context'] # get context\n",
    "data['data'][0]['paragraphs'][0]['qas'][0]['question'] # get question\n",
    "data['data'][0]['paragraphs'][0]['qas'][0]['answers'][0][\"text\"]# get answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05e57812",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = {\"title\" : [], \"context\" : [],\n",
    "                 \"question\": [], \"answer\" : []}\n",
    "for i in range(len(data['data'])) : \n",
    "    filtered_data['title'].append(data['data'][i]['title'])\n",
    "    filtered_data['context'].append(data['data'][i]['paragraphs'][0]['context'])\n",
    "    filtered_data['question'].append(data['data'][i]['paragraphs'][0]['qas'][0]['question'])\n",
    "    filtered_data['answer'].append(data['data'][i]['paragraphs'][0]['qas'][0]['answers'][0][\"text\"])\n",
    "\n",
    "df = pd.DataFrame.from_dict(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a70ae3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wound healing</td>\n",
       "      <td>Wound care encourages and speeds wound healing...</td>\n",
       "      <td>What is a way to increase your wound healing s...</td>\n",
       "      <td>Wound care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Burntisland Shipbuilding Company</td>\n",
       "      <td>Brothers Amos and Wilfrid Ayre founded Burntis...</td>\n",
       "      <td>Who founded the Burntisland Shipbuilding Company?</td>\n",
       "      <td>Amos and Wilfrid Ayre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cerebral cortex</td>\n",
       "      <td>The cerebral cortex is folded in a way that al...</td>\n",
       "      <td>What is the surface area of the human cortex?</td>\n",
       "      <td>1.3 square feet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agatha Christie</td>\n",
       "      <td>Guinness World Records lists Christie as the b...</td>\n",
       "      <td>How many units has Agatha Christie sold?</td>\n",
       "      <td>2 billion copies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nájera</td>\n",
       "      <td>The town was conquered by Ordoño II of Leon fo...</td>\n",
       "      <td>When was Nájera established?</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0                     Wound healing   \n",
       "1  Burntisland Shipbuilding Company   \n",
       "2                   Cerebral cortex   \n",
       "3                   Agatha Christie   \n",
       "4                            Nájera   \n",
       "\n",
       "                                             context  \\\n",
       "0  Wound care encourages and speeds wound healing...   \n",
       "1  Brothers Amos and Wilfrid Ayre founded Burntis...   \n",
       "2  The cerebral cortex is folded in a way that al...   \n",
       "3  Guinness World Records lists Christie as the b...   \n",
       "4  The town was conquered by Ordoño II of Leon fo...   \n",
       "\n",
       "                                            question                 answer  \n",
       "0  What is a way to increase your wound healing s...             Wound care  \n",
       "1  Who founded the Burntisland Shipbuilding Company?  Amos and Wilfrid Ayre  \n",
       "2      What is the surface area of the human cortex?        1.3 square feet  \n",
       "3           How many units has Agatha Christie sold?       2 billion copies  \n",
       "4                       When was Nájera established?                    923  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55eb69",
   "metadata": {},
   "source": [
    "## Sentence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7b0d3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['context'] = df.context.apply(lambda x: sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bf8ff4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['context_size'] = df.context.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0dcf648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.context_size >= 10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "230505e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(979, 4)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa9ccf",
   "metadata": {},
   "source": [
    "# Running Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61469ba2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5889f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2846cfb718487c84592b5abc476ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "da019738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>context_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zebra finch</td>\n",
       "      <td>[\\nThe zebra finch (Taeniopygia guttata)[2] is...</td>\n",
       "      <td>Do zebra finches have stripes?</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christian dietary laws</td>\n",
       "      <td>[\\nIn mainstream Nicene Christianity, there is...</td>\n",
       "      <td>Does Catholicism have any dietary restrictions?</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wound healing</td>\n",
       "      <td>[\\nWound healing is a complex process in which...</td>\n",
       "      <td>What is a way to increase your wound healing s...</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title                                            context  \\\n",
       "0             Zebra finch  [\\nThe zebra finch (Taeniopygia guttata)[2] is...   \n",
       "1  Christian dietary laws  [\\nIn mainstream Nicene Christianity, there is...   \n",
       "2           Wound healing  [\\nWound healing is a complex process in which...   \n",
       "\n",
       "                                            question  context_size  \n",
       "0                     Do zebra finches have stripes?           205  \n",
       "1    Does Catholicism have any dietary restrictions?            18  \n",
       "2  What is a way to increase your wound healing s...           349  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[0:2].copy()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5c052396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48411a7a9a6f4caaaa820e9de88a6c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Questions:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1/3: Do zebra finches have stripes?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q0 (n=205 docs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalai\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kalai\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE 32\n",
      "HERE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2662baf837b24828b9bcec84c7b72b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 2/3: Does Catholicism have any dietary restrictions?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q1 (n=18 docs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalai\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kalai\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE 32\n",
      "HERE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d1cd5e7cc24b62b089df246ecfac34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 3/3: What is a way to increase your wound healing speed?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q2 (n=349 docs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kalai\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kalai\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE 32\n",
      "HERE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48852c1db26b4723bdd97fa1b7711393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_questions_to_run=len(df.question)\n",
    "# num_questions_to_run=1\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "M=[]\n",
    "Fs=[]\n",
    "pairs=[]\n",
    "mse_inters=[]\n",
    "mse_lins=[]\n",
    "mse_fms=[]\n",
    "dataset_name = 'tidyQA'\n",
    "\n",
    "for i in tqdm(range(num_questions_to_run), desc=\"Processing Questions\", disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    if isinstance(df.context[i], list) == False: \n",
    "        docs=ast.literal_eval(df.context[i])\n",
    "    else: \n",
    "        docs = df.context[i]\n",
    "\n",
    "    utility_cache_base_dir = f\"../Experiment_data/{dataset_name}\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}_n{len(docs)}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "        print(f\"  Instantiating ShapleyExperimentHarness for Q{i} (n={len(docs)} docs)...\")\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        verbose=False\n",
    "    )\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "    # M.append(harness.compute_shapley_interaction_index_pairs_matrix())\n",
    "    if accelerator_main.is_main_process:\n",
    "\n",
    "        m_samples_map = {\"L\": 32}\n",
    "        T_iterations_map = { \"L\":20}\n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "            \n",
    "            print(\"HERE\", actual_samples)\n",
    "\n",
    "            if actual_samples > 0: \n",
    "                results_for_query[f\"ContextCite{actual_samples}\"] = harness.compute_contextcite(num_samples=actual_samples, seed=SEED)\n",
    "                print(\"HERE\")\n",
    "\n",
    "                # results_for_query[f\"WSS_FM{actual_samples}\"], F, mse_fm = harness.compute_wss(num_samples=actual_samples, seed=SEED)\n",
    "                # Fs.append(F)\n",
    "                # mse_fms.append(mse_fm)\n",
    "                results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=4, beta_b=4, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "        results_for_query[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "\n",
    "        # exact_scores = results_for_query.get(\"ExactInter\")\n",
    "        all_results.append(results_for_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b1e6fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Experiment_data/tidyQA/all_results.npy', all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6a98c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.load('../Experiment_data/tidyQA/all_results.npy', allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
