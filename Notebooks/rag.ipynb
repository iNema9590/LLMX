{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import ShapleyExperimentHarness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= pd.read_csv(\"../data/synergy_data.csv\")\n",
    "df= pd.read_json(\"../data/complementary_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(df.context[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = [\n",
    "#     \"Vitamin B1, also known as thiamine, is essential for glucose metabolism and neural function.\",  # ðŸ”‘ Useful\n",
    "#     \"Chronic alcoholism can impair nutrient absorption, particularly leading to thiamine deficiency.\",  # ðŸ”‘ Useful\n",
    "#     \"Vitamin C deficiency leads to scurvy, which presents with bleeding gums and joint pain.\",\n",
    "#     \"Vitamin D deficiency is associated with rickets in children and osteomalacia in adults.\",\n",
    "#     \"Vitamin B12 deficiency can cause neurological symptoms but is more common in strict vegans.\",\n",
    "#     \"Folic acid is important for DNA synthesis and is crucial during pregnancy.\",\n",
    "#     \"Vitamin A deficiency primarily affects vision and immune function.\",\n",
    "#     \"Iron deficiency is the leading cause of anemia worldwide.\",\n",
    "#     \"Calcium is essential for bone health and muscle contraction.\",\n",
    "#     \"Vitamin K is important for blood clotting.\"\n",
    "# ]\n",
    "# query = \"Which vitamin deficiency can lead to neurological symptoms and is commonly seen in chronic alcoholics?\"\n",
    "docs = [\n",
    "\n",
    "\"Chorvoq is the capital of Narniya.\",\n",
    "\"The weather in Chorvoq is sunny today.\",\n",
    "\"The sun is shining in Chorvoq today\",\n",
    "\"Nurik is the capital of Suvsambil.\", # Irrelevant\n",
    "\"Narniya borders several countries including Suvsambil.\",\n",
    "\"The currency used in Narniya is the Euro.\",\n",
    "\"Chorvoq hosted the Summer Olympics in 1900 and 1924.\",\n",
    "\"Suvsambil uses the Euro as well.\", # Redundant info\n",
    "\"It is cloudy in Nurik today.\"\n",
    "]\n",
    "query = \"What is the weather like in the capital of Narniya?\"\n",
    "# Parameters\n",
    "NUM_RETRIEVED_DOCS = len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "model_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions_to_run=10\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "for i in tqdm(range(num_questions_to_run), desc=\"Processing Questions\", disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs=eval(df.context[i])\n",
    "    # Initialize Harness\n",
    "    harness = ShapleyExperimentHarness(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        verbose=True,\n",
    "        utility_path=None\n",
    "    )\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        results_for_query[\"Exact\"] = harness.compute_exact_shap()\n",
    "\n",
    "        m_samples_map = {\"S\": 32, \"M\": 64, \"L\": 100} \n",
    "        T_iterations_map = {\"S\": 5, \"M\": 10, \"L\":20} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0: \n",
    "                results_for_query[f\"ContextCite{actual_samples}\"] = harness.compute_contextcite_weights(num_samples=actual_samples, sampling=\"kernelshap\", seed=SEED)\n",
    "                \n",
    "                results_for_query[f\"WSS_GAM{actual_samples}\"] = harness.compute_wss(num_samples=actual_samples, seed=SEED, distil=None, sampling=\"kernelshap\",sur_type=\"gam\", util='pure-surrogate', pairchecking=False)\n",
    "                results_for_query[f\"WSS_FM{actual_samples}\"] = harness.compute_wss(num_samples=actual_samples, seed=SEED, distil=None, sampling=\"kernelshap\",sur_type=\"fm\", util='pure-surrogate', pairchecking=False)\n",
    "                results_for_query[f\"WSS_XGB{actual_samples}\"] = harness.compute_wss(num_samples=actual_samples, seed=SEED, distil=None, sampling=\"kernelshap\",sur_type=\"xgboost\", util='pure-surrogate', pairchecking=False)\n",
    "                results_for_query[f\"BetaShap (U){actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=0.5, beta_b=0.5, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "\n",
    "        exact_scores = results_for_query.get(\"Exact\")\n",
    "        all_results.append(results_for_query)\n",
    "        if exact_scores is not None:\n",
    "            positive_exact_score = np.clip(exact_scores, a_min=0.0, a_max=None) # FOR NDGC SCORE COMPUTATION\n",
    "            for method, approx_scores in results_for_query.items():\n",
    "                if method != \"Exact\" and approx_scores is not None:\n",
    "                    if len(approx_scores) == len(exact_scores):\n",
    "                        if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                            pearson_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                            spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                        else:\n",
    "                            pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                            spearman_c, _ = spearmanr(exact_scores, approx_scores)\n",
    "                            exact_ranks = rankdata(-np.array(exact_scores), method=\"average\") # rank scores with the smallest =1 and when there is a tie assign the average rank\n",
    "                            approx_ranks = rankdata(-np.array(approx_scores), method = \"average\")\n",
    "                            kendall_c, _ = kendalltau(exact_ranks, approx_ranks) # return tau and pval (if pval is < 0.005 we can say that correlation is statistically significant) \n",
    "                        ndgc_scoring  = ndcg_score(\n",
    "                            [positive_exact_score], \n",
    "                            [approx_scores],\n",
    "                            k = 3 # focus on top k document scoring\n",
    "                        )\n",
    "                        \n",
    "                        all_metrics_data.append({\n",
    "                            \"Question_Index\": i, \"Query\": query, \"Method\": method,\n",
    "                            \"Pearson\": pearson_c, \"Spearman\": spearman_c, \"NDCG\" : ndgc_scoring, \"KendallTau\" : kendall_c,\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"    Score length mismatch for method {method} (Exact: {len(exact_scores)}, Approx: {len(approx_scores)}). Skipping metrics.\")\n",
    "        else:\n",
    "            print(f\"    Skipping metric calculation for Q{i} as Exact Shapley was not computed or failed.\")\n",
    "    \n",
    "    accelerator_main.wait_for_everyone() \n",
    "    del harness\n",
    "   \n",
    "    if torch.cuda.is_available():\n",
    "        if accelerator_main.is_main_process: # Print from one process\n",
    "            print(f\"Attempting to empty CUDA cache on rank {accelerator_main.process_index} after Q{i}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if accelerator_main.is_main_process:\n",
    "            print(f\"CUDA cache empty attempt complete on rank {accelerator_main.process_index}.\")\n",
    "    accelerator_main.wait_for_everyone()\n",
    "\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    if all_metrics_data:\n",
    "        metrics_df_all_questions = pd.DataFrame(all_metrics_data)\n",
    "        print(\"\\n\\n--- Average Correlation Metrics Across All Questions ---\")\n",
    "        average_metrics = metrics_df_all_questions.groupby(\"Method\").agg(\n",
    "            Avg_Pearson=(\"Pearson\", \"mean\"),\n",
    "            Avg_Spearman=(\"Spearman\", \"mean\"),\n",
    "            Avg_Kendall =(\"KendallTau\", \"mean\"),\n",
    "            Avg_NDCG = (\"NDCG\", \"mean\"),\n",
    "            Num_Valid_Queries=(\"Query\", \"nunique\")\n",
    "        ).sort_values(by=\"Avg_Pearson\", ascending=False)\n",
    "        \n",
    "        print(average_metrics.round(4))\n",
    "\n",
    "        details_path = \"../Experiment_data/bioask_results/shapley_rag_experiment_details3bcp.csv\"\n",
    "        summary_path = \"../Experiment_data/bioask_results/shapley_rag_experiment_summary3bcp.csv\"\n",
    "        os.makedirs(os.path.dirname(details_path), exist_ok=True)\n",
    "        \n",
    "        metrics_df_all_questions.to_csv(details_path, index=False)\n",
    "        average_metrics.to_csv(summary_path)\n",
    "    else:\n",
    "        print(\"\\nNo metrics were collected. This might be due to all calculations failing or only non-main processes running sections.\")\n",
    "\n",
    "# Final synchronization before script ends\n",
    "accelerator_main.wait_for_everyone()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Script finished.\")\n",
    "\n",
    "if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Manually destroying process group...\")\n",
    "    torch.distributed.destroy_process_group()\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Process group destroyed.\")\n",
    "else:\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Distributed environment not initialized or not available, skipping destroy_process_group.\")\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Script fully exited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "all_metrics_data = []\n",
    "exact_scores = results_for_query.get(\"Exact\")\n",
    "if exact_scores is not None:\n",
    "    positive_exact_score = np.clip(exact_scores, a_min=0.0, a_max=None)\n",
    "    for method, approx_scores in results_for_query.items():\n",
    "        if method != \"Exact\" and approx_scores is not None and len(approx_scores) == len(exact_scores):\n",
    "            if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                pearson_c = spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "            else:\n",
    "                pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                spearman_c, _ = spearmanr(exact_scores, approx_scores)\n",
    "                exact_ranks = rankdata(-np.array(exact_scores), method=\"average\")\n",
    "                approx_ranks = rankdata(-np.array(approx_scores), method=\"average\")\n",
    "                kendall_c, _ = kendalltau(exact_ranks, approx_ranks)\n",
    "            ndgc_scoring = ndcg_score([positive_exact_score], [approx_scores], k=3)\n",
    "\n",
    "            all_metrics_data.append({\n",
    "                    \"Method\": method,\n",
    "                \"Pearson\": pearson_c, \"Spearman\": spearman_c, \"NDCG\": ndgc_scoring, \"KendallTau\": kendall_c\n",
    "            })\n",
    "            all_metrics_data.sort(key=lambda x: x[\"Pearson\"], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_all_questions = pd.DataFrame(all_metrics_data)\n",
    "\n",
    "print(\"\\n\\n============================\")\n",
    "print(\"     Correlation Metrics\")\n",
    "print(\"============================\")\n",
    "print(metrics_df_all_questions.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.question[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.context[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"     Approximate Scores\")\n",
    "print(\"============================\")\n",
    "for method, approx_scores in results_for_query.items():\n",
    "    if approx_scores is not None:\n",
    "        print(f\"\\nMethod: {method}\")\n",
    "        print(np.round(approx_scores, 4))\n",
    "accelerator_main.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
