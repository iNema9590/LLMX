{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import ShapleyExperimentHarness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= pd.read_csv(\"../data/synergy_data.csv\")\n",
    "df= pd.read_csv(\"../data/complementary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = [\n",
    "#     \"Vitamin B1, also known as thiamine, is essential for glucose metabolism and neural function.\",  #\n",
    "#     \"Chronic alcoholism can impair nutrient absorption, particularly leading to thiamine deficiency.\",  # \n",
    "#     \"Vitamin C deficiency leads to scurvy, which presents with bleeding gums and joint pain.\",\n",
    "#     \"Vitamin D deficiency is associated with rickets in children and osteomalacia in adults.\",\n",
    "#     \"Vitamin B12 deficiency can cause neurological symptoms but is more common in strict vegans.\",\n",
    "#     \"Folic acid is important for DNA synthesis and is crucial during pregnancy.\",\n",
    "#     \"Vitamin A deficiency primarily affects vision and immune function.\",\n",
    "#     \"Iron deficiency is the leading cause of anemia worldwide.\",\n",
    "#     \"Calcium is essential for bone health and muscle contraction.\",\n",
    "#     \"Vitamin K is important for blood clotting.\"\n",
    "# ]\n",
    "# query = \"Which vitamin deficiency can lead to neurological symptoms and is commonly seen in chronic alcoholics?\"\n",
    "docs = [\n",
    "\"The sun is shining in Chorvoq today\",\n",
    "\"Nurik is the capital of Suvsambil.\", # Irrelevant\n",
    "\"Narniya borders several countries including Suvsambil.\",\n",
    "\"The currency used in Narniya is the Euro.\",\n",
    "\"The weather in Chorvoq is sunny today.\",\n",
    "\"Chorvoq is the capital of Narniya.\",\n",
    "\"Chorvoq hosted the Summer Olympics in 1900 and 1924.\",\n",
    "\"Suvsambil uses the Euro as well.\", # Redundant info\n",
    "\"It is cloudy in Nurik today.\"\n",
    "]\n",
    "query = \"What is the weather like in the capital of Narniya?\"\n",
    "# Parameters\n",
    "NUM_RETRIEVED_DOCS = len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The weather in the capital of Narniya, Chorvoq, is sunny today.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harness.target_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "model_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:   0%|          | 0/1 [00:00<?, ?it/s]/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1/1: What is the weather like in the capital of Narniya?... ---\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 11439.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'The weather in the capital of Narniya, Chorvoq, is sunny today.'\n",
      "Pre-computing utilities as they were not loaded.\n",
      "Starting pre-computation of utilities for 512 subsets using 1 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total utilities aggregated: 512/512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 9)\n",
      "(32, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 9)\n",
      "(64, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 9)\n",
      "(100, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q0\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 1/1 [00:31<00:00, 31.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--- Average Correlation Metrics Across All Questions ---\n",
      "                 Avg_Pearson  Avg_Spearman  Avg_Kendall  Avg_NDCG  \\\n",
      "Method                                                              \n",
      "WSS_FM100             0.9902        0.9500       0.8889    1.0000   \n",
      "WSS_BGAM64            0.9852        0.9500       0.8333    1.0000   \n",
      "ContextCite100        0.9734        0.9167       0.7778    0.9965   \n",
      "WSS_XGB100            0.9727        0.9667       0.8889    0.9965   \n",
      "WSS_BGAM100           0.9704        0.8167       0.7222    1.0000   \n",
      "WSS_FM64              0.9675        0.8500       0.7222    1.0000   \n",
      "TMC100                0.9554        0.9333       0.8333    0.9525   \n",
      "ContextCite64         0.9523        0.9667       0.8889    1.0000   \n",
      "TMC64                 0.9488        0.8833       0.7222    0.9525   \n",
      "WSS_XGB64             0.9341        0.9167       0.7778    0.9965   \n",
      "WSS_BGAM32            0.9105        0.9333       0.7778    0.9965   \n",
      "TMC32                 0.8766        0.9333       0.8333    0.8211   \n",
      "ContextCite32         0.8246        0.7833       0.6111    0.9965   \n",
      "LOO                   0.7992        0.5667       0.3333    0.9525   \n",
      "WSS_XGB32             0.7409        0.6167       0.5000    0.8259   \n",
      "BetaShap (U)64        0.6624        0.7333       0.5000    0.7888   \n",
      "BetaShap (U)100       0.5983        0.4167       0.3889    0.8211   \n",
      "BetaShap (U)32        0.4898        0.4667       0.2778    0.5509   \n",
      "WSS_FM32              0.3523        0.5000       0.3333    0.8731   \n",
      "\n",
      "                 Num_Valid_Queries  \n",
      "Method                              \n",
      "WSS_FM100                        1  \n",
      "WSS_BGAM64                       1  \n",
      "ContextCite100                   1  \n",
      "WSS_XGB100                       1  \n",
      "WSS_BGAM100                      1  \n",
      "WSS_FM64                         1  \n",
      "TMC100                           1  \n",
      "ContextCite64                    1  \n",
      "TMC64                            1  \n",
      "WSS_XGB64                        1  \n",
      "WSS_BGAM32                       1  \n",
      "TMC32                            1  \n",
      "ContextCite32                    1  \n",
      "LOO                              1  \n",
      "WSS_XGB32                        1  \n",
      "BetaShap (U)64                   1  \n",
      "BetaShap (U)100                  1  \n",
      "BetaShap (U)32                   1  \n",
      "WSS_FM32                         1  \n",
      "Script finished.\n",
      "Rank 0 (Local Main): Distributed environment not initialized or not available, skipping destroy_process_group.\n",
      "Script fully exited.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_questions_to_run=1\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "for i in tqdm(range(num_questions_to_run), desc=\"Processing Questions\", disable=not accelerator_main.is_main_process):\n",
    "    # query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    # docs=df.context[i]\n",
    "    # Initialize Harness\n",
    "    harness = ShapleyExperimentHarness(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        verbose=True,\n",
    "        utility_path=None\n",
    "    )\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        results_for_query[\"Exact\"] = harness.compute_exact_shap()\n",
    "\n",
    "        m_samples_map = {\"S\": 32, \"M\": 64, \"L\": 100} \n",
    "        T_iterations_map = {\"S\": 5, \"M\": 10, \"L\":20} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0: \n",
    "                results_for_query[f\"ContextCite{actual_samples}\"] = harness.compute_contextcite_weights(num_samples=actual_samples, sampling=\"kernelshap\", seed=SEED)\n",
    "                \n",
    "                results_for_query[f\"WSS_BGAM{actual_samples}\"] = harness.compute_wss(num_samples=actual_samples, seed=SEED, distil=None, sampling=\"kernelshap\",sur_type=\"boosted_gam\", util='pure-surrogate', pairchecking=False)\n",
    "                results_for_query[f\"WSS_FM{actual_samples}\"] = harness.compute_wss(num_samples=actual_samples, seed=SEED, distil=None, sampling=\"kernelshap\",sur_type=\"fm\", util='pure-surrogate', pairchecking=False)\n",
    "                results_for_query[f\"WSS_XGB{actual_samples}\"] = harness.compute_wss(num_samples=actual_samples, seed=SEED, distil=None, sampling=\"kernelshap\",sur_type=\"xgboost\", util='pure-surrogate', pairchecking=False)\n",
    "                results_for_query[f\"BetaShap (U){actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=0.5, beta_b=0.5, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "\n",
    "        exact_scores = results_for_query.get(\"Exact\")\n",
    "        all_results.append(results_for_query)\n",
    "        if exact_scores is not None:\n",
    "            positive_exact_score = np.clip(exact_scores, a_min=0.0, a_max=None) # FOR NDGC SCORE COMPUTATION\n",
    "            for method, approx_scores in results_for_query.items():\n",
    "                if method != \"Exact\" and approx_scores is not None:\n",
    "                    if len(approx_scores) == len(exact_scores):\n",
    "                        if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                            pearson_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                            spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                        else:\n",
    "                            pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                            spearman_c, _ = spearmanr(exact_scores, approx_scores)\n",
    "                            exact_ranks = rankdata(-np.array(exact_scores), method=\"average\") # rank scores with the smallest =1 and when there is a tie assign the average rank\n",
    "                            approx_ranks = rankdata(-np.array(approx_scores), method = \"average\")\n",
    "                            kendall_c, _ = kendalltau(exact_ranks, approx_ranks) # return tau and pval (if pval is < 0.005 we can say that correlation is statistically significant) \n",
    "                        ndgc_scoring  = ndcg_score(\n",
    "                            [positive_exact_score], \n",
    "                            [approx_scores],\n",
    "                            k = 3 # focus on top k document scoring\n",
    "                        )\n",
    "                        \n",
    "                        all_metrics_data.append({\n",
    "                            \"Question_Index\": i, \"Query\": query, \"Method\": method,\n",
    "                            \"Pearson\": pearson_c, \"Spearman\": spearman_c, \"NDCG\" : ndgc_scoring, \"KendallTau\" : kendall_c,\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"    Score length mismatch for method {method} (Exact: {len(exact_scores)}, Approx: {len(approx_scores)}). Skipping metrics.\")\n",
    "        else:\n",
    "            print(f\"    Skipping metric calculation for Q{i} as Exact Shapley was not computed or failed.\")\n",
    "    \n",
    "    accelerator_main.wait_for_everyone() \n",
    "   \n",
    "    if torch.cuda.is_available():\n",
    "        if accelerator_main.is_main_process: # Print from one process\n",
    "            print(f\"Attempting to empty CUDA cache on rank {accelerator_main.process_index} after Q{i}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if accelerator_main.is_main_process:\n",
    "            print(f\"CUDA cache empty attempt complete on rank {accelerator_main.process_index}.\")\n",
    "    accelerator_main.wait_for_everyone()\n",
    "\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    if all_metrics_data:\n",
    "        metrics_df_all_questions = pd.DataFrame(all_metrics_data)\n",
    "        print(\"\\n\\n--- Average Correlation Metrics Across All Questions ---\")\n",
    "        average_metrics = metrics_df_all_questions.groupby(\"Method\").agg(\n",
    "            Avg_Pearson=(\"Pearson\", \"mean\"),\n",
    "            Avg_Spearman=(\"Spearman\", \"mean\"),\n",
    "            Avg_Kendall =(\"KendallTau\", \"mean\"),\n",
    "            Avg_NDCG = (\"NDCG\", \"mean\"),\n",
    "            Num_Valid_Queries=(\"Query\", \"nunique\")\n",
    "        ).sort_values(by=\"Avg_Pearson\", ascending=False)\n",
    "        \n",
    "        print(average_metrics.round(4))\n",
    "    else:\n",
    "        print(\"\\nNo metrics were collected. This might be due to all calculations failing or only non-main processes running sections.\")\n",
    "\n",
    "# Final synchronization before script ends\n",
    "accelerator_main.wait_for_everyone()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Script finished.\")\n",
    "\n",
    "if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Manually destroying process group...\")\n",
    "    torch.distributed.destroy_process_group()\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Process group destroyed.\")\n",
    "else:\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Distributed environment not initialized or not available, skipping destroy_process_group.\")\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Script fully exited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exhaustive Top-3 Search: 100%|██████████| 84/84 [00:00<00:00, 11007.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 4, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harness.compute_exhaustive_top_k_removal_influence(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "all_metrics_data = []\n",
    "exact_scores = results_for_query.get(\"Exact\")\n",
    "if exact_scores is not None:\n",
    "    positive_exact_score = np.clip(exact_scores, a_min=0.0, a_max=None)\n",
    "    for method, approx_scores in results_for_query.items():\n",
    "        if method != \"Exact\" and approx_scores is not None and len(approx_scores) == len(exact_scores):\n",
    "            if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                pearson_c = spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "            else:\n",
    "                pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                spearman_c, _ = spearmanr(exact_scores, approx_scores)\n",
    "                exact_ranks = rankdata(-np.array(exact_scores), method=\"average\")\n",
    "                approx_ranks = rankdata(-np.array(approx_scores), method=\"average\")\n",
    "                kendall_c, _ = kendalltau(exact_ranks, approx_ranks)\n",
    "            ndgc_scoring = ndcg_score([positive_exact_score], [approx_scores], k=3)\n",
    "\n",
    "            all_metrics_data.append({\n",
    "                    \"Method\": method,\n",
    "                \"Pearson\": pearson_c, \"Spearman\": spearman_c, \"NDCG\": ndgc_scoring, \"KendallTau\": kendall_c\n",
    "            })\n",
    "            all_metrics_data.sort(key=lambda x: x[\"Pearson\"], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_all_questions = pd.DataFrame(all_metrics_data)\n",
    "\n",
    "print(\"\\n\\n============================\")\n",
    "print(\"     Correlation Metrics\")\n",
    "print(\"============================\")\n",
    "print(metrics_df_all_questions.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.question[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.context[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"     Approximate Scores\")\n",
    "print(\"============================\")\n",
    "for i in all_results:\n",
    "    for method, approx_scores in i.items():\n",
    "        if approx_scores is not None:\n",
    "            print(f\"\\nMethod: {method}\")\n",
    "            print(np.round(approx_scores, 4))\n",
    "    accelerator_main.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
