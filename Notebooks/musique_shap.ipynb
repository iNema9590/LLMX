{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(lst):\n",
    "    # Titles where is_supporting is True\n",
    "    supporting = [d['paragraph_text'] for d in lst if d.get('is_supporting') == True]\n",
    "    # Titles where is_supporting is False or missing AND not already in supporting\n",
    "    others = [d['paragraph_text'] for d in lst if d.get('is_supporting') != True and d['paragraph_text'] not in supporting]\n",
    "    # Combine: all supporting + as many others as needed to reach 10\n",
    "    result = supporting + others\n",
    "    return result[:10]\n",
    "\n",
    "df.paragraphs=df.paragraphs.apply(get_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.answer[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_questions_to_run=len(df.question)\n",
    "num_questions_to_run=20\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "pairs=[]\n",
    "Fs=[]\n",
    "Ms=[]\n",
    "for i in tqdm(range(num_questions_to_run), desc=\"Processing Questions\", disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs=df.paragraphs[i]\n",
    "\n",
    "    utility_cache_base_dir = \"../Experiment_data/musique\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}_n{len(docs)}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "        print(f\"  Instantiating ShapleyExperimentHarness for Q{i} (n={len(docs)} docs)...\")\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ShapleyExperimentHarness(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        verbose=True,\n",
    "        utility_path=current_utility_path\n",
    "    )\n",
    "    Ms.append(harness.compute_shapley_interaction_index_pairs_matrix())\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        results_for_query[\"ExactLinear\"] = harness.compute_exact_linear_shap()\n",
    "        results_for_query[\"ExactInter\"], pair = harness.compute_exact_inter_shap()\n",
    "        pairs.append(pair)\n",
    "        m_samples_map = {\"S\": 32, \"M\": 64, \"L\": 100} \n",
    "        T_iterations_map = {\"S\": 5, \"M\": 10, \"L\":20} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0: \n",
    "                results_for_query[f\"ContextCite{actual_samples}\"] = harness.compute_contextcite_weights(num_samples=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"WSS_FM{actual_samples}\"], F = harness.compute_wss(num_samples=actual_samples, seed=SEED, distil=None, sampling=\"kernelshap\",sur_type=\"fm\", util='pure-surrogate', pairchecking=False)\n",
    "                Fs.append(F)\n",
    "                results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=0.5, beta_b=0.5, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "\n",
    "        exact_scores = results_for_query.get(\"ExactInter\")\n",
    "        all_results.append(results_for_query)\n",
    "        if exact_scores is not None:\n",
    "            positive_exact_score = np.clip(exact_scores, a_min=0.0, a_max=None) # FOR NDGC SCORE COMPUTATION\n",
    "            for method, approx_scores in results_for_query.items():\n",
    "                if method != \"Exact\" and approx_scores is not None:\n",
    "                    if len(approx_scores) == len(exact_scores):\n",
    "                        if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                            pearson_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                            spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                        else:\n",
    "                            pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                            exact_ranks = rankdata(-np.array(exact_scores), method=\"average\") # rank scores with the smallest =1 and when there is a tie assign the average rank\n",
    "                            approx_ranks = rankdata(-np.array(approx_scores), method = \"average\")\n",
    "                            kendall_c, _ = kendalltau(exact_ranks, approx_ranks) # return tau and pval (if pval is < 0.005 we can say that correlation is statistically significant) \n",
    "                        \n",
    "                        all_metrics_data.append({\n",
    "                            \"Question_Index\": i, \"Query\": query, \"Method\": method,\n",
    "                            \"Pearson\": pearson_c, \"KendallTau\" : kendall_c,\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"    Score length mismatch for method {method} (Exact: {len(exact_scores)}, Approx: {len(approx_scores)}). Skipping metrics.\")\n",
    "        else:\n",
    "            print(f\"    Skipping metric calculation for Q{i} as Exact Shapley was not computed or failed.\")\n",
    "    \n",
    "    accelerator_main.wait_for_everyone() \n",
    "   \n",
    "    if torch.cuda.is_available():\n",
    "        if accelerator_main.is_main_process: # Print from one process\n",
    "            print(f\"Attempting to empty CUDA cache on rank {accelerator_main.process_index} after Q{i}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if accelerator_main.is_main_process:\n",
    "            print(f\"CUDA cache empty attempt complete on rank {accelerator_main.process_index}.\")\n",
    "    accelerator_main.wait_for_everyone()\n",
    "\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    if all_metrics_data:\n",
    "        metrics_df_all_questions = pd.DataFrame(all_metrics_data)\n",
    "        print(\"\\n\\n--- Average Correlation Metrics Across All Questions ---\")\n",
    "        average_metrics = metrics_df_all_questions.groupby(\"Method\").agg(\n",
    "            Avg_Pearson=(\"Pearson\", \"mean\"),\n",
    "            Avg_Kendall =(\"KendallTau\", \"mean\"),\n",
    "            Num_Valid_Queries=(\"Query\", \"nunique\")\n",
    "        ).sort_values(by=\"Avg_Pearson\", ascending=False)\n",
    "        \n",
    "        print(average_metrics.round(4))\n",
    "    else:\n",
    "        print(\"\\nNo metrics were collected. This might be due to all calculations failing or only non-main processes running sections.\")\n",
    "\n",
    "# Final synchronization before script ends\n",
    "accelerator_main.wait_for_everyone()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Script finished.\")\n",
    "\n",
    "if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Manually destroying process group...\")\n",
    "    torch.distributed.destroy_process_group()\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Process group destroyed.\")\n",
    "else:\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Distributed environment not initialized or not available, skipping destroy_process_group.\")\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Script fully exited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.answer[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.paragraphs[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.paragraphs[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(\"../Experiment_data/musique/utilities_q_idx4_n10.pkl\")[(0, 1, 0, 0, 0, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "method_scores = {}\n",
    "\n",
    "for result in all_results:\n",
    "    for method, scores in result.items():\n",
    "        if scores is not None:\n",
    "            method_scores[method] = np.round(scores, 4)\n",
    "\n",
    "for method, scores in method_scores.items():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(scores)), scores, color='skyblue')\n",
    "    plt.title(f\"Approximate Scores: {method}\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(len(scores)))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
