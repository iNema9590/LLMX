{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(lst):\n",
    "    # Titles where is_supporting is True\n",
    "    supporting = [d['paragraph_text'] for d in lst if d.get('is_supporting') == True]\n",
    "    # Titles where is_supporting is False or missing AND not already in supporting\n",
    "    others = [d['paragraph_text'] for d in lst if d.get('is_supporting') != True and d['paragraph_text'] not in supporting]\n",
    "    # Combine: all supporting + as many others as needed to reach 10\n",
    "    result = supporting + others\n",
    "    return result[:10]\n",
    "\n",
    "df.paragraphs=df.paragraphs.apply(get_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.answer[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1/20: When was the institute that owned The Collegian founded?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q0 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 11037.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: '1960'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx0_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q0\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:   5%|▌         | 1/20 [00:02<00:48,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 2/20: What year saw the creation of the region where the county of... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q1 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: '1994'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx1_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  10%|█         | 2/20 [00:03<00:26,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "--- Question 3/20: When was the abolishment of the studio that distributed The ... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q2 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: '1999'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx2_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  15%|█▌        | 3/20 [00:03<00:19,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "--- Question 4/20: When was the publisher of Crux launched?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q3 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: '2001'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx3_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q3\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  20%|██        | 4/20 [00:04<00:15,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 5/20: Jan Šindel's was born in what country?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q4 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'Czech Republic.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx4_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q4\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  25%|██▌       | 5/20 [00:05<00:13,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 6/20: What city is the person who broadened the doctrine of philos... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q5 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'Copenhagen.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx5_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q5\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  30%|███       | 6/20 [00:06<00:12,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 7/20: When was the baseball team winning the world series in 2015 ... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q6 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: '1969.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx6_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q6\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  35%|███▌      | 7/20 [00:07<00:11,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 8/20: Where did the Baldevins bryllup director die?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q7 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'I couldn't find any information about the director of Baldevins bryllup dying.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx7_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q7\n",
      "CUDA cache empty attempt complete on rank 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  40%|████      | 8/20 [00:08<00:12,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 9/20: Who was thee first president of the association that wrote t... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q8 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'G. Stanley Hall.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx8_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  45%|████▌     | 9/20 [00:09<00:11,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "--- Question 10/20: Which major Russian city borders the body of water in which ... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q9 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'The Baltic Sea.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx9_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q9\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  50%|█████     | 10/20 [00:10<00:09,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 11/20: When was the employer of John J. Collins established?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q10 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'Yale Divinity School.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx10_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  55%|█████▌    | 11/20 [00:11<00:08,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "--- Question 12/20: When did Bush declare the war causing Kerry to criticize him... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q11 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'March 2003.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx11_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q11\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  60%|██████    | 12/20 [00:12<00:08,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 13/20: What is the college Francis Walsingham attended an instance ... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q12 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'King's College.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx12_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q12\n",
      "CUDA cache empty attempt complete on rank 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  65%|██████▌   | 13/20 [00:13<00:06,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 14/20: What type of university is the college Kyeon Mi-ri attended?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q13 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'Private university.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx13_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q13\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  70%|███████   | 14/20 [00:13<00:05,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 15/20: In what year was the author of The Insider's Guide to the Co... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q14 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: '1878'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx14_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q14\n",
      "CUDA cache empty attempt complete on rank 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  75%|███████▌  | 15/20 [00:14<00:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Question 16/20: When was the territory covered by RIBA's Cambridge branch of... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q15 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: '1994'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx15_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  80%|████████  | 16/20 [00:15<00:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "--- Question 17/20: What's the meaning of the name of the school that does not i... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q16 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'Theravada.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx16_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  85%|████████▌ | 17/20 [00:16<00:02,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "--- Question 18/20: Where did the director who provided the lyrics to A Time for... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q17 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'University College London.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx17_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  90%|█████████ | 18/20 [00:17<00:01,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "--- Question 19/20: When did the country formerly known as Zaire become independ... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q18 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: '1960.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx18_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  95%|█████████▌| 19/20 [00:18<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "--- Question 20/20: Where did Peter and Paul Fortress' designer die?... ---\n",
      "  Instantiating ShapleyExperimentHarness for Q19 (n=10 docs)...\n",
      "Generating target response based on full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target response: 'Domenico Trezzini.'\n",
      "Successfully loaded utilities from ../Experiment_data/synergy/utilities_q_idx19_n10.pkl. Found 1024 entries.\n",
      "Broadcasted loaded utilities to all processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to empty CUDA cache on rank 0 after Q19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 20/20 [00:18<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache empty attempt complete on rank 0.\n",
      "\n",
      "\n",
      "--- Average Correlation Metrics Across All Questions ---\n",
      "                Avg_Pearson  Avg_Kendall  Num_Valid_Queries\n",
      "Method                                                     \n",
      "ExactInter           1.0000       1.0000                 20\n",
      "ExactLinear          1.0000       1.0000                 20\n",
      "ContextCite100       0.9951       0.7622                 20\n",
      "WSS_FM100            0.9943       0.7200                 20\n",
      "ContextCite64        0.9899       0.7422                 20\n",
      "WSS_FM64             0.9891       0.6933                 20\n",
      "TMC100               0.9838       0.6422                 20\n",
      "TMC64                0.9779       0.5933                 20\n",
      "LOO                  0.9741       0.4778                 20\n",
      "ContextCite32        0.9552       0.6778                 20\n",
      "TMC32                0.9486       0.5222                 20\n",
      "WSS_FM32             0.9154       0.4867                 20\n",
      "BetaShap100          0.8623       0.4067                 20\n",
      "BetaShap32           0.8339       0.4133                 20\n",
      "BetaShap64           0.8334       0.4533                 20\n",
      "Script finished.\n",
      "Rank 0 (Local Main): Distributed environment not initialized or not available, skipping destroy_process_group.\n",
      "Script fully exited.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# num_questions_to_run=len(df.question)\n",
    "num_questions_to_run=20\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "for i in tqdm(range(num_questions_to_run), desc=\"Processing Questions\", disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs=df.paragraphs[i]\n",
    "\n",
    "    utility_cache_base_dir = \"../Experiment_data/synergy\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}_n{len(docs)}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "        print(f\"  Instantiating ShapleyExperimentHarness for Q{i} (n={len(docs)} docs)...\")\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ShapleyExperimentHarness(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        verbose=True,\n",
    "        utility_path=current_utility_path\n",
    "    )\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        results_for_query[\"ExactLinear\"] = harness.compute_exact_linear_shap()\n",
    "        results_for_query[\"ExactInter\"], pairs = harness.compute_exact_inter_shap()\n",
    "\n",
    "        m_samples_map = {\"S\": 32, \"M\": 64, \"L\": 100} \n",
    "        T_iterations_map = {\"S\": 5, \"M\": 10, \"L\":20} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0: \n",
    "                results_for_query[f\"ContextCite{actual_samples}\"] = harness.compute_contextcite_weights(num_samples=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"WSS_FM{actual_samples}\"], F = harness.compute_wss(num_samples=actual_samples, seed=SEED, distil=None, sampling=\"kernelshap\",sur_type=\"fm\", util='pure-surrogate', pairchecking=False)\n",
    "                results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=0.5, beta_b=0.5, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "\n",
    "        exact_scores = results_for_query.get(\"ExactInter\")\n",
    "        all_results.append(results_for_query)\n",
    "        if exact_scores is not None:\n",
    "            positive_exact_score = np.clip(exact_scores, a_min=0.0, a_max=None) # FOR NDGC SCORE COMPUTATION\n",
    "            for method, approx_scores in results_for_query.items():\n",
    "                if method != \"Exact\" and approx_scores is not None:\n",
    "                    if len(approx_scores) == len(exact_scores):\n",
    "                        if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                            pearson_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                            spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                        else:\n",
    "                            pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                            exact_ranks = rankdata(-np.array(exact_scores), method=\"average\") # rank scores with the smallest =1 and when there is a tie assign the average rank\n",
    "                            approx_ranks = rankdata(-np.array(approx_scores), method = \"average\")\n",
    "                            kendall_c, _ = kendalltau(exact_ranks, approx_ranks) # return tau and pval (if pval is < 0.005 we can say that correlation is statistically significant) \n",
    "                        \n",
    "                        all_metrics_data.append({\n",
    "                            \"Question_Index\": i, \"Query\": query, \"Method\": method,\n",
    "                            \"Pearson\": pearson_c, \"KendallTau\" : kendall_c,\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"    Score length mismatch for method {method} (Exact: {len(exact_scores)}, Approx: {len(approx_scores)}). Skipping metrics.\")\n",
    "        else:\n",
    "            print(f\"    Skipping metric calculation for Q{i} as Exact Shapley was not computed or failed.\")\n",
    "    \n",
    "    accelerator_main.wait_for_everyone() \n",
    "   \n",
    "    if torch.cuda.is_available():\n",
    "        if accelerator_main.is_main_process: # Print from one process\n",
    "            print(f\"Attempting to empty CUDA cache on rank {accelerator_main.process_index} after Q{i}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if accelerator_main.is_main_process:\n",
    "            print(f\"CUDA cache empty attempt complete on rank {accelerator_main.process_index}.\")\n",
    "    accelerator_main.wait_for_everyone()\n",
    "\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    if all_metrics_data:\n",
    "        metrics_df_all_questions = pd.DataFrame(all_metrics_data)\n",
    "        print(\"\\n\\n--- Average Correlation Metrics Across All Questions ---\")\n",
    "        average_metrics = metrics_df_all_questions.groupby(\"Method\").agg(\n",
    "            Avg_Pearson=(\"Pearson\", \"mean\"),\n",
    "            Avg_Kendall =(\"KendallTau\", \"mean\"),\n",
    "            Num_Valid_Queries=(\"Query\", \"nunique\")\n",
    "        ).sort_values(by=\"Avg_Pearson\", ascending=False)\n",
    "        \n",
    "        print(average_metrics.round(4))\n",
    "    else:\n",
    "        print(\"\\nNo metrics were collected. This might be due to all calculations failing or only non-main processes running sections.\")\n",
    "\n",
    "# Final synchronization before script ends\n",
    "accelerator_main.wait_for_everyone()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Script finished.\")\n",
    "\n",
    "if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Manually destroying process group...\")\n",
    "    torch.distributed.destroy_process_group()\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Process group destroyed.\")\n",
    "else:\n",
    "    if accelerator_main.is_local_main_process:\n",
    "        print(f\"Rank {accelerator_main.process_index} (Local Main): Distributed environment not initialized or not available, skipping destroy_process_group.\")\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Script fully exited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "method_scores = {}\n",
    "\n",
    "for result in all_results:\n",
    "    for method, scores in result.items():\n",
    "        if scores is not None:\n",
    "            method_scores[method] = np.round(scores, 4)\n",
    "\n",
    "for method, scores in method_scores.items():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(scores)), scores, color='skyblue')\n",
    "    plt.title(f\"Approximate Scores: {method}\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(len(scores)))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
