{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *\n",
    "from SHapRAG.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed color palette per method family/name\n",
    "METHOD_COLORS = {\n",
    "    \"FM\": \"#1f77b4\",\n",
    "    \"FMW\": \"#ff7f0e\",\n",
    "    \"Spex\": \"#2ca02c\",\n",
    "    \"Shapiq\": \"#d62728\",\n",
    "    \"Flk\": \"#9467bd\",\n",
    "    \"Exact-FSII\": \"#8c564b\",\n",
    "    \"Exact-Shap\": \"#e377c2\",\n",
    "    \"LOO\": \"#7f7f7f\",\n",
    "    \"ARC-JSD\": \"#bcbd22\",\n",
    "    \"ContextCite\": \"#17becf\",\n",
    "    \"default\": \"#17becf\",\n",
    "}\n",
    "\n",
    "def family_of(method_key: str) -> str:\n",
    "    \"\"\"Map a notebook method key to a color family key.\"\"\"\n",
    "    if method_key.startswith(\"FMW\"):\n",
    "        return \"FMW\"\n",
    "    if method_key.startswith(\"FM\"):\n",
    "        return \"FM\"\n",
    "    if method_key.startswith(\"Spex\"):\n",
    "        return \"Spex\"\n",
    "    if method_key.startswith(\"Shapiq\"):\n",
    "        return \"Shapiq\"\n",
    "    if method_key.startswith(\"Flk\") or method_key.startswith(\"Flu\"):\n",
    "        return \"Flk\"\n",
    "    if method_key.startswith(\"ContextCite\"):\n",
    "        return \"ContextCite\"\n",
    "    if method_key in (\"Exact-FSII\", \"Exact-Shap\", \"LOO\", \"ARC-JSD\"):\n",
    "        return method_key\n",
    "    return \"default\"\n",
    "\n",
    "def get_color(method_key: str) -> str:\n",
    "    return METHOD_COLORS.get(family_of(method_key), METHOD_COLORS[\"default\"])\n",
    "\n",
    "# Set matplotlib default cycle to the custom palette (keeps plotting consistent)\n",
    "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(color=list(METHOD_COLORS.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(lst):\n",
    "    # Titles where is_supporting is True\n",
    "    supporting = [d['paragraph_text'] for d in lst if d.get('is_supporting') == True]\n",
    "    # Titles where is_supporting is False or missing AND not already in supporting\n",
    "    others = [d['paragraph_text'] for d in lst if d.get('is_supporting') != True and d['paragraph_text'] not in supporting]\n",
    "    # Combine: all supporting + as many others as needed to reach 10\n",
    "    result = supporting + others\n",
    "    return result[:10]\n",
    "\n",
    "df.paragraphs=df.paragraphs.apply(get_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Sentences'] = df['paragraphs'].apply(\n",
    "#     lambda para_list: [sent for para in para_list for sent in nltk.sent_tokenize(para)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_save=pd.read_csv('../data/musique/sen_labeled.csv',\n",
    "#     quotechar='\"',\n",
    "#     skipinitialspace=True,\n",
    "#     engine='python' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"paragraphs\"] = df[\"paragraphs\"].apply(lambda p: p[:5]+ [p[1]] + p[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answered.txt\", \"r\") as f:\n",
    "    res = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtset_k():\n",
    "    return [0, 1,5]\n",
    "\n",
    "num_questions_to_run = 50\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "all_results = []\n",
    "extras = []\n",
    "\n",
    "for i in range(num_questions_to_run):\n",
    "    query = df.question[i]\n",
    "    # if res[i]==\"True\":\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs = df.paragraphs[i]\n",
    "    utility_cache_base_dir = f\"../Experiment_data/musique/{model_path.split('/')[1]}/duplicate\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\"\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model=prepared_model,\n",
    "        prepared_tokenizer=tokenizer,\n",
    "        accelerator=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "    \n",
    "    full_budget=pow(2,harness.n_items)\n",
    "    # res = evaluate(df.question[i], harness.target_response, df.answer[i])\n",
    "    # res='True'\n",
    "    if accelerator_main.is_main_process:\n",
    "        methods_results = {}\n",
    "        metrics_results = {}\n",
    "        extra_results = {}\n",
    "\n",
    "        m_samples_map = {\"XS\": 32, \"S\":64, \"M\":128, \"L\":264, \"XL\":528, \"XXL\":724, \"XXXL\":1024}\n",
    "\n",
    "        # Store FM models for later RÂ²/MSE\n",
    "        fm_models = {}\n",
    "        methods_results['Exact-Shap']=harness._calculate_shapley()\n",
    "        for size_key, actual_samples in m_samples_map.items():\n",
    "            print(f\"Running sample size: {actual_samples}\")\n",
    "            methods_results[f\"ContextCite_{actual_samples}\"], fm_models[f\"ContextCite_{actual_samples}\"] = harness.compute_contextcite(\n",
    "                num_samples=actual_samples, seed=SEED\n",
    "            )\n",
    "            # FM Weights (loop over ranks 0â€“5)\n",
    "            # for rank in[1,2,4,8, 10]:\n",
    "                # methods_results[f\"FM_WeightsLK_{rank}_{actual_samples}\"], extra_results[f\"Flk_{rank}_{actual_samples}\"], fm_models[f\"FM_WeightsLK_{rank}_{actual_samples}\"] = harness.compute_wss(\n",
    "                #     num_samples=actual_samples,\n",
    "                #     seed=SEED,\n",
    "                #     sampling=\"kernelshap\",\n",
    "                #     sur_type=\"fm\",\n",
    "                # amples}\"], extra_results[f\"Flu_{rank}_{actual_samples}\"], fm_models[f\"FM_WeightsLU_{rank}_{actual_samples}\"] = harness.compute_wss(\n",
    "                #     num_samples=actual_samples,\n",
    "                #     seed=SEED,\n",
    "                #     sampling=\"kernelshap\",\n",
    "                #     sur_type=\"fm\",\n",
    "                #     rank=rank\n",
    "                # )\n",
    "            methods_results[f\"FMW_{actual_samples}\"], extra_results[f\"FMW_{actual_samples}\"], fm_models[f\"FMW_{actual_samples}\"] = harness.compute_wss(\n",
    "                    num_samples=actual_samples,\n",
    "                    seed=SEED,\n",
    "                    sampling=\"kernelshap\",\n",
    "                    sur_type=\"fm\")\n",
    "            methods_results[f\"FM_{actual_samples}\"], extra_results[f\"FM_{actual_samples}\"], fm_models[f\"FM_{actual_samples}\"] = harness.compute_wss(\n",
    "                    num_samples=actual_samples,\n",
    "                    seed=SEED,\n",
    "                    sampling=\"kernelshap\",\n",
    "                    sur_type=\"fm_ft\")\n",
    "                    \n",
    "            # FM models with dynamic k pruning\n",
    "            # methods_results[f\"FM_k_dynamic_{actual_samples}\"], extra_results[f\"FM_k_dynamic_{actual_samples}\"], fm_models[f\"FM_k_dynamic_{actual_samples}\"] = harness.compute_wss_dynamic_pruning_reuse_utility(\n",
    "            #     num_samples=actual_samples, \n",
    "            #     initial_rank=1, \n",
    "            #     final_rank=2,\n",
    "            # )\n",
    "            attributionshapiq, interactionshapiq, fm_models[f\"Shapiq_{actual_samples}\"] = harness.compute_shapiq_fsii(budget=actual_samples)\n",
    "            methods_results[f\"Shapiq_{actual_samples}\"] = attributionshapiq\n",
    "            extra_results.update({\n",
    "                f\"Shapiq_{actual_samples}\":interactionshapiq\n",
    "                                                                        })\n",
    "            try:\n",
    "                attributionshap, interactionshap, fm_models[f\"Spex_{actual_samples}\"] = harness.compute_fsii(sample_budget=actual_samples, max_order=harness.n_items)\n",
    "                # attributionban, interactionban, fm_models[f\"FBII_{actual_samples}\"] = harness.compute_fbii(sample_budget=actual_samples, max_order=harness.n_items)\n",
    "                # methods_results[f\"FBII_{actual_samples}\"] = attributionban\n",
    "                methods_results[f\"Spex_{actual_samples}\"] = attributionshap\n",
    "\n",
    "                extra_results.update({\n",
    "                f\"Spex_{actual_samples}\":interactionshap\n",
    "                                                                        })\n",
    "            except Exception: pass\n",
    "\n",
    "\n",
    "    #     methods_results[\"LOO\"] = harness.compute_loo()\n",
    "    #     methods_results[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "        attributionxs, interactionxs, fm_models[\"Exact-FSII\"] = harness.compute_exact_fsii(max_order=2)\n",
    "\n",
    "        extra_results.update({\n",
    "        \"Exact-FSII\": interactionxs\n",
    "    })\n",
    "        methods_results[\"Exact-FSII\"]=attributionxs\n",
    "\n",
    "        # --- Evaluation Metrics ---\n",
    "        metrics_results[\"topk_probability\"] = harness.evaluate_topk_performance(\n",
    "            methods_results, fm_models, k_values\n",
    "        )\n",
    "\n",
    "        # RÂ² and Delta RÂ²\n",
    "        metrics_results[\"R2\"] = harness.r2(methods_results,100,mode='log-perplexity', models=fm_models)\n",
    "        metrics_results[\"Delta_R2\"] = harness.delta_r2(methods_results,100,mode='log-perplexity', models=fm_models)\n",
    "        metrics_results['Recall']=harness.recall_at_k(gtset_k(), methods_results, k_values)\n",
    "\n",
    "        # LDS per method\n",
    "        metrics_results[\"LDS\"] = harness.lds(methods_results,50,mode='log-perplexity', models=fm_models)\n",
    "\n",
    "\n",
    "\n",
    "        all_results.append({\n",
    "            \"query_index\": i,\n",
    "            \"query\": query,\n",
    "            \"ground_truth\": df.answer[i],\n",
    "            \"response\": harness.target_response,\n",
    "            \"methods\": methods_results,\n",
    "            \"metrics\": metrics_results\n",
    "        })\n",
    "        extras.append(extra_results)\n",
    "\n",
    "        # Save utility cache\n",
    "        harness.save_utility_cache(current_utility_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(harness.utility_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{utility_cache_base_dir}/results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "with open(f\"{utility_cache_base_dir}/extras.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "with open(f\"../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "with open(f\"../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/extras.pkl\", \"rb\") as f:\n",
    "    extras = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/extras.pkl\", \"rb\") as f:\n",
    "    extras = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "spearmans = {i: [] for i in all_results[0]['methods'] if \"Exact\" not in i}\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for method_res in all_results:\n",
    "    for method, attribution in method_res['methods'].items():\n",
    "        if \"Exact\" not in method:\n",
    "            # Convert to numpy arrays for scaling\n",
    "            ref = np.array(method_res['methods'][\"Exact-Shap\"]).reshape(-1, 1)\n",
    "            att = np.array(attribution).reshape(-1, 1)\n",
    "            \n",
    "            # Scale both reference and attribution to [0, 1]\n",
    "            ref_scaled = scaler.fit_transform(ref).flatten()\n",
    "            att_scaled = scaler.fit_transform(att).flatten()\n",
    "            \n",
    "            # Compute NDCG score\n",
    "            spear = ndcg_score([ref_scaled], [att_scaled])\n",
    "            spearmans[method].append(spear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parse methods and budgets\n",
    "parsed = {}\n",
    "budgets = set()\n",
    "for key, values in spearmans.items():\n",
    "    avg_val = np.mean(values)\n",
    "    \n",
    "    match = re.match(r\"(.+?)_(\\d+)$\", key)  # method_budget pattern\n",
    "    if match:\n",
    "        method, budget = match.groups()\n",
    "        budget = int(budget)\n",
    "        budgets.add(budget)\n",
    "        parsed.setdefault(method, {})[budget] = avg_val\n",
    "    else:\n",
    "        # constant methods (no budget)\n",
    "        parsed.setdefault(key, {})[None] = avg_val\n",
    "\n",
    "budgets = sorted(budgets)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for method, results in parsed.items():\n",
    "    if None in results:  # constant method\n",
    "        plt.hlines(results[None], xmin=min(budgets), xmax=max(budgets), \n",
    "                   linestyles='--', label=method)\n",
    "    else:\n",
    "        xs = sorted(results.keys())\n",
    "        ys = [results[b] for b in xs]\n",
    "        plt.plot(xs, ys, marker='o', label=method)\n",
    "\n",
    "plt.xlabel(\"Budget\")\n",
    "plt.ylabel(\"Average Spearman\")\n",
    "plt.title(\"Average NDCG to Exact Shapley per Method vs Budget\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_and_print(all_results, k_values=[1, 2, 3,4,5]):\n",
    "    table_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Mapping for consistency\n",
    "    method_name_map = {\n",
    "        \n",
    "    }\n",
    "\n",
    "    for res in all_results:\n",
    "        metrics = res[\"metrics\"]\n",
    "        # LDS and R2\n",
    "        for method_name, lds_val in metrics.get(\"LDS\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"LDS\"].append(lds_val)\n",
    "\n",
    "        for method_name, r2_val in metrics.get(\"R2\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"R2\"].append(r2_val)\n",
    "\n",
    "        # Delta R2 (new)\n",
    "        for method_name, delta_val in metrics.get(\"Delta_R2\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"Delta_R2\"].append(delta_val)\n",
    "\n",
    "        # Top-k\n",
    "        for method_name, k_dict in metrics.get(\"topk_probability\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            for k in k_values:\n",
    "                if k in k_dict:\n",
    "                    col_name = f\"topk_probability_k{k}\"\n",
    "                    table_data[method][col_name].append(k_dict[k])\n",
    "        \n",
    "        for method_name, k_dict in metrics.get(\"Recall\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            for k in k_values:\n",
    "                col_name = f\"Recall@{k}\"\n",
    "                table_data[method][col_name].append(k_dict[k-1])\n",
    "\n",
    "    # Averages\n",
    "    avg_table = {\n",
    "        method: {metric: np.nanmean(values) for metric, values in metric_dict.items()}\n",
    "        for method, metric_dict in table_data.items()\n",
    "    }\n",
    "\n",
    "    # Standard deviations for LDS, RÂ², and Delta_R2\n",
    "    for method, metric_dict in table_data.items():\n",
    "        for metric in [\"LDS\", \"R2\", \"Delta_R2\"]:\n",
    "            if metric in metric_dict:\n",
    "                avg_table[method][f\"{metric}_std\"] = np.nanstd(metric_dict[metric])\n",
    "\n",
    "    df_summary = pd.DataFrame.from_dict(avg_table, orient=\"index\").sort_index()\n",
    "\n",
    "    print(\"\\n=== Metrics Summary Across All Queries ===\")\n",
    "    print(df_summary.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "    return df_summary\n",
    "df_res=summarize_and_print(all_results, k_values=[1, 2, 3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reset index\n",
    "df_reset = df_res.reset_index().rename(columns={'index': 'method'})\n",
    "\n",
    "# Separate constant methods (no budget) and budgeted methods\n",
    "constant_methods = ['LOO', 'ARC-JSD', 'Exact-FSII', 'Exact-Shap']\n",
    "df_const = df_reset[df_reset['method'].isin(constant_methods)]\n",
    "df_budgeted = df_reset[~df_reset['method'].isin(constant_methods)]\n",
    "\n",
    "# Extract family and budget for budgeted methods\n",
    "df_budgeted['family'] = df_budgeted['method'].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "df_budgeted['budget'] = df_budgeted['method'].apply(lambda x: int(x.split(\"_\")[-1]))\n",
    "df_budgeted = df_budgeted.sort_values(by=['family', 'budget'])\n",
    "\n",
    "# Function to plot metric\n",
    "def plot_metric(metric, ylabel):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot budgeted families\n",
    "    families = df_budgeted['family'].unique()\n",
    "    for fam in families:\n",
    "        # if 'LK' not in fam:\n",
    "        subset = df_budgeted[df_budgeted['family'] == fam]\n",
    "        plt.plot(subset['budget'], subset[metric], marker='o', label=fam)\n",
    "\n",
    "    # Plot constant methods as horizontal lines\n",
    "    colors = plt.cm.tab10.colors  # categorical palette\n",
    "    # for idx, (_, row) in enumerate(df_const.iterrows()):\n",
    "    #     plt.axhline(y=row[metric], color=colors[idx % len(colors)],marker='x', label=row['method'])\n",
    "\n",
    "    plt.xlabel(\"Budget\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"Evolution of {ylabel} with Increasing Budget\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metric(\"R2\", \"R2\")\n",
    "plot_metric(\"LDS\", \"LDS\")\n",
    "plot_metric(\"Delta_R2\", \"Delta R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter budgeted methods at budget = 274\n",
    "df_budgeted_264 = df_budgeted[df_budgeted['budget'] == 724]\n",
    "\n",
    "# Metrics to plot\n",
    "recall_metrics = [f\"Recall@{k}\" for k in range(1, 6)]\n",
    "k_values = list(range(1, 6))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot budgeted families at budget 274\n",
    "families = df_budgeted_264['family'].unique()\n",
    "for fam in families:\n",
    "    # if 'LK' not in fam:\n",
    "    subset = df_budgeted_264[df_budgeted_264['family'] == fam]\n",
    "    if not subset.empty:\n",
    "        recalls = subset[recall_metrics].values.flatten()\n",
    "        plt.plot(k_values, recalls, marker='o', label=fam)\n",
    "\n",
    "# Plot constant methods\n",
    "# for idx, (_, row) in enumerate(df_const.iterrows()):\n",
    "#     recalls = [row[m] for m in recall_metrics]\n",
    "#     plt.plot(k_values, recalls, marker='x', linestyle=\"--\", label=row['method'])\n",
    "\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall@k\")\n",
    "plt.title(\"Recall@k for Budget = 264\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter budgeted methods at budget = 724\n",
    "df_budgeted_724 = df_budgeted[df_budgeted['budget'] == 724]\n",
    "\n",
    "k_values = list(range(1, 6))\n",
    "\n",
    "# ðŸ‘‰ CHANGE THESE TWO LINES to match your actual column names\n",
    "recall_metrics    = [f\"Recall@{k}\" for k in k_values]      # if you have these\n",
    "precision_metrics = [f\"Precision@{k}\" for k in k_values]   # <-- adjust to real names\n",
    "\n",
    "print(\"Trying to use recall columns:\", recall_metrics)\n",
    "print(\"Trying to use precision columns:\", precision_metrics)\n",
    "\n",
    "# Sanity check: which of these actually exist?\n",
    "print(\"Existing columns:\", df_budgeted_724.columns.tolist())\n",
    "missing = [c for c in recall_metrics + precision_metrics if c not in df_budgeted_724.columns]\n",
    "print(\"Missing metric columns:\", missing)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "families = df_budgeted_724['family'].unique()\n",
    "for fam in families:\n",
    "    subset = df_budgeted_724[df_budgeted_724['family'] == fam]\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    # Average over possible multiple rows / seeds\n",
    "    precisions = subset[precision_metrics].mean(axis=0).values\n",
    "    recalls    = subset[recall_metrics].mean(axis=0).values\n",
    "\n",
    "    # Sort by recall\n",
    "    order = recalls.argsort()\n",
    "    recalls_sorted = recalls[order]\n",
    "    precisions_sorted = precisions[order]\n",
    "\n",
    "    plt.plot(recalls_sorted, precisions_sorted, marker='o', label=fam)\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precisionâ€“Recall Curve for Budget = 724\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "for method in df_res.index:\n",
    "    if \"264\" in method :\n",
    "        plt.plot(\n",
    "            [1, 2, 3,4,5],\n",
    "            df_res.loc[method, ['topk_probability_k1', 'topk_probability_k2', 'topk_probability_k3', 'topk_probability_k4', 'topk_probability_k5']],\n",
    "            marker='o',\n",
    "            label=method\n",
    "        )\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Logit-Probability Drop')\n",
    "plt.title('Top-k Logit-Probability Drop')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras[2].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exact match with human labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_methods(extras, k, m, interaction_type=\"max\"):\n",
    "\n",
    "    methods = extras[0].keys()\n",
    "    scores = {m: 0 for m in methods}\n",
    "    n_experiments = len(extras)\n",
    "\n",
    "    for exp in extras:\n",
    "        for method in methods:\n",
    "            if \"Fl\" in method or \"FM\" in method:\n",
    "                # Flu is a matrix\n",
    "                value = exp[method][k][m]\n",
    "                all_values = exp[method].flatten()\n",
    "            else:\n",
    "                # Dictionaries with tuple keys\n",
    "                d = exp[method]\n",
    "                value = None\n",
    "                for key, v in d.items():\n",
    "                    if key == (k, m):\n",
    "                        value = v\n",
    "                        break\n",
    "                if value is None:\n",
    "                    continue  # skip if (k,m) not found\n",
    "                all_values = list(d.values())\n",
    "\n",
    "            if interaction_type == \"max\":\n",
    "                if value == max(all_values):\n",
    "                    scores[method] += 1\n",
    "            elif interaction_type == \"min\":\n",
    "                if value == min(all_values):\n",
    "                    scores[method] += 1\n",
    "\n",
    "    # Convert to fraction of experiments\n",
    "    results = {method: scores[method] / n_experiments for method in methods}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovery rate\n",
    "em={}\n",
    "for i, j in enumerate(np.array(list(evaluate_methods(extras, k=0, m=1, interaction_type=\"max\").values()))+np.array(list(evaluate_methods(extras, k=0, m=5, interaction_type=\"max\").values()))+np.array(list(evaluate_methods(extras, k=1, m=5, interaction_type=\"min\").values()))):\n",
    "    em.update({list(extras[0].keys())[i]:j})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for k, v in em.items():\n",
    "    parts = k.split(\"_\")\n",
    "    if parts[0] == \"Flk\" and parts[1]!='0':\n",
    "        _, rank, budget = parts\n",
    "        rows.append({\"method\": \"Flk\", \"rank\": int(rank), \"budget\": int(budget), \"recovery\": v})\n",
    "    elif parts[0] == \"FM\"and parts[1]!='k':\n",
    "        _,budget = parts\n",
    "        rows.append({\"method\": f'FM', \"budget\": int(budget), \"recovery\": v})\n",
    "    elif parts[0] == \"FMW\":\n",
    "        _,budget = parts\n",
    "        rows.append({\"method\": f'FMW', \"budget\": int(budget), \"recovery\": v})\n",
    "    elif \"Spex\" in parts[0]:\n",
    "        name, budget = parts\n",
    "        rows.append({\"method\": name, \"budget\": int(budget), \"recovery\": v})\n",
    "\n",
    "    elif \"Shapiq\" in parts[0]:\n",
    "        name, budget = parts\n",
    "        rows.append({\"method\": name, \"budget\": int(budget), \"recovery\": v})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Flu (different ranks as lines)\n",
    "# for rank in sorted(df[df[\"method\"]==\"Flk\"][\"rank\"].unique()):\n",
    "#     sub = df[(df[\"method\"]==\"Flk\") & (df[\"rank\"]==rank)].sort_values(\"budget\")\n",
    "#     plt.plot(sub[\"budget\"], sub[\"recovery\"]/3, marker=\"o\", label=f\"Flk rank {rank}\")\n",
    "\n",
    "# for rank in sorted(df[df[\"method\"]==\"FM_k_dynamic\"][\"rank\"].unique()):\n",
    "#     sub = df[(df[\"method\"]==\"FM_k_dynamic\") & (df[\"rank\"]==rank)].sort_values(\"budget\")\n",
    "#     plt.plot(sub[\"budget\"], sub[\"recovery\"]/3, marker=\"o\", label=f\"FM_k_dynamic rank {rank}\")\n",
    "# for rank in sorted(df[df[\"method\"]==\"FM_r_dynamic\"]):\n",
    "sub1 = df[(df[\"method\"]==\"FM\")].sort_values(\"budget\")\n",
    "# sub2 = df[(df[\"method\"]==\"FMW\")].sort_values(\"budget\")\n",
    "plt.plot(sub1[\"budget\"], sub1[\"recovery\"]/3, marker=\"o\", label=f\"FM\")\n",
    "# plt.plot(sub2[\"budget\"], sub2[\"recovery\"]/3, marker=\"o\", label=f\"FMW\")\n",
    "plt.plot(sub1[\"budget\"],em['Exact-FSII']*np.ones(7)/3, marker=\"x\", linestyle=\"--\", label=f\"Exact-FSII\")\n",
    "\n",
    "# Plot Int methods (evolve with budget, start at 264)\n",
    "for m in df[df[\"method\"].isin([\"Spex\",\"FBII\",\"Shapiq\"])]['method'].unique():\n",
    "    sub = df[df[\"method\"]==m].sort_values(\"budget\")\n",
    "    plt.plot(sub[\"budget\"], sub[\"recovery\"]/3, marker=\"+\", linestyle=\"--\", label=f\"{m}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Budget\")\n",
    "plt.ylabel(\"Exact match\")\n",
    "plt.title(\"Evolution of Exact match with Increasing Budget\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. compare the new and old fm\n",
    "2. iteration with the interactions\n",
    "3. shapley is for first order and faithshap for pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RR@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rr_at_k(interaction, ground_truth, k):\n",
    "    \"\"\"\n",
    "    Compute Recovery@k for a method's interaction dict or matrix.\n",
    "    interaction: dict {(i, j): value} or 2D numpy array/matrix\n",
    "    ground_truth: set of ground-truth indices (R^*)\n",
    "    k: number of top interactions to consider\n",
    "    Returns: RR@k value\n",
    "    \"\"\"\n",
    "    # Convert matrix to dict if needed\n",
    "    if isinstance(interaction, (np.ndarray, list)):\n",
    "        mat = np.array(interaction)\n",
    "        pairs = {(i, j): mat[i][j] for i in range(mat.shape[0]) for j in range(mat.shape[1]) if i != j}\n",
    "    else:\n",
    "        pairs = interaction\n",
    "\n",
    "    # Sort pairs by value (descending)\n",
    "    sorted_pairs = sorted(pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    rr_sum = 0.0\n",
    "    for i in range(min(k, len(sorted_pairs))):\n",
    "        pair_indices = set(sorted_pairs[i][0])\n",
    "        rr_sum += len(ground_truth & pair_indices) / len(pair_indices)\n",
    "    return rr_sum / k if k > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = set([0, 1, 5])  # Example ground-truth indices\n",
    "k = 5  # Number of top interactions to consider\n",
    "\n",
    "rr_results = {}\n",
    "for method, interaction in extras[0].items():  # Use the correct experiment index\n",
    "    rr_results[method] = compute_rr_at_k(interaction, ground_truth, k)\n",
    "\n",
    "print(rr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Parse RR@k for all experiments and budgets\n",
    "def extract_budget(key):\n",
    "    match = re.search(r'_(\\d+)$', key)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_family(key):\n",
    "    if key.startswith(\"FMW\"):\n",
    "        return \"FMW\"\n",
    "    elif key.startswith(\"FM\"):\n",
    "        return \"FM\"\n",
    "    elif key.startswith(\"Spex\"):\n",
    "        return \"Spex\"\n",
    "    elif key.startswith(\"Flk\"):\n",
    "        return key.split(\"_\")[0]+\"_\"+key.split(\"_\")[1]\n",
    "    elif key.startswith(\"Shapiq\"):\n",
    "        return \"Shapiq\"\n",
    "    return None\n",
    "\n",
    "def collect_rr_at_k_over_budgets(extras, ground_truth, k):\n",
    "    # For each experiment, for each method, collect RR@k by budget\n",
    "    rr_by_method_budget = defaultdict(lambda: defaultdict(list))\n",
    "    for exp in extras:\n",
    "        for method, interaction in exp.items():\n",
    "            budget = extract_budget(method)\n",
    "            family = extract_family(method)\n",
    "            if budget and family:\n",
    "                rr = compute_rr_at_k(interaction, ground_truth, k)\n",
    "                rr_by_method_budget[family][budget].append(rr)\n",
    "    # Average over experiments\n",
    "    rr_avg = defaultdict(dict)\n",
    "    for family, budgets in rr_by_method_budget.items():\n",
    "        for budget, vals in budgets.items():\n",
    "            rr_avg[family][budget] = np.mean(vals)\n",
    "    return rr_avg\n",
    "rr_avg = collect_rr_at_k_over_budgets(extras, ground_truth, k)\n",
    "# Plot RR@k as line chart for each method family, with constant methods as parallel lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot budgeted families\n",
    "for family, budget_rrs in rr_avg.items():\n",
    "    budgets = sorted(budget_rrs.keys())\n",
    "    values = [budget_rrs[b] for b in budgets]\n",
    "    plt.plot(budgets, values, marker='o', label=family)\n",
    "\n",
    "# Plot constant methods (e.g., Exact-FSII, LOO, ARC-JSD) as horizontal lines\n",
    "# constant_methods = ['Exact-FSII', 'LOO', 'ARC-JSD']\n",
    "# for method in constant_methods:\n",
    "#     # Collect RR@k for each experiment and average\n",
    "#     rr_vals = []\n",
    "#     for exp in extras:\n",
    "#         if method in exp:\n",
    "#             rr_vals.append(compute_rr_at_k(exp[method], ground_truth, k))\n",
    "#     if rr_vals:\n",
    "#         avg_rr = np.mean(rr_vals)\n",
    "#         plt.axhline(y=avg_rr, color=None, linestyle='--', label=method)\n",
    "\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel(f'RR@{k}')\n",
    "plt.title(f'Recovery Rate at k={k} vs Budget (Averaged over experiments)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NDCG to Exact-FSII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaithShap (absolute values) â†’ NDCG plotting\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def extract_budget(key):\n",
    "    m = re.search(r'_(\\d+)$', key)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def extract_family(key):\n",
    "    # Map method keys to families used in earlier plots\n",
    "    if key.startswith(\"FMW\"):\n",
    "        return \"FMW\"\n",
    "    elif key.startswith(\"FM\"):\n",
    "        return \"FM\"\n",
    "    elif key.startswith(\"Flk_\"):\n",
    "        # collapse to Flu/Flk family prefix (keep as-is for plotting)\n",
    "        parts = key.split(\"_\")\n",
    "        return parts[0]+parts[1] if parts else None\n",
    "    elif key.startswith(\"Spex\"):\n",
    "        return \"Spex\"\n",
    "    elif key.startswith(\"Shapiq\"):\n",
    "        return \"Shapiq\"\n",
    "    return None\n",
    "\n",
    "def pairs_from_exact(exp_list):\n",
    "    # Find a canonical pair ordering from Exact-FSII of the first experiment that has it\n",
    "    for exp in exp_list:\n",
    "        exact = exp.get('Exact-FSII')\n",
    "        if exact and isinstance(exact, dict):\n",
    "            return sorted(exact.keys())\n",
    "    # fallback: try to infer from any dict-valued method\n",
    "    for exp in exp_list:\n",
    "        for v in exp.values():\n",
    "            if isinstance(v, dict) and v:\n",
    "                return sorted(v.keys())\n",
    "    return []\n",
    "\n",
    "def vector_for_pairs(val, pairs):\n",
    "    # val can be dict {(i,j):score} or a square matrix/list/ndarray\n",
    "    if isinstance(val, (list, np.ndarray)):\n",
    "        mat = np.array(val)\n",
    "        return [abs(mat[i][j]) if (0 <= i < mat.shape[0] and 0 <= j < mat.shape[1]) else 0.0 for (i,j) in pairs]\n",
    "    elif isinstance(val, dict):\n",
    "        return [abs(val.get(pair, 0.0)) for pair in pairs]\n",
    "    else:\n",
    "        # Unknown type -> zeros\n",
    "        return [0.0 for _ in pairs]\n",
    "\n",
    "# Build canonical pair list\n",
    "pairs = pairs_from_exact(extras)\n",
    "if not pairs:\n",
    "    print('No pair ordering could be inferred from Exact-FSII or other dicts in extras. Aborting NDCG computation.')\n",
    "else:\n",
    "    # Compute per-experiment NDCG scores for each method (relative to Exact-FSII)\n",
    "    per_method_ndcg = defaultdict(list)\n",
    "    for exp in extras:\n",
    "        exact = exp.get('Exact-FSII', {})\n",
    "        exact_vec = vector_for_pairs(exact, pairs)\n",
    "        # if exact vector is all zeros, skip this experiment for fairness\n",
    "        if np.allclose(exact_vec, 0.0):\n",
    "            continue\n",
    "        for method, val in exp.items():\n",
    "            if method == 'Exact-FSII':\n",
    "                continue\n",
    "            try:\n",
    "                vec = vector_for_pairs(val, pairs)\n",
    "                # ndcg_score expects shape (n_samples, n_labels) for both y_true and y_score\n",
    "                score = ndcg_score([exact_vec], [vec], k=5)\n",
    "                per_method_ndcg[method].append(score)\n",
    "            except Exception:\n",
    "                # skip methods we cannot convert\n",
    "                continue\n",
    "\n",
    "    # Average NDCG across experiments for each method\n",
    "    avg_ndcg = {m: float(np.mean(scores)) for m, scores in per_method_ndcg.items() if len(scores)>0}\n",
    "\n",
    "    # Group budgeted methods by family and budget\n",
    "    family_budget = defaultdict(lambda: defaultdict(list))\n",
    "    for method, score in avg_ndcg.items():\n",
    "        budget = extract_budget(method)\n",
    "        family = extract_family(method)\n",
    "        if budget is not None and family is not None:\n",
    "            family_budget[family][budget].append(score)\n",
    "\n",
    "    # Compute mean per family-budget (in case multiple variant keys map to same family-budget)\n",
    "    family_budget_avg = {}\n",
    "    for fam, bd in family_budget.items():\n",
    "        family_budget_avg[fam] = {b: float(np.mean(vals)) for b, vals in bd.items()}\n",
    "\n",
    "    # Plotting: line per family (budgeted), horizontal lines for constant methods\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # Plot budgeted families\n",
    "    for fam, bd in family_budget_avg.items():\n",
    "        xs = sorted(bd.keys())\n",
    "        ys = [bd[x] for x in xs]\n",
    "        plt.plot(xs, ys, marker='o', label=fam)\n",
    "\n",
    "    # Constant methods: plot as horizontal lines using avg_ndcg if available\n",
    "    constant_methods = ['Exact-FSII','Exact-Shap','LOO','ARC-JSD']\n",
    "    for cm in constant_methods:\n",
    "        if cm in avg_ndcg:\n",
    "            plt.axhline(y=avg_ndcg[cm], linestyle='--', label=cm)\n",
    "\n",
    "    plt.xlabel('Budget')\n",
    "    plt.ylabel('NDCG to Exact-FSII (absolute interaction magnitudes)')\n",
    "    plt.title('FaithShap â€” NDCG of absolute interactions vs Exact-FSII')\n",
    "    plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
