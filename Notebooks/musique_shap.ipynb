{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\"\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *\n",
    "from SHapRAG.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(lst):\n",
    "    # Titles where is_supporting is True\n",
    "    supporting = [d['paragraph_text'] for d in lst if d.get('is_supporting') == True]\n",
    "    # Titles where is_supporting is False or missing AND not already in supporting\n",
    "    others = [d['paragraph_text'] for d in lst if d.get('is_supporting') != True and d['paragraph_text'] not in supporting]\n",
    "    # Combine: all supporting + as many others as needed to reach 10\n",
    "    result = supporting + others\n",
    "    return result[:10]\n",
    "\n",
    "df.paragraphs=df.paragraphs.apply(get_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Sentences'] = df['paragraphs'].apply(\n",
    "#     lambda para_list: [sent for para in para_list for sent in nltk.sent_tokenize(para)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_save=pd.read_csv('../data/musique/sen_labeled.csv',\n",
    "#     quotechar='\"',\n",
    "#     skipinitialspace=True,\n",
    "#     engine='python' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"paragraphs\"] = df[\"paragraphs\"].apply(lambda p: p[:5]+ [p[1]] + p[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 08:46:34.684521: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 08:46:34.736395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 08:46:36.268557: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 08:46:36.268557: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answered.txt\", \"r\") as f:\n",
    "    res = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 2/50: What year saw the creation of the region where the county of... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx1.pkl...\n",
      "Successfully loaded 2048 cached utility entries.\n",
      "Running sample size: 32\n",
      "Initial scores: [0.46768362 2.48132338 0.92720086 1.52526116 1.23844564 4.11825813\n",
      " 1.10390648 1.46441797 0.07421359 0.24894526 0.29845612]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 64\n",
      "Initial scores: [5.03847300e-01 7.33616224e+00 1.38127676e+00 7.44849138e-01\n",
      " 1.03131567e+00 4.58525371e+00 3.78735932e-03 1.15978803e-01\n",
      " 6.92608620e-01 1.16109147e+00 8.70767382e-01]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 128\n",
      "Initial scores: [0.12291553 5.42581114 0.90794406 0.34627293 0.21403151 5.88306015\n",
      " 0.62363095 0.26174867 0.51573362 0.45089137 0.06614052]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 264\n",
      "Initial scores: [0.77504564 6.19785278 0.31904142 0.36494439 0.27268563 5.85743992\n",
      " 0.01459604 0.72447423 0.80764726 0.01453764 0.73090412]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 528\n",
      "Initial scores: [0.31625376 5.83185255 0.29436497 0.30180934 0.09808931 5.99807793\n",
      " 0.21914688 0.12995037 0.26045775 0.28261773 0.4306655 ]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 724\n",
      "Initial scores: [1.77372452e-01 5.77922189e+00 2.06995657e-02 1.58083301e-01\n",
      " 2.77385083e-03 6.09371982e+00 1.29760520e-02 3.64870035e-01\n",
      " 2.15612117e-01 4.06151771e-01 3.74411189e-01]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 1024\n",
      "Initial scores: [0.01433836 6.07398131 0.09399033 0.40857198 0.06819314 6.03061557\n",
      " 0.05304466 0.08565051 0.48837825 0.0847651  0.29852375]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Main Process: Saving 2048 utility entries to ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx1.pkl...\n",
      "Save complete.\n",
      "\n",
      "--- Question 3/50: When was the abolishment of the studio that distributed The ... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx2.pkl...\n",
      "Successfully loaded 2048 cached utility entries.\n",
      "Running sample size: 32\n",
      "Initial scores: [15.49928243  0.39547238  0.35871008  0.31809123  0.6358019   0.10992223\n",
      "  0.54822543  0.07222463  0.11609583  0.06359315  0.36430173]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 64\n",
      "Initial scores: [1.46317437e+01 1.83480735e+00 1.77871852e-01 1.77785267e-01\n",
      " 6.20734769e-01 1.29907064e+00 1.29453810e-01 1.75909373e-01\n",
      " 2.07991868e-03 2.69835040e-01 5.90568567e-01]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 128\n",
      "Initial scores: [15.47755688  1.53502758  0.37548005  0.08224311  0.02867657  1.27477138\n",
      "  0.27215953  0.09824941  0.15193779  0.3587581   0.38508199]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 264\n",
      "Initial scores: [15.79277716  1.01381095  0.13266823  0.02278949  0.06070166  0.70188298\n",
      "  0.0855051   0.01644131  0.17780367  0.06422796  0.08267759]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 528\n",
      "Initial scores: [1.57346506e+01 9.32704814e-01 1.88699691e-01 2.98943084e-02\n",
      " 1.56059234e-02 8.70360625e-01 1.68396051e-01 8.26101021e-02\n",
      " 2.26123269e-02 2.78939008e-02 3.19854603e-01]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 724\n",
      "Initial scores: [1.57918207e+01 9.47266501e-01 2.69007217e-01 4.12253901e-02\n",
      " 1.42638995e-02 8.99230377e-01 1.41281951e-01 7.25242264e-02\n",
      " 9.03556284e-02 1.02154278e-01 3.34273245e-01]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 1024\n",
      "Initial scores: [1.57775257e+01 9.48144734e-01 2.02159996e-01 7.49887739e-02\n",
      " 1.88172216e-02 9.18903703e-01 1.29802742e-01 8.60770697e-02\n",
      " 4.30120976e-02 1.16787531e-02 3.11124423e-01]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Main Process: Saving 2048 utility entries to ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx2.pkl...\n",
      "Save complete.\n",
      "\n",
      "--- Question 5/50: Jan Šindel's was born in what country?... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx4.pkl...\n",
      "Successfully loaded 2048 cached utility entries.\n",
      "Running sample size: 32\n",
      "Initial scores: [6.98252549 2.06598962 1.89112907 1.01593826 0.02365954 2.47807712\n",
      " 0.4551006  0.173444   0.17366043 0.31362223 1.71719562]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 64\n",
      "Initial scores: [7.86296678 3.19793164 0.19283049 0.43852414 0.68590725 1.83702113\n",
      " 0.73424394 0.04661016 0.39267755 0.45937089 0.07654768]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 128\n",
      "Initial scores: [7.28684571 2.30667048 1.08173137 0.44736592 0.03277756 2.34361461\n",
      " 0.28388229 0.39097588 0.16914689 0.06756396 0.41609207]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 264\n",
      "Initial scores: [7.77093063 2.18382803 1.17907619 0.34000094 0.17549541 2.3314601\n",
      " 0.14774991 0.01610581 0.16767961 0.25549245 0.03865926]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 528\n",
      "Initial scores: [7.90090147 2.02088198 0.98169395 0.59938166 0.13359064 2.16159881\n",
      " 0.31919241 0.33488044 0.35262614 0.26487699 0.03302153]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 724\n",
      "Initial scores: [7.70493255e+00 1.96788430e+00 1.15575869e+00 5.11614178e-01\n",
      " 3.69976236e-03 2.29191465e+00 1.62859199e-01 8.04648679e-02\n",
      " 3.42661057e-01 2.07139204e-01 6.36314307e-02]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 1024\n",
      "Initial scores: [7.64774599 2.06507627 0.99508656 0.44101335 0.06157566 2.10590598\n",
      " 0.10732683 0.23891884 0.28299284 0.21489959 0.136486  ]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Main Process: Saving 2048 utility entries to ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx4.pkl...\n",
      "Save complete.\n",
      "\n",
      "--- Question 6/50: What city is the person who broadened the doctrine of philos... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx5.pkl...\n",
      "Successfully loaded 2048 cached utility entries.\n",
      "Running sample size: 32\n",
      "Initial scores: [5.07364603 2.83161702 1.10241197 2.42838516 1.60341325 4.86099632\n",
      " 0.28675534 0.98598978 1.17322004 0.2045701  1.51570006]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 64\n",
      "Initial scores: [6.30404061 4.56302045 0.04228013 0.47432296 0.70887562 3.49264294\n",
      " 0.24937328 0.22091819 2.55643561 0.47047595 0.57059919]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 128\n",
      "Initial scores: [5.36460774 3.11497054 0.52008298 0.21678314 0.53778218 4.15256304\n",
      " 0.46346308 0.25592897 1.48941586 0.51784341 0.56841511]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 264\n",
      "Initial scores: [5.63983036 4.02025932 0.91331803 0.23301895 0.35053279 3.8527577\n",
      " 0.17798266 0.14673079 2.15974663 0.64789204 0.05127614]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 528\n",
      "Initial scores: [5.72405878 3.61444112 0.64785031 0.25548223 0.01852719 3.81901254\n",
      " 0.18002227 0.02690485 2.34165377 0.29717209 0.37078602]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 724\n",
      "Initial scores: [5.51976237 3.47426979 0.76650085 0.431582   0.03103313 3.66232142\n",
      " 0.1669274  0.12838616 2.00260132 0.23869854 0.05389723]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 1024\n",
      "Initial scores: [5.402209   3.3852464  0.69397349 0.31519251 0.11114914 3.51822521\n",
      " 0.20958442 0.25662785 2.08646922 0.23739791 0.0369336 ]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Main Process: Saving 2048 utility entries to ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx5.pkl...\n",
      "Save complete.\n",
      "\n",
      "--- Question 7/50: When was the baseball team winning the world series in 2015 ... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx6.pkl...\n",
      "Successfully loaded 2048 cached utility entries.\n",
      "Running sample size: 32\n",
      "Initial scores: [1.26640019e+01 1.15718738e+00 5.22318094e-01 7.18229891e-03\n",
      " 7.86337037e-01 5.04852206e-01 1.17462180e+00 4.94098119e-02\n",
      " 6.01503314e-01 5.59592216e-01 2.69888745e-01]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 64\n",
      "Initial scores: [12.98583904  0.87123818  0.28431393  0.18090254  0.2534142   0.04914216\n",
      "  0.48675038  0.16092489  0.14599424  0.25035213  0.10059116]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 128\n",
      "Initial scores: [12.64323464  0.12658589  0.0706234   0.29823155  0.38251038  0.39920839\n",
      "  0.13840831  0.13487618  0.16054042  0.04873108  0.15437548]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 264\n",
      "Initial scores: [1.25233600e+01 3.25090162e-01 2.55227775e-01 1.86922483e-01\n",
      " 1.58498477e-01 4.57467230e-01 5.38524044e-01 9.89420363e-03\n",
      " 1.40754197e-02 1.32195524e-01 8.73289444e-02]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 528\n",
      "Initial scores: [12.69247989  0.41064467  0.36671654  0.15715627  0.37779757  0.42508334\n",
      "  0.49063537  0.0301193   0.18134778  0.05762706  0.01773983]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 724\n",
      "Initial scores: [12.6537212   0.35040509  0.31862365  0.125648    0.19526254  0.38638503\n",
      "  0.5139281   0.12299204  0.07483466  0.05353452  0.0998678 ]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 1024\n",
      "Initial scores: [12.70567607  0.33782949  0.34158773  0.14732853  0.23901648  0.23563543\n",
      "  0.48109252  0.07944517  0.07238642  0.04858693  0.02440609]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Main Process: Saving 2048 utility entries to ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx6.pkl...\n",
      "Save complete.\n",
      "\n",
      "--- Question 9/50: Who was thee first president of the association that wrote t... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx8.pkl...\n",
      "Successfully loaded 2048 cached utility entries.\n",
      "Running sample size: 32\n",
      "Initial scores: [14.16527762  0.53737301  0.0228254   1.09442874  0.63639828  1.16381796\n",
      "  0.69587756  2.75049059  0.8214895   0.35888368  0.91703953]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 64\n",
      "Initial scores: [12.89401945  0.19810072  0.45319613  0.74509092  0.59547892  0.25528179\n",
      "  1.66093847  0.02011866  0.30348561  1.24355474  0.32925141]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 128\n",
      "Initial scores: [12.80387179  0.21261208  1.03824209  0.70386034  0.39061569  0.07389506\n",
      "  1.18592876  0.46469667  0.98053616  0.17853404  0.05190414]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 264\n",
      "Initial scores: [13.34400808  1.11351083  0.22179802  0.34734777  0.61004231  0.4415359\n",
      "  0.72367012  0.39897875  0.39940663  0.61895732  0.51609106]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 528\n",
      "Initial scores: [13.63496195  0.90292171  0.32426091  0.43619552  0.15493153  0.56911186\n",
      "  0.37830875  0.51833622  0.21232358  0.52333049  0.64043468]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 724\n",
      "Initial scores: [13.66390391  0.81409486  0.18145839  0.49349751  0.17720149  0.51190559\n",
      "  0.43432304  0.58579206  0.30772731  0.27901367  0.47645755]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 1024\n",
      "Initial scores: [13.83727195  0.77050166  0.17554121  0.36066261  0.11151958  0.50224381\n",
      "  0.34371289  0.60146242  0.25781361  0.32284287  0.34071545]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Main Process: Saving 2048 utility entries to ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx8.pkl...\n",
      "Save complete.\n",
      "\n",
      "--- Question 10/50: Which major Russian city borders the body of water in which ... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/utilities_q_idx9.pkl...\n",
      "Successfully loaded 2048 cached utility entries.\n",
      "Running sample size: 32\n",
      "Initial scores: [7.60858828 2.95692179 4.31389531 0.34300892 2.48593671 1.9380591\n",
      " 1.07065024 0.94579093 0.66924887 0.38220901 0.53875282]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 64\n",
      "Initial scores: [6.03719344 3.57268978 1.80174572 0.71258394 0.13192721 0.09320303\n",
      " 0.88168789 1.21415544 1.40654695 0.24532127 1.12865958]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 128\n",
      "Initial scores: [5.71002145 3.24851329 3.2941917  0.29635328 0.29119356 0.43470448\n",
      " 0.24494972 0.36681557 1.30662952 1.08684374 0.05156206]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "Running sample size: 264\n",
      "Initial scores: [5.60130725 3.02733396 2.91319874 0.13153658 0.11713056 0.44143116\n",
      " 0.41181927 0.03130803 1.48725061 0.51681502 0.01354658]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n",
      "SPEX approximation completed.\n",
      "Running sample size: 528\n",
      "Initial scores: [5.72025263 2.54331438 3.39817473 0.05551922 0.21072885 0.66583234\n",
      " 0.06074686 0.25923937 1.19410225 0.56238514 0.01080467]\n",
      "We are keeping 7 documents\n",
      "Running SPEX with method\n",
      "SPEX approximator initialized.\n"
     ]
    }
   ],
   "source": [
    "def gtset_k():\n",
    "    return [0, 1,5]\n",
    "\n",
    "num_questions_to_run = 50\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "all_results = []\n",
    "extras = []\n",
    "\n",
    "for i in range(num_questions_to_run):\n",
    "    query = df.question[i]\n",
    "    if res[i]==\"True\":\n",
    "        if accelerator_main.is_main_process:\n",
    "            print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "        docs = df.paragraphs[i]\n",
    "        utility_cache_base_dir = f\"../Experiment_data/musique/{model_path.split('/')[1]}/new/duplicate\"\n",
    "        utility_cache_filename = f\"utilities_q_idx{i}.pkl\"\n",
    "        current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "\n",
    "        if accelerator_main.is_main_process:\n",
    "            os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "\n",
    "        harness = ContextAttribution(\n",
    "            items=docs,\n",
    "            query=query,\n",
    "            prepared_model=prepared_model,\n",
    "            prepared_tokenizer=tokenizer,\n",
    "            accelerator=accelerator_main,\n",
    "            utility_cache_path=current_utility_path\n",
    "        )\n",
    "        full_budget=pow(2,harness.n_items)\n",
    "        # res = evaluate(df.question[i], harness.target_response, df.answer[i])\n",
    "        # res='True'\n",
    "        if accelerator_main.is_main_process:\n",
    "            methods_results = {}\n",
    "            metrics_results = {}\n",
    "            extra_results = {}\n",
    "\n",
    "            m_samples_map = {\"XS\": 32, \"S\":64, \"M\":128, \"L\":264, \"XL\":528, \"XXL\":724, \"XXXL\":1024}\n",
    "\n",
    "            # Store FM models for later R²/MSE\n",
    "            fm_models = {}\n",
    "            methods_results['Exact-Shap']=harness._calculate_shapley()\n",
    "            for size_key, actual_samples in m_samples_map.items():\n",
    "                print(f\"Running sample size: {actual_samples}\")\n",
    "                methods_results[f\"ContextCite_{actual_samples}\"], fm_models[f\"ContextCite_{actual_samples}\"] = harness.compute_contextcite(\n",
    "                    num_samples=actual_samples, seed=SEED\n",
    "                )\n",
    "                # FM Weights (loop over ranks 0–5)\n",
    "                # for rank in range(5, -1, -1):\n",
    "                    # methods_results[f\"FM_WeightsLK_{rank}_{actual_samples}\"], extra_results[f\"Flk_{rank}_{actual_samples}\"], fm_models[f\"FM_WeightsLK_{rank}_{actual_samples}\"] = harness.compute_wss(\n",
    "                    #     num_samples=actual_samples,\n",
    "                    #     seed=SEED,\n",
    "                    #     sampling=\"kernelshap\",\n",
    "                    #     sur_type=\"fm\",\n",
    "                    #     rank=rank\n",
    "                    # )\n",
    "                    # methods_results[f\"FM_WeightsLU_{rank}_{actual_samples}\"], extra_results[f\"Flu_{rank}_{actual_samples}\"], fm_models[f\"FM_WeightsLU_{rank}_{actual_samples}\"] = harness.compute_wss(\n",
    "                    #     num_samples=actual_samples,\n",
    "                    #     seed=SEED,\n",
    "                    #     sampling=\"kernelshap\",\n",
    "                    #     sur_type=\"fm\",\n",
    "                    #     rank=rank\n",
    "                    # )\n",
    "                # methods_results[f\"FM_u_dynamic_{actual_samples}\"], extra_results[f\"FM_u_dynamic_{actual_samples}\"], fm_models[f\"FM_u_dynamic_{actual_samples}\"] = harness.compute_wss_dynamic_pruning_reuse_utility(num_samples=actual_samples)\n",
    "                methods_results[f\"FM_k_dynamic_{actual_samples}\"], extra_results[f\"FM_k_dynamic_{actual_samples}\"], fm_models[f\"FM_k_dynamic_{actual_samples}\"] = harness.compute_wss_dynamic_pruning_reuse_utility(num_samples=actual_samples)\n",
    "                # methods_results[f\"FM_k_dynamice_{actual_samples}\"], extra_results[f\"FM_k_dynamice_{actual_samples}\"], fm_models[f\"FM_k_dynamice_{actual_samples}\"] = harness.compute_wss_dynamic_pruning_reuse_utility(num_samples=actual_samples, pruning_strategy='elbow')\n",
    "                try:\n",
    "                    # attributionsspex, interactionspex = harness.compute_spex(sample_budget=actual_samples, max_order=2)\n",
    "                    attributionshap, interactionshap, fm_models[f\"FSII_{actual_samples}\"] = harness.compute_fsii(sample_budget=actual_samples, max_order=2)\n",
    "                    # attributionban, interactionban, fm_models[f\"FBII_{actual_samples}\"] = harness.compute_fbii(sample_budget=actual_samples, max_order=harness.n_items)\n",
    "                    # methods_results[f\"FBII_{actual_samples}\"] = attributionban\n",
    "                    methods_results[f\"FSII_{actual_samples}\"] = attributionshap\n",
    "                    # methods_results[f\"Spex_{actual_samples}\"] = attributionsspex\n",
    "\n",
    "\n",
    "                    extra_results.update({\n",
    "                    f\"Int_FSII_{actual_samples}\":interactionshap,\n",
    "                    # f\"Int_FBII_{actual_samples}\":interactionban,\n",
    "                    # f\"Int_Spex_{actual_samples}\":interactionspex\n",
    "                                                                            })\n",
    "                except Exception: pass\n",
    "\n",
    "\n",
    "        #     methods_results[\"LOO\"] = harness.compute_loo()\n",
    "        #     methods_results[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "            attributionxs, interactionxs, fm_models[\"Exact-FSII\"] = harness.compute_exact_fsii(max_order=2)\n",
    "\n",
    "            extra_results.update({\n",
    "            \"Exact-FSII\": interactionxs\n",
    "        })\n",
    "            methods_results[\"Exact-FSII\"]=attributionxs\n",
    "\n",
    "            # --- Evaluation Metrics ---\n",
    "            metrics_results[\"topk_probability\"] = harness.evaluate_topk_performance(\n",
    "                methods_results, fm_models, k_values\n",
    "            )\n",
    "\n",
    "            # R²\n",
    "            metrics_results[\"R2\"] = harness.r2(methods_results,100,mode='logit-prob', models=fm_models)\n",
    "            metrics_results['Recall']=harness.recall_at_k(gtset_k(), methods_results, k_values)\n",
    "\n",
    "            # LDS per method\n",
    "            metrics_results[\"LDS\"] = harness.lds(methods_results,100,mode='logit-prob', models=fm_models)\n",
    "\n",
    "\n",
    "\n",
    "            all_results.append({\n",
    "                \"query_index\": i,\n",
    "                \"query\": query,\n",
    "                \"ground_truth\": df.answer[i],\n",
    "                \"response\": harness.target_response,\n",
    "                \"methods\": methods_results,\n",
    "                \"metrics\": metrics_results\n",
    "            })\n",
    "            extras.append(extra_results)\n",
    "\n",
    "            # Save utility cache\n",
    "            harness.save_utility_cache(current_utility_path)\n",
    "# with open(f\"{utility_cache_base_dir}/results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(all_results, f)\n",
    "\n",
    "# with open(f\"{utility_cache_base_dir}/extras.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(extras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness._run_spex(\"FBII\", 524, harness.n_items, 'logit-prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(harness.utility_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{utility_cache_base_dir}/results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "with open(f\"{utility_cache_base_dir}/extras.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../Experiment_data/musique/Llama-3.1-8B-Instruct/new/duplicate/extras.pkl\", \"rb\") as f:\n",
    "    extras = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness._get_top_k_utility_subsets(4, 'logit-prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmans={i:[] for i in all_results[0]['methods'] if i!=\"Exact-Shap\"}\n",
    "for method_res in all_results:\n",
    "    for method, attribution in method_res['methods'].items():\n",
    "        if method!=\"Exact-Shap\":\n",
    "            spear=len(set(np.array(method_res['methods'][\"Exact-Shap\"]).argsort()[-3:]).intersection(set(np.array(attribution).argsort()[-3:])))/3\n",
    "            spearmans[method].append(spear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ndcg_score\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m spearmans \u001b[38;5;241m=\u001b[39m {i: [] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethods\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExact-Shap\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWei\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethods\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m      6\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method_res \u001b[38;5;129;01min\u001b[39;00m all_results:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "spearmans = {i: [] for i in all_results[0]['methods'] if i != \"Exact-Shap\" and \"Wei\" not in all_results[0]['methods']}\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for method_res in all_results:\n",
    "    for method, attribution in method_res['methods'].items():\n",
    "        if method != \"Exact-Shap\" and \"Wei\" not in method:\n",
    "            # Convert to numpy arrays for scaling\n",
    "            ref = np.array(method_res['methods'][\"Exact-Shap\"]).reshape(-1, 1)\n",
    "            att = np.array(attribution).reshape(-1, 1)\n",
    "            \n",
    "            # Scale both reference and attribution to [0, 1]\n",
    "            ref_scaled = scaler.fit_transform(ref).flatten()\n",
    "            att_scaled = scaler.fit_transform(att).flatten()\n",
    "            \n",
    "            # Compute NDCG score\n",
    "            spear = ndcg_score([ref_scaled], [att_scaled], k=4)\n",
    "            spearmans[method].append(spear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spearmans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m parsed \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      7\u001b[0m budgets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, values \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspearmans\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m     avg_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(values)\n\u001b[1;32m     11\u001b[0m     match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)_(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)$\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)  \u001b[38;5;66;03m# method_budget pattern\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spearmans' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parse methods and budgets\n",
    "parsed = {}\n",
    "budgets = set()\n",
    "for key, values in spearmans.items():\n",
    "    avg_val = np.mean(values)\n",
    "    \n",
    "    match = re.match(r\"(.+?)_(\\d+)$\", key)  # method_budget pattern\n",
    "    if match:\n",
    "        method, budget = match.groups()\n",
    "        budget = int(budget)\n",
    "        budgets.add(budget)\n",
    "        parsed.setdefault(method, {})[budget] = avg_val\n",
    "    else:\n",
    "        # constant methods (no budget)\n",
    "        parsed.setdefault(key, {})[None] = avg_val\n",
    "\n",
    "budgets = sorted(budgets)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for method, results in parsed.items():\n",
    "    if None in results:  # constant method\n",
    "        plt.hlines(results[None], xmin=min(budgets), xmax=max(budgets), \n",
    "                   linestyles='--', label=method)\n",
    "    else:\n",
    "        xs = sorted(results.keys())\n",
    "        ys = [results[b] for b in xs]\n",
    "        plt.plot(xs, ys, marker='o', label=method)\n",
    "\n",
    "plt.xlabel(\"Budget\")\n",
    "plt.ylabel(\"Average Spearman\")\n",
    "plt.title(\"Average Recall to Exact Shap per Method vs Budget\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_and_print(all_results, k_values=[1, 2, 3,4,5]):\n",
    "    table_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Mapping for consistency\n",
    "    method_name_map = {\n",
    "        \n",
    "    }\n",
    "\n",
    "    for res in all_results:\n",
    "        metrics = res[\"metrics\"]\n",
    "        # LDS and R2\n",
    "        for method_name, lds_val in metrics[\"LDS\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"LDS\"].append(lds_val)\n",
    "\n",
    "        for method_name, lds_val in metrics[\"R2\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"R2\"].append(lds_val)\n",
    "        # Top-k\n",
    "        for method_name, k_dict in metrics[\"topk_probability\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            for k in k_values:\n",
    "                if k in k_dict:\n",
    "                    col_name = f\"topk_probability_k{k}\"\n",
    "                    table_data[method][col_name].append(k_dict[k])\n",
    "        \n",
    "        for method_name, k_dict in metrics[\"Recall\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            for k in k_values:\n",
    "                col_name = f\"Recall@{k}\"\n",
    "                table_data[method][col_name].append(k_dict[k-1])\n",
    "    # Averages\n",
    "    avg_table = {\n",
    "        method: {metric: np.nanmean(values) for metric, values in metric_dict.items()}\n",
    "        for method, metric_dict in table_data.items()\n",
    "    }\n",
    "\n",
    "    # Standard deviations for LDS, R², and MSE\n",
    "    for method, metric_dict in table_data.items():\n",
    "        for metric in [\"LDS\", \"R2\"]:\n",
    "            if metric in metric_dict:\n",
    "                avg_table[method][f\"{metric}_std\"] = np.nanstd(metric_dict[metric])\n",
    "\n",
    "    df_summary = pd.DataFrame.from_dict(avg_table, orient=\"index\").sort_index()\n",
    "\n",
    "    print(\"\\n=== Metrics Summary Across All Queries ===\")\n",
    "    print(df_summary.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "    return df_summary\n",
    "df_res=summarize_and_print(all_results, k_values=[1, 2, 3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reset index\n",
    "df_reset = df_res.reset_index().rename(columns={'index': 'method'})\n",
    "\n",
    "# Separate constant methods (no budget) and budgeted methods\n",
    "constant_methods = ['LOO', 'ARC-JSD', 'Exact-FSII', 'Exact-Shap']\n",
    "df_const = df_reset[df_reset['method'].isin(constant_methods)]\n",
    "df_budgeted = df_reset[~df_reset['method'].isin(constant_methods)]\n",
    "\n",
    "# Extract family and budget for budgeted methods\n",
    "df_budgeted['family'] = df_budgeted['method'].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "df_budgeted['budget'] = df_budgeted['method'].apply(lambda x: int(x.split(\"_\")[-1]))\n",
    "df_budgeted = df_budgeted.sort_values(by=['family', 'budget'])\n",
    "\n",
    "# Function to plot metric\n",
    "def plot_metric(metric, ylabel):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot budgeted families\n",
    "    families = df_budgeted['family'].unique()\n",
    "    for fam in families:\n",
    "        if 'LK' not in fam and \"Wei\" not in method:\n",
    "            subset = df_budgeted[df_budgeted['family'] == fam]\n",
    "            plt.plot(subset['budget'], subset[metric], marker='o', label=fam)\n",
    "\n",
    "    # Plot constant methods as horizontal lines\n",
    "    colors = plt.cm.tab10.colors  # categorical palette\n",
    "    for idx, (_, row) in enumerate(df_const.iterrows()):\n",
    "        plt.axhline(y=row[metric], color=colors[idx % len(colors)],marker='x', label=row['method'])\n",
    "\n",
    "    plt.xlabel(\"Budget\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"Evolution of {ylabel} with Increasing Budget\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot LDS\n",
    "# plot_metric(\"LDS\", \"LDS\")\n",
    "\n",
    "# Plot R²\n",
    "# plot_metric(\"R2\", \"R²\")\n",
    "\n",
    "# plot_metric(\"Recall@1\", \"Recall 1\")\n",
    "# plot_metric(\"Recall@2\", \"Recall 2\")\n",
    "# plot_metric(\"Recall@3\", \"Recall 3\")\n",
    "# plot_metric(\"Recall@4\", \"Recall 4\")\n",
    "# plot_metric(\"Recall@5\", \"Recall 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\"R2\", \"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\"LDS\", \"LDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter budgeted methods at budget = 274\n",
    "df_budgeted_264 = df_budgeted[df_budgeted['budget'] == 1024]\n",
    "\n",
    "# Metrics to plot\n",
    "recall_metrics = [f\"Recall@{k}\" for k in range(1, 6)]\n",
    "k_values = list(range(1, 6))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot budgeted families at budget 274\n",
    "families = df_budgeted_264['family'].unique()\n",
    "for fam in families:\n",
    "    if 'LK' not in fam:\n",
    "        subset = df_budgeted_264[df_budgeted_264['family'] == fam]\n",
    "        if not subset.empty:\n",
    "            recalls = subset[recall_metrics].values.flatten()\n",
    "            plt.plot(k_values, recalls, marker='o', label=fam)\n",
    "\n",
    "# Plot constant methods\n",
    "for idx, (_, row) in enumerate(df_const.iterrows()):\n",
    "    recalls = [row[m] for m in recall_metrics]\n",
    "    plt.plot(k_values, recalls, marker='x', linestyle=\"--\", label=row['method'])\n",
    "\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall@k\")\n",
    "plt.title(\"Recall@k for Budget = 264\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reset index\n",
    "df_reset = df_res.reset_index().rename(columns={'index': 'method'})\n",
    "\n",
    "# Parse FM methods (rank + budget)\n",
    "def parse_fm(method):\n",
    "    parts = method.split(\"_\")\n",
    "    if parts[0] == \"FM\" and parts[1].startswith(\"Weights\"):\n",
    "        # Example: FM_WeightsLU_5_32 → parts = ['FM', 'WeightsLU', '5', '32']\n",
    "        variant = parts[1]        # 'WeightsLU' or 'WeightsLK'\n",
    "        rank = int(parts[2])\n",
    "        budget = int(parts[-1])\n",
    "        return f\"{parts[0]}_{variant}\", rank, budget\n",
    "    return None, None, None\n",
    "\n",
    "df_reset[[\"family\", \"rank\", \"budget\"]] = pd.DataFrame(\n",
    "    df_reset[\"method\"].apply(parse_fm).tolist(),\n",
    "    index=df_reset.index\n",
    ")\n",
    "# Separate FM and non-FM methods\n",
    "df_fm = df_reset[df_reset['family'].notnull()]\n",
    "df_nonfm = df_reset[df_reset['family'].isnull()]\n",
    "\n",
    "# Keep only ContextCite baselines\n",
    "# df_rest = df_nonfm[df_nonfm['method'].str.startswith(\"ContextCite\")]\n",
    "\n",
    "# Function to plot R² for a given budget\n",
    "def plot_r2_for_budget(budget):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for family, subset in df_fm[df_fm['budget'] == budget].groupby(\"family\"):\n",
    "        subset = subset.sort_values(by=\"rank\")\n",
    "        plt.plot(subset[\"rank\"], subset[\"R2\"], marker='o', label=f\"{family} (R²)\")\n",
    "\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", len(df_nonfm['method'].unique()))\n",
    "\n",
    "    # Assign each unique method a color\n",
    "    method_to_color = {\n",
    "        method: cmap(i) for i, method in enumerate(df_nonfm['method'].unique())\n",
    "    }\n",
    "\n",
    "    # Plot\n",
    "    for _, row in df_nonfm.iterrows():\n",
    "        if row['method'].endswith(f\"_{budget}\"):\n",
    "            plt.axhline(\n",
    "                y=row['R2'],\n",
    "                alpha=0.7,\n",
    "                label=row['method'],\n",
    "                color=method_to_color[row['method']]\n",
    "            )\n",
    "\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"R²\")\n",
    "    plt.title(f\"Evolution of R² with Rank (Budget = {budget})\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: plot for budget = 528\n",
    "plot_r2_for_budget(32)\n",
    "plot_r2_for_budget(64)\n",
    "plot_r2_for_budget(128)\n",
    "plot_r2_for_budget(264)\n",
    "plot_r2_for_budget(528)\n",
    "plot_r2_for_budget(724)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "for method in df_res.index:\n",
    "    if \"264\" in method and \"Wei\" not in method:\n",
    "        plt.plot(\n",
    "            [1, 2, 3,4,5],\n",
    "            df_res.loc[method, ['topk_probability_k1', 'topk_probability_k2', 'topk_probability_k3', 'topk_probability_k4', 'topk_probability_k5']],\n",
    "            marker='o',\n",
    "            label=method\n",
    "        )\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Probability Drop')\n",
    "plt.title('Top-k Probability Drop')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras[2].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_methods(extras, k, m, interaction_type=\"max\"):\n",
    "\n",
    "    methods = extras[0].keys()\n",
    "    scores = {m: 0 for m in methods}\n",
    "    n_experiments = len(extras)\n",
    "\n",
    "    for exp in extras:\n",
    "        for method in methods:\n",
    "            if \"Fl\" in method or \"FM\" in method:\n",
    "                # Flu is a matrix\n",
    "                value = exp[method][k][m]\n",
    "                all_values = exp[method].flatten()\n",
    "            else:\n",
    "                # Dictionaries with tuple keys\n",
    "                d = exp[method]\n",
    "                value = None\n",
    "                for key, v in d.items():\n",
    "                    if key == (k, m):\n",
    "                        value = v\n",
    "                        break\n",
    "                if value is None:\n",
    "                    continue  # skip if (k,m) not found\n",
    "                all_values = list(d.values())\n",
    "\n",
    "            if interaction_type == \"max\":\n",
    "                if value == max(all_values):\n",
    "                    scores[method] += 1\n",
    "            elif interaction_type == \"min\":\n",
    "                if value == min(all_values):\n",
    "                    scores[method] += 1\n",
    "\n",
    "    # Convert to fraction of experiments\n",
    "    results = {method: scores[method] / n_experiments for method in methods}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovery rate\n",
    "em={}\n",
    "for i, j in enumerate(np.array(list(evaluate_methods(extras, k=0, m=1, interaction_type=\"max\").values()))+np.array(list(evaluate_methods(extras, k=0, m=5, interaction_type=\"max\").values()))+np.array(list(evaluate_methods(extras, k=1, m=5, interaction_type=\"min\").values()))):\n",
    "    em.update({list(extras[0].keys())[i]:j})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for k, v in em.items():\n",
    "    parts = k.split(\"_\")\n",
    "    if parts[0] == \"Flu\" and parts[1]!='0':\n",
    "        _, rank, budget = parts\n",
    "        rows.append({\"method\": \"Flu\", \"rank\": int(rank), \"budget\": int(budget), \"recovery\": v})\n",
    "    elif parts[0] == \"FM\"and parts[3]!='0':\n",
    "        _,_,_,budget = parts\n",
    "        rows.append({\"method\": f'FM_k_dynamic', \"budget\": int(budget), \"recovery\": v})\n",
    "    elif parts[0] == \"Int\":\n",
    "        _, name, budget = parts\n",
    "        rows.append({\"method\": name, \"budget\": int(budget), \"recovery\": v})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Flu (different ranks as lines)\n",
    "# for rank in sorted(df[df[\"method\"]==\"Flu\"][\"rank\"].unique()):\n",
    "#     sub = df[(df[\"method\"]==\"Flu\") & (df[\"rank\"]==rank)].sort_values(\"budget\")\n",
    "#     plt.plot(sub[\"budget\"], sub[\"recovery\"]/3, marker=\"o\", label=f\"Flu rank {rank}\")\n",
    "\n",
    "# for rank in sorted(df[df[\"method\"]==\"FM_k_dynamic\"][\"rank\"].unique()):\n",
    "#     sub = df[(df[\"method\"]==\"FM_k_dynamic\") & (df[\"rank\"]==rank)].sort_values(\"budget\")\n",
    "#     plt.plot(sub[\"budget\"], sub[\"recovery\"]/3, marker=\"o\", label=f\"FM_k_dynamic rank {rank}\")\n",
    "# for rank in sorted(df[df[\"method\"]==\"FM_r_dynamic\"]):\n",
    "sub1 = df[(df[\"method\"]==\"FM_k_dynamic\")].sort_values(\"budget\")\n",
    "plt.plot(sub1[\"budget\"], sub1[\"recovery\"], marker=\"o\", label=f\"FM_k_dynamic\")\n",
    "plt.plot(sub1[\"budget\"],em['Exact-FSII']*np.ones(7), marker=\"x\", linestyle=\"--\", label=f\"Exact-FSII\")\n",
    "\n",
    "# Plot Int methods (evolve with budget, start at 264)\n",
    "for m in df[df[\"method\"].isin([\"FSII\",\"FBII\",\"Spex\"])]['method'].unique():\n",
    "    sub = df[df[\"method\"]==m].sort_values(\"budget\")\n",
    "    plt.plot(sub[\"budget\"], sub[\"recovery\"]/3, marker=\"+\", linestyle=\"--\", label=f\"Int {m}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Budget\")\n",
    "plt.ylabel(\"Recovery Rate\")\n",
    "plt.title(\"Evolution of Recovery Rate with Increasing Budget\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. compare the new and old fm\n",
    "2. iteration with the interactions\n",
    "3. shapley is for first order and faithshap for pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaithShap recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_top2_min_from_dict(d):\n",
    "    \"\"\"Return top-2 and min interaction pairs from a dict {(i, j): val}.\"\"\"\n",
    "    if not d:\n",
    "        return set(), None\n",
    "    sorted_items = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    top2 = {sorted_items[0][0], sorted_items[1][0]} if len(sorted_items) >= 2 else {sorted_items[0][0]}\n",
    "    min_pair = min(d.items(), key=lambda x: x[1])[0]\n",
    "    return top2, min_pair\n",
    "\n",
    "def get_top2_min_from_matrix(mat):\n",
    "    \"\"\"Return top-2 and min interaction pairs from a numpy or list matrix.\"\"\"\n",
    "    mat = np.array(mat)\n",
    "    pairs = {(i, j): mat[i][j] for i in range(mat.shape[0]) for j in range(mat.shape[1]) if i != j}\n",
    "    return get_top2_min_from_dict(pairs)\n",
    "\n",
    "def extract_budget(key):\n",
    "    match = re.search(r'_(\\d+)$', key)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_family(key):\n",
    "\n",
    "    if key.startswith(\"Flu_\"):\n",
    "        # e.g. Flu_5_128 → Flu_5\n",
    "        m = re.match(r\"(Flu_\\d+)_\\d+\", key)\n",
    "        return m.group(1) if m else None\n",
    "    elif key.startswith(\"FM_k_dynamic\"):\n",
    "        return \"FM_k_dynamic\"\n",
    "    elif key.startswith(\"FSII\"):\n",
    "        return \"FSII\"\n",
    "    elif key.startswith(\"FBII\"):\n",
    "        return \"FBII\"\n",
    "    return None\n",
    "\n",
    "def compare_methods_concordance(extras):\n",
    "    concordance = defaultdict(list)\n",
    "\n",
    "    for d in extras:\n",
    "        exact = d.get('Exact-FSII', {})\n",
    "        exact_top2, exact_min = get_top2_min_from_dict(exact)\n",
    "\n",
    "        for key, val in d.items():\n",
    "            if key == 'Exact-FSII':\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if isinstance(val, dict):\n",
    "                    top2, min_pair = get_top2_min_from_dict(val)\n",
    "                else:\n",
    "                    top2, min_pair = get_top2_min_from_matrix(val)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            matches = len(exact_top2.intersection(top2))\n",
    "            if exact_min == min_pair:\n",
    "                matches += 1\n",
    "\n",
    "            score = matches / 3\n",
    "            concordance[key].append(score)\n",
    "\n",
    "    avg_concordance = {k: np.mean(v) for k, v in concordance.items() if v}\n",
    "    return avg_concordance\n",
    "\n",
    "def group_by_family_and_budget(avg_concordance):\n",
    "    grouped = defaultdict(list)\n",
    "    for key, score in avg_concordance.items():\n",
    "        budget = extract_budget(key)\n",
    "        family = extract_family(key)\n",
    "        if family and budget:\n",
    "            grouped[family].append((budget, score))\n",
    "\n",
    "    # Sort budgets\n",
    "    for family in grouped:\n",
    "        grouped[family].sort(key=lambda x: x[0])\n",
    "    return grouped\n",
    "\n",
    "def plot_concordance(grouped):\n",
    "    plt.figure(figsize=(9,6))\n",
    "    for family, data in grouped.items():\n",
    "        budgets, scores = zip(*data)\n",
    "        plt.plot(budgets, scores, marker='o', label=family)\n",
    "\n",
    "    plt.xlabel(\"Budget\")\n",
    "    plt.ylabel(\"Average Concordance to Exact-FSII\")\n",
    "    plt.title(\"Concordance vs Budget (Top-2 + Min Matching)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "avg_concordance = compare_methods_concordance(extras)\n",
    "grouped = group_by_family_and_budget(avg_concordance)\n",
    "plot_concordance(grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RR@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rr_at_k(interaction, ground_truth, k):\n",
    "    \"\"\"\n",
    "    Compute Recovery@k for a method's interaction dict or matrix.\n",
    "    interaction: dict {(i, j): value} or 2D numpy array/matrix\n",
    "    ground_truth: set of ground-truth indices (R^*)\n",
    "    k: number of top interactions to consider\n",
    "    Returns: RR@k value\n",
    "    \"\"\"\n",
    "    # Convert matrix to dict if needed\n",
    "    if isinstance(interaction, (np.ndarray, list)):\n",
    "        mat = np.array(interaction)\n",
    "        pairs = {(i, j): mat[i][j] for i in range(mat.shape[0]) for j in range(mat.shape[1]) if i != j}\n",
    "    else:\n",
    "        pairs = interaction\n",
    "\n",
    "    # Sort pairs by value (descending)\n",
    "    sorted_pairs = sorted(pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    rr_sum = 0.0\n",
    "    for i in range(min(k, len(sorted_pairs))):\n",
    "        pair_indices = set(sorted_pairs[i][0])\n",
    "        rr_sum += len(ground_truth & pair_indices) / len(pair_indices)\n",
    "    return rr_sum / k if k > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = set([0, 1, 5])  # Example ground-truth indices\n",
    "k = 5  # Number of top interactions to consider\n",
    "\n",
    "rr_results = {}\n",
    "for method, interaction in extras[0].items():  # Use the correct experiment index\n",
    "    rr_results[method] = compute_rr_at_k(interaction, ground_truth, k)\n",
    "\n",
    "print(rr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_rr_at_k(extras[0]['FM_k_dynamic_1024'], set([0, 1, 5]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Parse RR@k for all experiments and budgets\n",
    "def extract_budget(key):\n",
    "    match = re.search(r'_(\\d+)$', key)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_family(key):\n",
    "    if key.startswith(\"FM_k_dynamic\"):\n",
    "        return \"FM_k_dynamic\"\n",
    "    elif key.startswith(\"FSII\"):\n",
    "        return \"FSII\"\n",
    "    elif key.startswith(\"FBII\"):\n",
    "        return \"FBII\"\n",
    "    return None\n",
    "\n",
    "def collect_rr_at_k_over_budgets(extras, ground_truth, k):\n",
    "    # For each experiment, for each method, collect RR@k by budget\n",
    "    rr_by_method_budget = defaultdict(lambda: defaultdict(list))\n",
    "    for exp in extras:\n",
    "        for method, interaction in exp.items():\n",
    "            budget = extract_budget(method)\n",
    "            family = extract_family(method)\n",
    "            if budget and family:\n",
    "                rr = compute_rr_at_k(interaction, ground_truth, k)\n",
    "                rr_by_method_budget[family][budget].append(rr)\n",
    "    # Average over experiments\n",
    "    rr_avg = defaultdict(dict)\n",
    "    for family, budgets in rr_by_method_budget.items():\n",
    "        for budget, vals in budgets.items():\n",
    "            rr_avg[family][budget] = np.mean(vals)\n",
    "    return rr_avg\n",
    "# Plot RR@k as line chart for each method family, with constant methods as parallel lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot budgeted families\n",
    "for family, budget_rrs in rr_avg.items():\n",
    "    budgets = sorted(budget_rrs.keys())\n",
    "    values = [budget_rrs[b] for b in budgets]\n",
    "    plt.plot(budgets, values, marker='o', label=family)\n",
    "\n",
    "# Plot constant methods (e.g., Exact-FSII, LOO, ARC-JSD) as horizontal lines\n",
    "constant_methods = ['Exact-FSII', 'LOO', 'ARC-JSD']\n",
    "for method in constant_methods:\n",
    "    # Collect RR@k for each experiment and average\n",
    "    rr_vals = []\n",
    "    for exp in extras:\n",
    "        if method in exp:\n",
    "            rr_vals.append(compute_rr_at_k(exp[method], ground_truth, k))\n",
    "    if rr_vals:\n",
    "        avg_rr = np.mean(rr_vals)\n",
    "        plt.axhline(y=avg_rr, color=None, linestyle='--', label=method)\n",
    "\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel(f'RR@{k}')\n",
    "plt.title(f'Recovery Rate at k={k} vs Budget (Averaged over experiments)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RR@k as line chart for each method family, with constant methods as parallel lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot budgeted families\n",
    "for family, budget_rrs in rr_avg.items():\n",
    "    budgets = sorted(budget_rrs.keys())\n",
    "    values = [budget_rrs[b] for b in budgets]\n",
    "    plt.plot(budgets, values, marker='o', label=family)\n",
    "\n",
    "# Plot constant methods (e.g., Exact-FSII, LOO, ARC-JSD) as horizontal lines\n",
    "constant_methods = ['Exact-FSII', 'LOO', 'ARC-JSD']\n",
    "for method in constant_methods:\n",
    "    # Collect RR@k for each experiment and average\n",
    "    rr_vals = []\n",
    "    for exp in extras:\n",
    "        if method in exp:\n",
    "            rr_vals.append(compute_rr_at_k(exp[method], ground_truth, k))\n",
    "    if rr_vals:\n",
    "        avg_rr = np.mean(rr_vals)\n",
    "        plt.axhline(y=avg_rr, color=None, linestyle='--', label=method)\n",
    "\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel(f'RR@{k}')\n",
    "plt.title(f'Recovery Rate at k={k} vs Budget (Averaged over experiments)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
