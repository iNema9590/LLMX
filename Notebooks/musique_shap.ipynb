{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(lst):\n",
    "    # Titles where is_supporting is True\n",
    "    supporting = [d['paragraph_text'] for d in lst if d.get('is_supporting') == True]\n",
    "    # Titles where is_supporting is False or missing AND not already in supporting\n",
    "    others = [d['paragraph_text'] for d in lst if d.get('is_supporting') != True and d['paragraph_text'] not in supporting]\n",
    "    # Combine: all supporting + as many others as needed to reach 10\n",
    "    result = supporting + others\n",
    "    return result[:10]\n",
    "\n",
    "df.paragraphs=df.paragraphs.apply(get_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1/2: When was the institute that owned The Collegian founded?... ---\n",
      "Loading existing utility cache from ../Experiment_data/musique/utilities_q_idx0.pkl...\n",
      "Successfully loaded 346 cached utilities.\n",
      "Response: Houston Baptist University was founded in 1960.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.03it/s]\n",
      " 50%|█████     | 1/2 [00:12<00:12, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 2/2: What year saw the creation of the region where the county of... ---\n",
      "Loading existing utility cache from ../Experiment_data/musique/utilities_q_idx1.pkl...\n",
      "Successfully loaded 346 cached utilities.\n",
      "Response: 1994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.44it/s]\n",
      "100%|██████████| 2/2 [00:29<00:00, 14.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# num_questions_to_run=len(df.question)\n",
    "num_questions_to_run=2\n",
    "k_values = [1, 2]\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "spearman_fms=[]\n",
    "spearman_ccs=[]\n",
    "r2_ccs=[]\n",
    "r2_fms=[]\n",
    "for i in tqdm(range(num_questions_to_run), disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs=df.paragraphs[i]\n",
    "\n",
    "    utility_cache_base_dir = \"../Experiment_data/musique\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "\n",
    "    print(f'Response: {harness.target_response}')\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "    if accelerator_main.is_main_process:\n",
    "        m_samples_map = {\"L\": 100} \n",
    "        T_iterations_map = {\"L\":40} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0: \n",
    "                results_for_query[f\"ContextCite{actual_samples}\"], model_cc = harness.compute_contextcite(num_samples=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"FM_Shap{actual_samples}\"], results_for_query[f\"FM_Weights{actual_samples}\"], F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\")\n",
    "                results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=16, beta_b=1, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "        results_for_query[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "\n",
    "        prob_topk = harness.evaluate_topk_performance(\n",
    "                                                results_for_query, \n",
    "                                                k_values, \n",
    "                                                utility_type=\"probability\"\n",
    "                                            )\n",
    "    \n",
    "        div_topk = harness.evaluate_topk_performance(\n",
    "                                            results_for_query, \n",
    "                                            k_values, \n",
    "                                            utility_type=\"divergence\"\n",
    "                                        )\n",
    "    \n",
    "        results_for_query[\"topk_probability\"] = prob_topk\n",
    "        results_for_query[\"topk_divergence\"] = div_topk\n",
    "        harness.save_utility_cache(current_utility_path)\n",
    "        all_results.append(results_for_query)\n",
    "        spearman_cc, spearman_fm, r2_cc, r2_fm =harness.lds_and_faithfulness(modelfm, model_cc, 30)\n",
    "        spearman_ccs.append(spearman_cc)\n",
    "        spearman_fms.append(spearman_fm)\n",
    "        r2_ccs.append(r2_cc)\n",
    "        r2_fms.append(r2_fm)\n",
    "        all_results.append(results_for_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probability-based Top-k Performance:\n",
      "  ContextCite100:\n",
      "    k=1: Drop = 11.4530\n",
      "    k=2: Drop = 11.7507\n",
      "  FM_Shap100:\n",
      "    k=1: Drop = 11.4530\n",
      "    k=2: Drop = 11.7507\n",
      "  FM_Weights100:\n",
      "    k=1: Drop = 11.4530\n",
      "    k=2: Drop = 11.7507\n",
      "  BetaShap100:\n",
      "    k=1: Drop = 11.4530\n",
      "    k=2: Drop = 12.3379\n",
      "  TMC100:\n",
      "    k=1: Drop = 11.4530\n",
      "    k=2: Drop = 11.7507\n",
      "  LOO:\n",
      "    k=1: Drop = 11.4530\n",
      "    k=2: Drop = 12.3379\n",
      "  ARC-JSD:\n",
      "    k=1: Drop = 11.4530\n",
      "    k=2: Drop = 11.9030\n",
      "\n",
      "Divergence-based Top-k Performance:\n",
      "  ContextCite100:\n",
      "    k=1: JSD = 1.1366\n",
      "    k=2: JSD = 1.0900\n",
      "  FM_Shap100:\n",
      "    k=1: JSD = 1.1366\n",
      "    k=2: JSD = 1.0900\n",
      "  FM_Weights100:\n",
      "    k=1: JSD = 1.1366\n",
      "    k=2: JSD = 1.0900\n",
      "  BetaShap100:\n",
      "    k=1: JSD = 1.1366\n",
      "    k=2: JSD = 1.0931\n",
      "  TMC100:\n",
      "    k=1: JSD = 1.1366\n",
      "    k=2: JSD = 1.0900\n",
      "  LOO:\n",
      "    k=1: JSD = 1.1366\n",
      "    k=2: JSD = 1.0931\n",
      "  ARC-JSD:\n",
      "    k=1: JSD = 1.1366\n",
      "    k=2: JSD = 1.0704\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_drops = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Probability-based results\n",
    "print(\"\\nProbability-based Top-k Performance:\")\n",
    "for method, drops in prob_topk.items():\n",
    "    print(f\"  {method}:\")\n",
    "    for k, drop in drops.items():\n",
    "        print(f\"    k={k}: Drop = {drop:.4f}\")\n",
    "\n",
    "# Divergence-based results\n",
    "print(\"\\nDivergence-based Top-k Performance:\")\n",
    "for method, jsds in div_topk.items():\n",
    "    print(f\"  {method}:\")\n",
    "    for k, jsd in jsds.items():\n",
    "        print(f\"    k={k}: JSD = {jsd:.4f}\")\n",
    "\n",
    "# methods = list(all_drops.keys())\n",
    "# k_values = sorted(list(all_drops[methods[0]].keys()))\n",
    "\n",
    "# for method in methods:\n",
    "#     means = [np.nanmean(all_drops[method][k]) for k in k_values]\n",
    "#     plt.plot(k_values, means, label=method, marker='o')\n",
    "\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Mean Utility Drop')\n",
    "# plt.title('Top-k Removal Probability Drop')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.answer[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.target_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subsets = list(itertools.product([0, 1], repeat=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_tuples = harness._generate_sampled_ablations(10, sampling_method='uniform', seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "method_scores = {}\n",
    "\n",
    "for result in all_results:\n",
    "    for method, scores in result.items():\n",
    "        if scores is not None:\n",
    "            method_scores[method] = np.round(scores, 4)\n",
    "\n",
    "for method, scores in method_scores.items():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(scores)), scores, color='skyblue')\n",
    "    plt.title(f\"Approximate Scores: {method}\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(len(scores)))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
