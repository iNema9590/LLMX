{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *\n",
    "from SHapRAG.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(lst):\n",
    "    # Titles where is_supporting is True\n",
    "    supporting = [d['paragraph_text'] for d in lst if d.get('is_supporting') == True]\n",
    "    # Titles where is_supporting is False or missing AND not already in supporting\n",
    "    others = [d['paragraph_text'] for d in lst if d.get('is_supporting') != True and d['paragraph_text'] not in supporting]\n",
    "    # Combine: all supporting + as many others as needed to reach 10\n",
    "    result = supporting + others\n",
    "    return result[:10]\n",
    "\n",
    "df.paragraphs=df.paragraphs.apply(get_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Sentences'] = df['paragraphs'].apply(\n",
    "#     lambda para_list: [sent for para in para_list for sent in nltk.sent_tokenize(para)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_save=pd.read_csv('../data/musique/sen_labeled.csv',\n",
    "#     quotechar='\"',\n",
    "#     skipinitialspace=True,\n",
    "#     engine='python' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"paragraphs\"] = df[\"paragraphs\"].apply(lambda p: p[:5]+ [p[1]] + p[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtset_k():\n",
    "    return [0, 1,5]\n",
    "\n",
    "num_questions_to_run = 50\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "all_results = []\n",
    "extras = []\n",
    "\n",
    "for i in tqdm(range(num_questions_to_run), disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    \n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs = df.paragraphs[i]\n",
    "    utility_cache_base_dir = f\"../Experiment_data/musique/{model_path.split('/')[1]}/new/duplicate\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\"\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model=prepared_model,\n",
    "        prepared_tokenizer=tokenizer,\n",
    "        accelerator=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "    full_budget=pow(2,harness.n_items)\n",
    "    res = evaluate(df.question[i], harness.target_response, df.answer[i])\n",
    "    if res==\"True\":\n",
    "        if accelerator_main.is_main_process:\n",
    "            methods_results = {}\n",
    "            metrics_results = {}\n",
    "            extra_results = {}\n",
    "\n",
    "            m_samples_map = {\"XS\": 32, \"S\":64, \"M\":128, \"L\":264, \"XL\":528, \"XXL\":724}\n",
    "\n",
    "            # Store FM models for later R²/MSE\n",
    "            fm_models = {}\n",
    "\n",
    "            for size_key, actual_samples in m_samples_map.items():\n",
    "\n",
    "                methods_results[f\"ContextCite_{actual_samples}\"], fm_models[f\"ContextCite_{actual_samples}\"] = harness.compute_contextcite(\n",
    "                    num_samples=actual_samples, seed=SEED\n",
    "                )\n",
    "                # FM Weights (loop over ranks 0–5)\n",
    "                for rank in range(5, -1, -1):\n",
    "                    methods_results[f\"FM_WeightsLU_{rank}_{actual_samples}\"], extra_results[f\"Flu_{rank}_{actual_samples}\"], fm_models[f\"FM_WeightsLU_{rank}_{actual_samples}\"] = harness.compute_wss(\n",
    "                        num_samples=actual_samples,\n",
    "                        seed=SEED,\n",
    "                        sampling=\"kernelshap\",\n",
    "                        sur_type=\"fm\",\n",
    "                        rank=rank\n",
    "                    )\n",
    "\n",
    "                try:\n",
    "                    attributionsspex, interactionspex = harness.compute_spex(sample_budget=actual_samples, max_order=2)\n",
    "                    attributionshap, interactionshap = harness.compute_fsii(sample_budget=actual_samples, max_order=2)\n",
    "                    attributionban, interactionban = harness.compute_fbii(sample_budget=actual_samples, max_order=2)\n",
    "                    methods_results[f\"FBII_{actual_samples}\"] = attributionban\n",
    "                    methods_results[f\"Spex_{actual_samples}\"] = attributionsspex\n",
    "                    methods_results[f\"FSII_{actual_samples}\"] = attributionshap\n",
    "\n",
    "                    extra_results.update({\n",
    "                    f\"Int_FSII_{actual_samples}\":interactionshap,\n",
    "                    f\"Int_FBII_{actual_samples}\":interactionban,\n",
    "                    f\"Int_Spex_{actual_samples}\":interactionspex\n",
    "                                                                            })\n",
    "                except Exception: pass\n",
    "\n",
    "\n",
    "            methods_results[\"LOO\"] = harness.compute_loo()\n",
    "            methods_results[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "            attributionxs, interactionxs = harness.compute_fsii(sample_budget=full_budget, max_order=2)\n",
    "            extra_results.update({\n",
    "            \"Exact-Faith-Shap\": interactionxs\n",
    "        })\n",
    "            methods_results[\"Exact-Faith-Shap\"]=attributionxs\n",
    "\n",
    "            # --- Evaluation Metrics ---\n",
    "            metrics_results[\"topk_probability\"] = harness.evaluate_topk_performance(\n",
    "                methods_results, fm_models, k_values\n",
    "            )\n",
    "\n",
    "            # R²\n",
    "            metrics_results[\"R2\"] = harness.r2(methods_results,30,mode='logit-prob', models=fm_models)\n",
    "            metrics_results['Recall']=harness.recall_at_k(gtset_k(), methods_results, k_values)\n",
    "\n",
    "            # LDS per method\n",
    "            metrics_results[\"LDS\"] = harness.lds(methods_results,30,mode='logit-prob', models=fm_models)\n",
    "\n",
    "\n",
    "\n",
    "            all_results.append({\n",
    "                \"query_index\": i,\n",
    "                \"query\": query,\n",
    "                \"ground_truth\": df.answer[i],\n",
    "                \"response\": harness.target_response,\n",
    "                \"methods\": methods_results,\n",
    "                \"metrics\": metrics_results\n",
    "            })\n",
    "            extras.append(extra_results)\n",
    "\n",
    "            # Save utility cache\n",
    "            harness.save_utility_cache(current_utility_path)\n",
    "\n",
    "with open(f\"{utility_cache_base_dir}/results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "with open(f\"{utility_cache_base_dir}/extras.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"../Experiment_data/musique/Mistral-7B-Instruct-v0.3/duplicate/results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../Experiment_data/musique/Mistral-7B-Instruct-v0.3/duplicate/extras.pkl\", \"rb\") as f:\n",
    "    extras = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_and_print(all_results, k_values=[1, 2, 3,4,5]):\n",
    "    table_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Mapping for consistency\n",
    "    method_name_map = {\n",
    "        \n",
    "    }\n",
    "\n",
    "    for res in all_results:\n",
    "        metrics = res[\"metrics\"]\n",
    "\n",
    "\n",
    "        # LDS and R2\n",
    "        for method_name, lds_val in metrics[\"LDS\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"LDS\"].append(lds_val)\n",
    "\n",
    "        for method_name, lds_val in metrics[\"R2\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"R2\"].append(lds_val)\n",
    "        # Top-k\n",
    "        for method_name, k_dict in metrics[\"topk_probability\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            for k in k_values:\n",
    "                if k in k_dict:\n",
    "                    col_name = f\"topk_probability_k{k}\"\n",
    "                    table_data[method][col_name].append(k_dict[k])\n",
    "        \n",
    "        for method_name, k_dict in metrics[\"Recall\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            for k in k_values:\n",
    "                col_name = f\"Recall@{k}\"\n",
    "                table_data[method][col_name].append(k_dict[k-1])\n",
    "    # Averages\n",
    "    avg_table = {\n",
    "        method: {metric: np.nanmean(values) for metric, values in metric_dict.items()}\n",
    "        for method, metric_dict in table_data.items()\n",
    "    }\n",
    "\n",
    "    # Standard deviations for LDS, R², and MSE\n",
    "    for method, metric_dict in table_data.items():\n",
    "        for metric in [\"LDS\", \"R2\"]:\n",
    "            if metric in metric_dict:\n",
    "                avg_table[method][f\"{metric}_std\"] = np.nanstd(metric_dict[metric])\n",
    "\n",
    "    df_summary = pd.DataFrame.from_dict(avg_table, orient=\"index\").sort_index()\n",
    "\n",
    "    print(\"\\n=== Metrics Summary Across All Queries ===\")\n",
    "    print(df_summary.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "    return df_summary\n",
    "df_res=summarize_and_print(all_results, k_values=[1, 2, 3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras[1][\"ShapiQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reset index\n",
    "df_reset = df_res.reset_index().rename(columns={'index': 'method'})\n",
    "\n",
    "# Separate constant methods (no budget) and budgeted methods\n",
    "constant_methods = ['LOO', 'ARC-JSD', 'Exact-Faith-Shap']\n",
    "df_const = df_reset[df_reset['method'].isin(constant_methods)]\n",
    "df_budgeted = df_reset[~df_reset['method'].isin(constant_methods)]\n",
    "\n",
    "# Extract family and budget for budgeted methods\n",
    "df_budgeted['family'] = df_budgeted['method'].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "df_budgeted['budget'] = df_budgeted['method'].apply(lambda x: int(x.split(\"_\")[-1]))\n",
    "df_budgeted = df_budgeted.sort_values(by=['family', 'budget'])\n",
    "\n",
    "# Function to plot metric\n",
    "def plot_metric(metric, ylabel):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot budgeted families\n",
    "    families = df_budgeted['family'].unique()\n",
    "    for fam in families:\n",
    "        subset = df_budgeted[df_budgeted['family'] == fam]\n",
    "        plt.plot(subset['budget'], subset[metric], marker='o', label=fam)\n",
    "\n",
    "    # Plot constant methods as horizontal lines\n",
    "    colors = plt.cm.tab10.colors  # categorical palette\n",
    "    for idx, (_, row) in enumerate(df_const.iterrows()):\n",
    "        plt.axhline(y=row[metric], color=colors[idx % len(colors)],marker='x', label=row['method'])\n",
    "\n",
    "    plt.xlabel(\"Budget\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"Evolution of {ylabel} with Increasing Budget\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot LDS\n",
    "# plot_metric(\"LDS\", \"LDS\")\n",
    "\n",
    "# Plot R²\n",
    "# plot_metric(\"R2\", \"R²\")\n",
    "\n",
    "plot_metric(\"Recall@1\", \"Recall 1\")\n",
    "plot_metric(\"Recall@2\", \"Recall 2\")\n",
    "plot_metric(\"Recall@3\", \"Recall 3\")\n",
    "plot_metric(\"Recall@4\", \"Recall 4\")\n",
    "plot_metric(\"Recall@5\", \"Recall 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reset index\n",
    "df_reset = df_res.reset_index().rename(columns={'index': 'method'})\n",
    "\n",
    "# Parse FM methods (rank + budget)\n",
    "def parse_fm(method):\n",
    "    parts = method.split(\"_\")\n",
    "    if parts[0] == \"FM\" and \"WeightsLU\" in parts[1]:\n",
    "        rank = int(parts[2])\n",
    "        budget = int(parts[-1])\n",
    "        return rank, budget\n",
    "    return None, None\n",
    "\n",
    "df_reset['rank'], df_reset['budget'] = zip(*df_reset['method'].apply(parse_fm))\n",
    "\n",
    "# Separate FM and non-FM methods\n",
    "df_fm = df_reset[df_reset['rank'].notnull()]\n",
    "df_nonfm = df_reset[df_reset['rank'].isnull()]\n",
    "\n",
    "# Keep only ContextCite baselines\n",
    "df_contextcite = df_nonfm[df_nonfm['method'].str.startswith(\"ContextCite\")]\n",
    "\n",
    "# Function to plot R² for a given budget\n",
    "def plot_r2_for_budget(budget):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Subset FM methods for this budget\n",
    "    subset_fm = df_fm[df_fm['budget'] == budget].sort_values(by='rank')\n",
    "\n",
    "    # Plot FM evolution (R² vs rank)\n",
    "    plt.plot(subset_fm['rank'], subset_fm['R2'], marker='o', color=\"blue\", label=\"FM (R²)\")\n",
    "\n",
    "    # Plot only ContextCite baselines for this budget\n",
    "    for _, row in df_contextcite.iterrows():\n",
    "        if row['method'].endswith(f\"_{budget}\"):\n",
    "            plt.axhline(y=row['R2'], linestyle='--', color=\"black\", alpha=0.7, label=row['method'])\n",
    "\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"R²\")\n",
    "    plt.title(f\"Evolution of R² with Rank (Budget = {budget})\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: plot for budget = 528\n",
    "plot_r2_for_budget(32)\n",
    "plot_r2_for_budget(64)\n",
    "plot_r2_for_budget(128)\n",
    "plot_r2_for_budget(264)\n",
    "plot_r2_for_budget(528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reset index\n",
    "df_reset = df_res.reset_index().rename(columns={'index': 'method'})\n",
    "\n",
    "# Parse FM methods (rank + budget)\n",
    "def parse_fm(method):\n",
    "    parts = method.split(\"_\")\n",
    "    if parts[0] == \"FM\" and \"WeightsLU\" in parts[1]:\n",
    "        rank = int(parts[2])\n",
    "        budget = int(parts[-1])\n",
    "        return rank, budget\n",
    "    return None, None\n",
    "\n",
    "df_reset['rank'], df_reset['budget'] = zip(*df_reset['method'].apply(parse_fm))\n",
    "\n",
    "# Separate FM and non-FM methods\n",
    "df_fm = df_reset[df_reset['rank'].notnull()]\n",
    "df_nonfm = df_reset[df_reset['rank'].isnull()]\n",
    "\n",
    "# Keep only ContextCite baselines\n",
    "df_contextcite = df_nonfm[df_nonfm['method'].str.startswith(\"ContextCite\")]\n",
    "\n",
    "# Function to plot R² for a given budget\n",
    "def plot_r2_for_budget(budget):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Subset FM methods for this budget\n",
    "    subset_fm = df_fm[df_fm['budget'] == budget].sort_values(by='rank')\n",
    "\n",
    "    # Plot FM evolution (R² vs rank)\n",
    "    plt.plot(subset_fm['rank'], subset_fm['LDS'], marker='o', color=\"blue\", label=\"FM (LDS)\")\n",
    "\n",
    "    # Plot only ContextCite baselines for this budget\n",
    "    for _, row in df_contextcite.iterrows():\n",
    "        if row['method'].endswith(f\"_{budget}\"):\n",
    "            plt.axhline(y=row['LDS'], linestyle='--', color=\"black\", alpha=0.7, label=row['method'])\n",
    "\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"LDS\")\n",
    "    plt.title(f\"Evolution of LDS with Rank (Budget = {budget})\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: plot for budget = 528\n",
    "plot_r2_for_budget(32)\n",
    "plot_r2_for_budget(64)\n",
    "plot_r2_for_budget(128)\n",
    "plot_r2_for_budget(264)\n",
    "plot_r2_for_budget(528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reset index\n",
    "df_reset = df_res.reset_index().rename(columns={'index': 'method'})\n",
    "\n",
    "# Parse FM methods (rank + budget)\n",
    "def parse_fm(method):\n",
    "    parts = method.split(\"_\")\n",
    "    if parts[0] == \"FM\" and \"WeightsLU\" in parts[1]:\n",
    "        rank = int(parts[2])\n",
    "        budget = int(parts[-1])\n",
    "        return rank, budget\n",
    "    return None, None\n",
    "\n",
    "df_reset['rank'], df_reset['budget'] = zip(*df_reset['method'].apply(parse_fm))\n",
    "\n",
    "# Separate FM and non-FM methods\n",
    "df_fm = df_reset[df_reset['rank'].notnull()]\n",
    "df_nonfm = df_reset[df_reset['rank'].isnull()]\n",
    "\n",
    "# Function to plot topk_probability_k1 for a given budget\n",
    "def plot_topk_for_budget(budget):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Subset FM methods for this budget\n",
    "    subset_fm = df_fm[df_fm['budget'] == budget].sort_values(by='rank')\n",
    "\n",
    "    # Plot FM evolution (vs rank)\n",
    "    plt.plot(subset_fm['rank'], subset_fm['topk_probability_k1'],\n",
    "             marker='o', color=\"blue\", linewidth=2, label=\"FM (topk_k1)\")\n",
    "\n",
    "    # Plot all other methods as black dashed lines\n",
    "    for _, row in df_nonfm.iterrows():\n",
    "        if row['method'].endswith(f\"_{budget}\") or row['method'] in [\"LOO\", \"ARC-JSD\"]:\n",
    "            plt.axhline(y=row['topk_probability_k1'],\n",
    "                        linestyle='--', color=\"black\", alpha=0.7, label=row['method'])\n",
    "\n",
    "    plt.xlabel(\"Rank (only for FM)\")\n",
    "    plt.ylabel(\"Top-k Probability (k=1)\")\n",
    "    plt.title(f\"topk_probability_k1 vs Rank (Budget = {budget})\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: plot for budget = 528\n",
    "plot_topk_for_budget(32)\n",
    "plot_topk_for_budget(64)\n",
    "plot_topk_for_budget(128)\n",
    "plot_topk_for_budget(264)\n",
    "plot_topk_for_budget(528)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_table=compute_recall_at_k(all_results, k_values=[1, 2, 3, 4, 5])\n",
    "plot_recall_at_k(recall_table, k_values=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "for method in df_res.index:\n",
    "    if \"FM_WeightsLU_2\" in method:\n",
    "        plt.plot(\n",
    "            [1, 2, 3,4,5],\n",
    "            df_res.loc[method, ['topk_probability_k1', 'topk_probability_k2', 'topk_probability_k3', 'topk_probability_k4', 'topk_probability_k5']],\n",
    "            marker='o',\n",
    "            label=method\n",
    "        )\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Probability Drop')\n",
    "plt.title('Top-k Probability Drop')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_methods(extras, k, m, interaction_type=\"max\"):\n",
    "\n",
    "    methods = extras[0].keys()\n",
    "    scores = {m: 0 for m in methods}\n",
    "    n_experiments = len(extras)\n",
    "\n",
    "    for exp in extras:\n",
    "        for method in methods:\n",
    "            if \"Int\" not in method:\n",
    "                # Flu is a matrix\n",
    "                value = exp[method][k][m]\n",
    "                all_values = exp[method].flatten()\n",
    "            else:\n",
    "                # Dictionaries with tuple keys\n",
    "                d = exp[method]\n",
    "                value = None\n",
    "                for key, v in d.items():\n",
    "                    ones = [i for i, bit in enumerate(key) if bit == 1]\n",
    "                    if set(ones) == {k, m}:\n",
    "                        value = v\n",
    "                        break\n",
    "                if value is None:\n",
    "                    continue  # skip if (k,m) not found\n",
    "                all_values = list(d.values())\n",
    "\n",
    "            if interaction_type == \"max\":\n",
    "                if value == max(all_values):\n",
    "                    scores[method] += 1\n",
    "            elif interaction_type == \"min\":\n",
    "                if value == min(all_values):\n",
    "                    scores[method] += 1\n",
    "\n",
    "    # Convert to fraction of experiments\n",
    "    results = {method: scores[method] / n_experiments for method in methods}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovery rate\n",
    "rr={}\n",
    "for i, j in enumerate(np.array(list(evaluate_methods(extras, k=0, m=1, interaction_type=\"max\").values()))+np.array(list(evaluate_methods(extras, k=0, m=5, interaction_type=\"max\").values()))+np.array(list(evaluate_methods(extras, k=1, m=5, interaction_type=\"min\").values()))):\n",
    "    rr.update({list(extras[1].keys())[i]:j})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for k, v in rr.items():\n",
    "    parts = k.split(\"_\")\n",
    "    if parts[0] == \"Flu\" and parts[1]!='0':\n",
    "        _, rank, budget = parts\n",
    "        rows.append({\"method\": \"Flu\", \"rank\": int(rank), \"budget\": int(budget), \"recovery\": v})\n",
    "    elif parts[0] == \"Int\":\n",
    "        _, name, budget = parts\n",
    "        rows.append({\"method\": name, \"rank\": None, \"budget\": int(budget), \"recovery\": v})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Flu (different ranks as lines)\n",
    "for rank in sorted(df[df[\"method\"]==\"Flu\"][\"rank\"].unique()):\n",
    "    sub = df[(df[\"method\"]==\"Flu\") & (df[\"rank\"]==rank)].sort_values(\"budget\")\n",
    "    plt.plot(sub[\"budget\"], sub[\"recovery\"], marker=\"o\", label=f\"Flu rank {rank}\")\n",
    "\n",
    "# Plot Int methods (evolve with budget, start at 264)\n",
    "for m in df[df[\"method\"].isin([\"FSII\",\"FBII\",\"Spex\"])]['method'].unique():\n",
    "    sub = df[df[\"method\"]==m].sort_values(\"budget\")\n",
    "    plt.plot(sub[\"budget\"], sub[\"recovery\"], marker=\"s\", linestyle=\"--\", label=f\"Int {m}\")\n",
    "\n",
    "\n",
    "plt.plot(rr['ShapiQ'], marker=\"s\", linestyle=\"--\", label=f\"ShapiQ\")\n",
    "\n",
    "plt.xlabel(\"Budget\")\n",
    "plt.ylabel(\"Recovery Rate\")\n",
    "plt.title(\"Evolution of Recovery Rate with Increasing Budget\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(gtset_k, inf_scores, k_val ):\n",
    "    topk= np.array(inf_scores).argsort()[-k_val:]\n",
    "    recall= len(set(gtset_k).intersection(topk))/len(gtset_k)\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
