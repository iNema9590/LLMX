{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(lst):\n",
    "    # Titles where is_supporting is True\n",
    "    supporting = [d['paragraph_text'] for d in lst if d.get('is_supporting') == True]\n",
    "    # Titles where is_supporting is False or missing AND not already in supporting\n",
    "    others = [d['paragraph_text'] for d in lst if d.get('is_supporting') != True and d['paragraph_text'] not in supporting]\n",
    "    # Combine: all supporting + as many others as needed to reach 10\n",
    "    result = supporting + others\n",
    "    return result[:10]\n",
    "\n",
    "df.paragraphs=df.paragraphs.apply(get_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentences'] = df['paragraphs'].apply(\n",
    "    lambda para_list: [sent for para in para_list for sent in nltk.sent_tokenize(para)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save=pd.read_csv('../data/musique/sen_labeled.csv',\n",
    "    quotechar='\"',\n",
    "    skipinitialspace=True,\n",
    "    engine='python' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"paragraphs\"] = df[\"paragraphs\"].apply(lambda p: p[:5]+ [p[1]] + p[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1/50: ['The Collegian is the bi-weekly official student publication of Houston Baptist University in Houston, Texas.', 'It was founded in 1963 as a newsletter, and adopted the newspaper format in 1990.', \"Several private institutions of higher learning—ranging from liberal arts colleges, such as The University of St. Thomas, Houston's only Catholic university, to Rice University, the nationally recognized research university—are located within the city.\", 'Rice, with a total enrollment of slightly more than 6,000 students, has a number of distinguished graduate programs and research institutes, such as the James A. Baker Institute for Public Policy.', \"Houston Baptist University, affiliated with the Baptist General Convention of Texas, offers bachelor's and graduate degrees.\", 'It was founded in 1960 and is located in the Sharpstown area in Southwest Houston.', 'Pakistan Super League (Urdu: پاکستان سپر لیگ \\u202c \\u200e; PSL) is a Twenty20 cricket league, founded in Lahore on 9 September 2015 with five teams and now comprises six teams.', 'Instead of operating as an association of independently owned teams, the league is a single entity in which each franchise is owned and controlled by investors.', 'Serena Wilson (August 8, 1933 – June 17, 2007), often known just as \"Serena\", was a well-known dancer, choreographer, and teacher who helped popularize belly dance in the United States.', \"Serena's work also helped legitimize the dance form and helped it to be perceived as more than burlesque or stripping.\", 'Serena danced in clubs in her younger years, opened her own studio, hosted her own television show, founded her own dance troupe, and was the author of several books about belly dance.', 'Longman, also known as Pearson Longman, is a publishing company founded in London, England, in 1724 and is owned by Pearson PLC.', 'Bankhaus Lampe is a private bank in Germany, founded in 1852 and headquartered in Bielefeld.', 'It is wholly owned by the Oetker Group.', 'The bank owns 50% of Universal Investment.', 'Publix Super Markets, Inc., commonly known as Publix, is an employee - owned, American supermarket chain headquartered in Lakeland, Florida.', 'Founded in 1930 by George W. Jenkins, Publix is a private corporation that is wholly owned by present and past employees.', 'It is considered the largest employee - owned company in the world.', 'Publix operates throughout the Southeastern United States, with locations in Florida (785), Georgia (186), Alabama (68), South Carolina (58), Tennessee (42), North Carolina (35), and Virginia (8).', 'The Collegian is the oldest college newspaper in Michigan.', \"The paper's history traces back to 1878, when the Hillsdale Herald was first published.\", 'The administration started The Collegian in 1893 as a rival paper to the Herald.', 'This is a list of Old Scotch Collegians, who are notable former students of Scotch College in Melbourne, Victoria, Australia.', 'Renaissance Broadcasting, founded in 1982 by Michael Finkelstein, was a company that owned several UHF television stations, it was sold to Tribune Broadcasting in 1997.', 'The company was headquartered in Greenwich, Connecticut.']... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/sentence/utilities_q_idx0.pkl...\n",
      "Successfully loaded 994 cached utility entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 13472.07it/s]\n",
      "Computing utilities for ContextCite: 100%|██████████| 364/364 [01:56<00:00,  3.14it/s]\n",
      "  0%|          | 0/50 [03:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 55\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actual_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     methods_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContextCite\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], model_cc \u001b[38;5;241m=\u001b[39m harness\u001b[38;5;241m.\u001b[39mcompute_contextcite(\n\u001b[1;32m     52\u001b[0m         num_samples\u001b[38;5;241m=\u001b[39mactual_samples, seed\u001b[38;5;241m=\u001b[39mSEED\n\u001b[1;32m     53\u001b[0m     )\n\u001b[0;32m---> 55\u001b[0m     attributions, _ \u001b[38;5;241m=\u001b[39m \u001b[43mharness\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_spex\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactual_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     methods_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFBII\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m attributions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfbii\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     57\u001b[0m     methods_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpex\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m attributions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfourier\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/LLMX/SHapRAG/rag_shap.py:665\u001b[0m, in \u001b[0;36mContextAttribution.compute_spex\u001b[0;34m(self, sample_budget, max_order, utility_mode)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_values\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# Initialize explainer\u001b[39;00m\n\u001b[0;32m--> 665\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mspex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExplainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoc_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_items\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Placeholder names\u001b[39;49;00m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_budget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_order\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m attributions\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m    672\u001b[0m ints\u001b[38;5;241m=\u001b[39m{}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/spectralexplain/explainer.py:14\u001b[0m, in \u001b[0;36mExplainer.__init__\u001b[0;34m(self, value_function, features, sample_budget, max_order, name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_budget \u001b[38;5;241m=\u001b[39m sample_budget\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparsity_parameter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_budget \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_sparsity_parameter()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_interaction_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/spectralexplain/explainer.py:29\u001b[0m, in \u001b[0;36mExplainer.compute_interaction_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_interaction_values\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     signal, _ \u001b[38;5;241m=\u001b[39m \u001b[43mspex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparsity_parameter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_save_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spex\u001b[38;5;241m.\u001b[39msupport_recovery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m\"\u001b[39m, signal, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparsity_parameter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_order)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/spectralexplain/support_recovery.py:25\u001b[0m, in \u001b[0;36msampling_strategy\u001b[0;34m(sampling_function, min_b, max_b, n, sample_save_dir, t)\u001b[0m\n\u001b[1;32m     13\u001b[0m bs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(min_b, max_b \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m query_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_method\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: t\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 25\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[43mSubsampledSignal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_save_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m {b: get_num_samples(signal, b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bs}\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signal, num_samples\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sparse_transform/qsft/signals/input_signal.py:42\u001b[0m, in \u001b[0;36mSignal.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mInitializes the Signal object.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Keyword arguments to set signal parameters.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sparse_transform/qsft/signals/input_signal_subsampled.py:68\u001b[0m, in \u001b[0;36mSubsampledSignal._init_signal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubsampling_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqsft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_Ms_and_Ds_qsft()\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subsample_qsft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_transforms()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sparse_transform/qsft/signals/input_signal_subsampled.py:99\u001b[0m, in \u001b[0;36mSubsampledSignal._subsample_qsft\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfoldername:\n\u001b[1;32m     97\u001b[0m     Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfoldername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_or_load_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sparse_transform/qsft/signals/input_signal_subsampled.py:121\u001b[0m, in \u001b[0;36mSubsampledSignal._compute_or_load_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m pbar\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMs) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(query_indices)\n\u001b[1;32m    120\u001b[0m all_query_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(query_indices)\n\u001b[0;32m--> 121\u001b[0m all_subs_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_query_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(query_indices)):\n\u001b[1;32m    123\u001b[0m     samples[k] \u001b[38;5;241m=\u001b[39m all_subs_samples[k \u001b[38;5;241m*\u001b[39m block_length: (k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m block_length]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sparse_transform/qsft/signals/input_signal_subsampled.py:189\u001b[0m, in \u001b[0;36mSubsampledSignal.subsample\u001b[0;34m(self, query_indices)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLMX/SHapRAG/rag_shap.py:661\u001b[0m, in \u001b[0;36mContextAttribution.compute_spex.<locals>.value_function\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    659\u001b[0m     ablated_context \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m selected_indexes]\n\u001b[1;32m    660\u001b[0m     ablated_context_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ablated_context)\n\u001b[0;32m--> 661\u001b[0m     out_values\u001b[38;5;241m.\u001b[39mappend(\u001b[43mvaluef_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mablated_context_str\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_values\n",
      "File \u001b[0;32m~/LLMX/SHapRAG/rag_shap.py:649\u001b[0m, in \u001b[0;36mContextAttribution.compute_spex.<locals>.CallCounter.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLMX/SHapRAG/rag_shap.py:638\u001b[0m, in \u001b[0;36mContextAttribution.compute_spex.<locals>.raw_value_function\u001b[0;34m(context_str)\u001b[0m\n\u001b[1;32m    635\u001b[0m subset_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m ablated_items \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# Use cached utility function\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_utility\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutility_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLMX/SHapRAG/rag_shap.py:114\u001b[0m, in \u001b[0;36mContextAttribution.get_utility\u001b[0;34m(self, subset_tuple, mode)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Compute the utility if not found in cache\u001b[39;00m\n\u001b[1;32m    113\u001b[0m context_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ablated_context_from_vector(np\u001b[38;5;241m.\u001b[39marray(subset_tuple))\n\u001b[0;32m--> 114\u001b[0m utility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_response_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Store in the nested cache structure\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutility_cache[subset_tuple][mode] \u001b[38;5;241m=\u001b[39m utility\n",
      "File \u001b[0;32m~/LLMX/SHapRAG/rag_shap.py:200\u001b[0m, in \u001b[0;36mContextAttribution._compute_response_metric\u001b[0;34m(self, context_str, mode, response)\u001b[0m\n\u001b[1;32m    197\u001b[0m         final_metric \u001b[38;5;241m=\u001b[39m logit(prob) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m prob \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog-perplexity\u001b[39m\u001b[38;5;124m'\u001b[39m: final_metric \u001b[38;5;241m=\u001b[39m total_log_prob \u001b[38;5;241m/\u001b[39m num_answer_tokens\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m logits, shift_logits, log_probs_all, answer_log_probs, total_log_prob; \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_metric\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(final_metric, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m final_metric\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdivergence_utility\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# --- MODIFIED DIVERGENCE LOGIC ---\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:222\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 222\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_questions_to_run=50\n",
    "# num_questions_to_run=1\n",
    "k_values = [1,2,3,4,5]\n",
    "all_results=[]\n",
    "\n",
    "# Define ground truth set of docs for precision (adapt as needed)\n",
    "# e.g., if first 2 docs are always relevant\n",
    "# def get_gtset_k():\n",
    "#     return [0, 1]\n",
    "\n",
    "for i in tqdm(range(num_questions_to_run), disable=not accelerator_main.is_main_process):\n",
    "    query = df.Sentences[i]\n",
    "    gt=ast.literal_eval(df_save.labels[i])\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs=df.paragraphs[i]\n",
    "    utility_cache_base_dir = f\"../Experiment_data/musique/{model_path.split('/')[1]}/sentence\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\"\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model=prepared_model,\n",
    "        prepared_tokenizer=tokenizer,\n",
    "        accelerator=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        methods_results = {}\n",
    "        metrics_results = {}\n",
    "        extra_results = {}\n",
    "\n",
    "        m_samples_map = {\"L\": 364}\n",
    "\n",
    "        # Store FM models for later R²/MSE\n",
    "        fm_models = {}\n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2 ** len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2 ** len(docs) - 1 if 2 ** len(docs) > 0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0:\n",
    "                methods_results[f\"ContextCite{actual_samples}\"], model_cc = harness.compute_contextcite(\n",
    "                    num_samples=actual_samples, seed=SEED\n",
    "                )\n",
    "\n",
    "                attributions, _ = harness.compute_spex(sample_budget=actual_samples, max_order=2)\n",
    "                methods_results[f\"FBII{actual_samples}\"] = attributions['fbii']\n",
    "                methods_results[f\"Spex{actual_samples}\"] = attributions['fourier']\n",
    "                methods_results[f\"FSII{actual_samples}\"] = attributions['fsii']\n",
    "\n",
    "                # methods_results[f\"FM_WeightsDU{actual_samples}\"], Fdu, modelfmdu = harness.compute_wss(\n",
    "                #     num_samples=actual_samples, seed=SEED, sampling=\"uniform\",\n",
    "                #     sur_type=\"fm\", utility_mode=\"divergence_utility\"\n",
    "                # )\n",
    "                # methods_results[f\"FM_WeightsDK{actual_samples}\"], Fdk, modelfmdk = harness.compute_wss(\n",
    "                #     num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",\n",
    "                #     sur_type=\"fm\", utility_mode=\"divergence_utility\"\n",
    "                # )\n",
    "                methods_results[f\"FM_WeightsLK{actual_samples}\"], Flk, modelfmlk = harness.compute_wss(\n",
    "                    num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\", sur_type=\"fm\"\n",
    "                )\n",
    "                methods_results[f\"FM_WeightsLU{actual_samples}\"], Flu, modelfmlu = harness.compute_wss(\n",
    "                    num_samples=actual_samples, seed=SEED, sampling=\"uniform\", sur_type=\"fm\"\n",
    "                )\n",
    "\n",
    "                # Save FM models\n",
    "                fm_models.update({\n",
    "                    # f\"FM_WeightsDU{actual_samples}\": modelfmdu,\n",
    "                    # f\"FM_WeightsDK{actual_samples}\": modelfmdk,\n",
    "                    f\"FM_WeightsLK{actual_samples}\": modelfmlk,\n",
    "                    f\"FM_WeightsLU{actual_samples}\": modelfmlu\n",
    "                })\n",
    "\n",
    "                # Save extra Fs\n",
    "                extra_results.update({\n",
    "                    # \"Fdu\": Fdu,\n",
    "                    # \"Fdk\": Fdk,\n",
    "                    \"Flk\": Flk,\n",
    "                    \"Flu\": Flu\n",
    "                })\n",
    "\n",
    "        methods_results[\"LOO\"] = harness.compute_loo()\n",
    "        methods_results[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "\n",
    "        # --- Evaluation Metrics ---\n",
    "        metrics_results[\"topk_probability\"] = harness.evaluate_topk_performance(\n",
    "            methods_results, k_values, utility_type=\"probability\"\n",
    "        )\n",
    "        metrics_results[\"topk_divergence\"] = harness.evaluate_topk_performance(\n",
    "            methods_results, k_values, utility_type=\"divergence\"\n",
    "        )\n",
    "        metrics_results[\"topk_response_probability\"] = harness.top_k_response_probability(\n",
    "            methods_results, k_values=[1, 3, 5]\n",
    "        )\n",
    "\n",
    "        # R² and MSE for ContextCite\n",
    "        r2, mse = harness.r2_mse(30, 'logit-prob', model_cc, method='cc')\n",
    "        metrics_results[\"R2_cc\"] = r2\n",
    "        metrics_results[\"MSE_cc\"] = mse\n",
    "\n",
    "        # R² and MSE for each FM method that has a model\n",
    "        for method_name, fm_model in fm_models.items():\n",
    "            r2, mse = harness.r2_mse(30, 'logit-prob', fm_model, method='fm')\n",
    "            metrics_results[f\"R2_{method_name}\"] = r2\n",
    "            metrics_results[f\"MSE_{method_name}\"] = mse\n",
    "\n",
    "        # LDS per method\n",
    "        LDS = {}\n",
    "        for method_name, scores in methods_results.items():\n",
    "            if \"FM_WeightsLU\" in method_name:\n",
    "                LDS[method_name] = harness.lds(scores, 30, utl=True, model=modelfmlu)\n",
    "            else:\n",
    "                LDS[method_name] = harness.lds(scores, 30)\n",
    "        metrics_results[\"LDS\"] = LDS\n",
    "\n",
    "        # Precision per method\n",
    "        precision_scores = {}\n",
    "        gtset_k = gt\n",
    "        for method_name, scores in methods_results.items():\n",
    "            precision_scores[method_name] = harness.precision(gtset_k, scores)\n",
    "        metrics_results[\"precision\"] = precision_scores\n",
    "\n",
    "        harness.save_utility_cache(current_utility_path)\n",
    "\n",
    "        all_results.append({\n",
    "            \"query_index\": i,\n",
    "            \"query\": query,\n",
    "            \"ground_truth\": df.answer[i],\n",
    "            \"methods\": methods_results,\n",
    "            \"metrics\": metrics_results,\n",
    "            \"extra\": extra_results\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_and_print(all_results, k_values=[1, 3, 5]):\n",
    "    table_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Mapping for consistency\n",
    "    method_name_map = {\n",
    "        \"cc\": \"ContextCite364\"  # rename cc to full name\n",
    "    }\n",
    "\n",
    "    for res in all_results:\n",
    "        metrics = res[\"metrics\"]\n",
    "\n",
    "        # R² / MSE\n",
    "        for key, val in metrics.items():\n",
    "            if key.startswith(\"R2_\") or key.startswith(\"MSE_\"):\n",
    "                raw_method = key.split(\"_\", 1)[1]\n",
    "                method = method_name_map.get(raw_method, raw_method)  # rename if needed\n",
    "                metric_name = key.split(\"_\", 1)[0]  # \"R2\" or \"MSE\"\n",
    "                table_data[method][metric_name].append(val)\n",
    "\n",
    "        # LDS\n",
    "        for method_name, lds_val in metrics[\"LDS\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"LDS\"].append(lds_val)\n",
    "\n",
    "        # Precision\n",
    "        for method_name, prec_val in metrics[\"precision\"].items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"precision\"].append(prec_val)\n",
    "\n",
    "        # Top-k\n",
    "        for metric_type in [\"topk_probability\", \"topk_divergence\", \"topk_response_probability\"]:\n",
    "            for method_name, k_dict in metrics[metric_type].items():\n",
    "                method = method_name_map.get(method_name, method_name)\n",
    "                for k in k_values:\n",
    "                    if k in k_dict:\n",
    "                        col_name = f\"{metric_type}_k{k}\"\n",
    "                        table_data[method][col_name].append(k_dict[k])\n",
    "\n",
    "    # Averages\n",
    "    avg_table = {\n",
    "        method: {metric: np.nanmean(values) for metric, values in metric_dict.items()}\n",
    "        for method, metric_dict in table_data.items()\n",
    "    }\n",
    "\n",
    "    df_summary = pd.DataFrame.from_dict(avg_table, orient=\"index\").sort_index()\n",
    "\n",
    "    print(\"\\n=== Metrics Summary Across All Queries ===\")\n",
    "    print(df_summary.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "    return df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_and_print(all_results, k_values=[1, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_recall_at_k(all_results, k_values=[1,2, 3,4, 5]):\n",
    "    methods = list(all_results[0][\"methods\"].keys())\n",
    "    recall_table = {m: [] for m in methods}\n",
    "\n",
    "    for k in k_values:\n",
    "        for method in methods:\n",
    "            recalls = []\n",
    "            for p, res in enumerate(all_results):\n",
    "                gt_indices = set(ast.literal_eval(df_save.labels[p]))  # must exist in each result\n",
    "                scores = np.array(res[\"methods\"][method])\n",
    "                topk_indices = set(scores.argsort()[-k:])\n",
    "                hits = len(gt_indices & topk_indices)\n",
    "                recalls.append(hits / len(gt_indices) if gt_indices else np.nan)\n",
    "            recall_table[method].append(np.nanmean(recalls))\n",
    "    return recall_table\n",
    "\n",
    "\n",
    "def plot_recall_at_k(recall_table, k_values=[1,2, 3,4, 5]):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for method, recalls in recall_table.items():\n",
    "        plt.plot(k_values, recalls, marker=\"o\", label=method)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Recall@k\")\n",
    "    plt.title(\"Recall@k for All Methods\")\n",
    "    plt.xticks(k_values)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_table=compute_recall_at_k(all_results, k_values=[1, 2, 3, 4, 5])\n",
    "plot_recall_at_k(recall_table, k_values=[1, 2, 3, 4, 5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
