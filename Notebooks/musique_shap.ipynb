{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../data/musique/musique_ans_v1.0_train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(lst):\n",
    "    # Titles where is_supporting is True\n",
    "    supporting = [d['paragraph_text'] for d in lst if d.get('is_supporting') == True]\n",
    "    # Titles where is_supporting is False or missing AND not already in supporting\n",
    "    others = [d['paragraph_text'] for d in lst if d.get('is_supporting') != True and d['paragraph_text'] not in supporting]\n",
    "    # Combine: all supporting + as many others as needed to reach 10\n",
    "    result = supporting + others\n",
    "    return result[:10]\n",
    "\n",
    "df.paragraphs=df.paragraphs.apply(get_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentences'] = df['paragraphs'].apply(\n",
    "    lambda para_list: [sent for para in para_list for sent in nltk.sent_tokenize(para)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"paragraphs\"] = df[\"paragraphs\"].apply(lambda p: p[:5]+ [p[1]] + p[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1/5: When was the institute that owned The Collegian founded?... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/sentence/utilities_q_idx0.pkl...\n",
      "Successfully loaded 701 cached utility entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 11759.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Houston Baptist University was founded in 1960.\n",
      "GT: 1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing utilities for ContextCite: 100%|██████████| 364/364 [00:00<00:00, 342699.59it/s]\n",
      "Computing utilities for WSS (kernelshap): 100%|██████████| 298/298 [00:00<00:00, 179094.80it/s]\n",
      "Computing utilities for WSS (kernelshap): 100%|██████████| 298/298 [00:00<00:00, 388651.30it/s]\n",
      "LOO Calls (logit-prob): 100%|██████████| 25/25 [00:00<00:00, 123361.88it/s]\n",
      "LOO Calls (divergence_utility): 100%|██████████| 25/25 [00:00<00:00, 115864.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Divergence Utility) Caching baseline token distributions for full context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:26<05:45, 86.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Process: Saving 706 utility entries to ../Experiment_data/musique/Llama-3.1-8B-Instruct/sentence/utilities_q_idx0.pkl...\n",
      "Save complete.\n",
      "\n",
      "--- Question 2/5: What year saw the creation of the region where the county of... ---\n",
      "Main Process: Attempting to load utility cache from ../Experiment_data/musique/Llama-3.1-8B-Instruct/sentence/utilities_q_idx1.pkl...\n",
      "Successfully loaded 733 cached utility entries.\n",
      "Response: 1994.\n",
      "GT: 1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing utilities for ContextCite: 100%|██████████| 364/364 [00:00<00:00, 257675.38it/s]\n",
      "Computing utilities for WSS (kernelshap): 100%|██████████| 319/319 [00:00<00:00, 205843.53it/s]\n",
      "Computing utilities for WSS (kernelshap): 100%|██████████| 319/319 [00:00<00:00, 149729.52it/s]\n",
      "LOO Calls (logit-prob): 100%|██████████| 36/36 [00:00<00:00, 99996.65it/s]\n",
      "LOO Calls (divergence_utility): 100%|██████████| 36/36 [00:00<00:00, 113189.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# num_questions_to_run=len(df.question)\n",
    "num_questions_to_run=5\n",
    "k_values = [1,2,3,4,5]\n",
    "all_results=[]\n",
    "LDSs=[]\n",
    "r2_fm=[]\n",
    "r2_cc=[]\n",
    "for i in tqdm(range(num_questions_to_run), disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs=df.Sentences[i]\n",
    "    utility_cache_base_dir = f\"../Experiment_data/musique/{model_path.split('/')[1]}/sentence\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model=prepared_model,\n",
    "        prepared_tokenizer=tokenizer,\n",
    "        accelerator=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "\n",
    "    print(f'Response: {harness.target_response}')\n",
    "    print(f'GT: {df.answer[i]}')\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "    if accelerator_main.is_main_process:\n",
    "        m_samples_map = {\"L\": 364} \n",
    "        # m_samples_map = {\"L\": 128, \"XL\":256, \"XXL\":512} \n",
    "        T_iterations_map = {\"L\":40, \"XL\":50, \"XXL\":60} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0:\n",
    "                results_for_query[f\"ContextCite{actual_samples}\"], model_cc = harness.compute_contextcite(num_samples=actual_samples, seed=SEED)\n",
    "                attributions, ints=harness.compute_spex(sample_budget=actual_samples,max_order=2)\n",
    "                results_for_query[f\"FBII{actual_samples}\"]=attributions['fbii']\n",
    "                results_for_query[f\"Spex{actual_samples}\"]=attributions['fourier']\n",
    "                results_for_query[f\"FSII{actual_samples}\"]=attributions['fsii']\n",
    "                results_for_query[f\"FM_WeightsD{actual_samples}\"], F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\", utility_mode=\"divergence_utility\")\n",
    "                results_for_query[f\"FM_Weights{actual_samples}\"], F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\")\n",
    "                # results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=16, beta_b=1, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                # results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "        results_for_query[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "\n",
    "        prob_topk = harness.evaluate_topk_performance(\n",
    "                                                results_for_query, \n",
    "                                                k_values, \n",
    "                                                utility_type=\"probability\"\n",
    "                                            )\n",
    "\n",
    "        div_topk = harness.evaluate_topk_performance(\n",
    "                                            results_for_query, \n",
    "                                            k_values, \n",
    "                                            utility_type=\"divergence\"\n",
    "                                        )\n",
    "        \n",
    "        # r2_fm.append([harness.r2_mse(30, modelfm, method='fm')])\n",
    "        # r2_cc.append([harness.r2_mse(30, model_cc, method='cc')])\n",
    "\n",
    "        LDS = {}\n",
    "        for i in results_for_query:\n",
    "            if \"FM\" in i:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30, utl=True, model=modelfm)}\n",
    "                LDS.update(calculate_LDS)\n",
    "            else:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30)}\n",
    "                LDS.update(calculate_LDS)\n",
    "        LDS = [{i:harness.lds(results_for_query[i], 30)} for i in results_for_query]\n",
    "\n",
    "        results_for_query[\"topk_probability\"] = prob_topk\n",
    "        results_for_query[\"topk_divergence\"] = div_topk\n",
    "        results_for_query[\"LDS\"] = LDS\n",
    "        harness.save_utility_cache(current_utility_path)\n",
    "        \n",
    "        all_results.append(results_for_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.r2(30, model_cc, method='cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Spex512','FBII512', 'FSII512',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [f'ContextCite{actual_samples}', f'FM_Weights{actual_samples}',f'FM_WeightsD{actual_samples}',f'Spex{actual_samples}',f'FBII{actual_samples}',f'FSII{actual_samples}', 'LOO', 'ARC-JSD']\n",
    "\n",
    "# Initialize lists\n",
    "topk_probs = {method: [] for method in methods}\n",
    "topk_divs = {method: [] for method in methods}\n",
    "LDSs = {method: [] for method in methods}\n",
    "\n",
    "# Collect values\n",
    "for ind, entry in enumerate(all_results):\n",
    "    for method in methods:\n",
    "        topk_probs[method].append(entry['topk_probability'][method][3])\n",
    "        topk_divs[method].append(entry['topk_divergence'][method][3])\n",
    "        for d in entry['LDS']:\n",
    "            if method in d:\n",
    "                LDSs[method].append(d[method])\n",
    "                break\n",
    "        \n",
    "\n",
    "# Compute means\n",
    "mean_topk_probs = {method: np.mean(topk_probs[method]) for method in methods}\n",
    "mean_topk_divs = {method: np.mean(topk_divs[method]) for method in methods}\n",
    "mean_LDSs = {method: np.mean(LDSs[method]) for method in methods}\n",
    "\n",
    "print(\"Mean topk_probability:\", mean_topk_probs)\n",
    "print(\"Mean topk_divergence:\", mean_topk_divs)\n",
    "print(\"Mean LDS:\", mean_LDSs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.question[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[0]['FM_WeightsD364']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,np.array(all_results[1][i]).argsort()) for i in methods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.paragraphs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sentences[10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[all_results[0][i].argsort() for i in methods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[4]['LOO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(r2_fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(r2_cc)):\n",
    "    print(r2_cc[i], r2_fm[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
