{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from SHapRAG import*\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from itertools import combinations\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer # type: ignore\n",
    "# from google.oauth2 import service_account\n",
    "# import vertexai\n",
    "# from vertexai.generative_models import GenerativeModel, Image, Part\n",
    "# tqdm.pandas()\n",
    "\n",
    "# vertexai.init(\n",
    "#     project=\"oag-ai\",\n",
    "#     credentials=service_account.Credentials.from_service_account_file(\"google-credentials.json\"),\n",
    "# )\n",
    "torch.cuda.set_device(1)  # Use GPU1, or 2, or 3\n",
    "\n",
    "splits = {'train': 'question-answer-passages/train-00000-of-00001.parquet', 'test': 'question-answer-passages/test-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/enelpol/rag-mini-bioasq/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet(\"hf://datasets/enelpol/rag-mini-bioasq/text-corpus/test-00000-of-00001.parquet\")\n",
    "df1['passage']=df1['passage'].str.replace(r'[\\n]', ' ', regex=True)\n",
    "df['question']=df['question'].str.replace(r'[\\n]', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embs(qtext, model=\"nomic\"):\n",
    "    if model==\"nomic\":\n",
    "    \n",
    "        data = {\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": qtext\n",
    "        }\n",
    "        return np.array(requests.post('http://localhost:11434/api/embeddings', json=data).json()['embedding'])\n",
    "    else:\n",
    "        return SentenceTransformer(\"abhinand/MedEmbed-large-v0.1\").encode([qtext], convert_to_numpy=True)\n",
    "    \n",
    "# df1['embedding'] = df1['passage'].progress_apply(lambda x: gen_embs(x, model='medemb'))\n",
    "\n",
    "# # Save the embeddings to a pickle file\n",
    "# with open('embed_bioasq_medemb.pkl', 'wb') as f:\n",
    "#     pickle.dump(df1['embedding'].tolist(), f)\n",
    "# print(\"Embeddings saved to embed_bioasq_medemb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/embed_bioasq_medemb.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "embeddings = normalize_embeddings(embeddings)\n",
    "\n",
    "def index_documents(method=\"faiss\", index_name=\"recipes_nomic\", es_host=\"http://localhost:9200\"):\n",
    "    if method == \"faiss\":\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, \"/data/bioasq_nomic_faiss.index\")\n",
    "        print(\"FAISS index saved.\")\n",
    "        return index\n",
    "    elif method == \"elasticsearch\":\n",
    "        es = Elasticsearch(es_host)\n",
    "        mapping = {\"mappings\": {\"properties\": {\"text\": {\"type\": \"text\"}, \"vector\": {\"type\": \"dense_vector\", \"dims\": embeddings.shape[1]}}}}\n",
    "        es.indices.create(index=index_name, body=mapping, ignore=400)\n",
    "        for i, (text, vector) in enumerate(zip(documents, embeddings)):\n",
    "            es.index(index=index_name, id=i, body={\"text\": text, \"vector\": vector.tolist()})\n",
    "        print(\"Elasticsearch index created.\")\n",
    "        return es\n",
    "# index_documents(method=\"faiss\", index_name=\"bioasq_nomic_faiss\", es_host=\"http://localhost:9200\")\n",
    "faiss_index = faiss.read_index(\"/data/medemb_bioasq_faiss.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, k=5):\n",
    "    query_embedding = gen_embs(query, model=\"medemb\")\n",
    "    query_embedding = normalize_embeddings(query_embedding.reshape(1, -1))\n",
    "    scores, indices = faiss_index.search(query_embedding, k)\n",
    "    return [df1['passage'][i] for i in indices[0]], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='What is the implication of histone lysine methylation in medulloblastoma?'\n",
    "docs, scores=retrieve_documents(query=query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13011"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\".join(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instantiating ShapleyExperimentHarness (will pre-compute all utilities)...\n",
      "Loading LLM 'meta-llama/Meta-Llama-3.1-8B-Instruct' on device 'cuda'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n",
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/transformers/src/transformers/generation/configuration_utils.py:641: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded successfully.\n",
      "Generating target_response using all items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 13751.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated target_response: 'The implication of histone lysine methylation in medulloblastoma is that it contributes to the pathogenesis of the disease. The study found that copy number aberrations of genes involved in histone lysine methylation, particularly at H3'\n",
      "Pre-computing utilities for all 32 subsets (n=5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-computing Utilities: 100%|██████████| 32/32 [06:39<00:00, 12.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computation complete. Made 32 LLM calls.\n",
      "Harness target_response automatically generated: 'The implication of histone lysine methylation in medulloblastoma is that it contributes to the pathogenesis of the disease. The study found that copy number aberrations of genes involved in histone lysine methylation, particularly at H3'\n",
      "Number of items (n): 5\n",
      "\n",
      "--- Computing Attributions using Harness (from pre-computed utilities) ---\n",
      "Computing ContextCite Weights (m=64, using pre-computed utilities)...\n",
      "Computing Weakly Supervised Shapley (m=64, using pre-computed utilities)...\n",
      "Computing TMC-Shapley (T=100, using pre-computed utilities)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Beta-Shapley (T=100, α=0.5, β=0.5, using pre-computed utilities)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing LOO (n=5, using pre-computed utilities)...\n",
      "Computing Exact Shapley (using pre-computed utilities)...\n",
      "\n",
      "\n",
      "--- Attribution Scores (from Harness) ---\n",
      "       ContextCite      WSS      TMC  BetaShap (U)      LOO    Exact\n",
      "Doc 0      58.7644  58.4856  59.2063       57.7403  45.8141  58.4856\n",
      "Doc 1       1.7093   3.4574   3.9607        7.7198   1.1588   3.4574\n",
      "Doc 2       3.4685   3.8260   3.6045        5.6109  -1.0610   3.8260\n",
      "Doc 3       7.9895   6.6452   5.7517        4.3082   1.2248   6.6452\n",
      "Doc 4       1.6065   2.2331   2.1240        3.9184   0.6422   2.2331\n",
      "\n",
      "--- Evaluation Metrics vs Exact Shapley ---\n",
      "              Pearson  Spearman\n",
      "Method                         \n",
      "ContextCite    0.9990       1.0\n",
      "WSS            1.0000       1.0\n",
      "TMC            0.9998       0.9\n",
      "BetaShap (U)   0.9951       0.6\n",
      "LOO            0.9976       0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\nInstantiating ShapleyExperimentHarness (will pre-compute all utilities)...\")\n",
    "harness = ShapleyExperimentHarness(\n",
    "    items=docs,\n",
    "    query=query,\n",
    "    # llm_model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", # Use a smaller model for faster demo if preferred\n",
    "    llm_model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", # Or your chosen model\n",
    "    verbose=True\n",
    ")\n",
    "print(f\"Harness target_response automatically generated: '{harness.target_response}'\")\n",
    "print(f\"Number of items (n): {harness.n_items}\")\n",
    "\n",
    "# 3. Compute Attributions using different methods from the harness\n",
    "results = {}\n",
    "seed = 42 # For reproducibility of stochastic methods\n",
    "\n",
    "print(\"\\n--- Computing Attributions using Harness (from pre-computed utilities) ---\")\n",
    "\n",
    "# Adjust num_samples for WSS/ContextCite if n is very small\n",
    "m_samples_for_approx = 64\n",
    "results[\"ContextCite\"] = harness.compute_contextcite_weights(num_samples=m_samples_for_approx, lasso_alpha=0.0, seed=seed) # LinReg\n",
    "results[\"WSS\"] = harness.compute_wss(num_samples=m_samples_for_approx, lasso_alpha=0.0, seed=seed) # LinReg\n",
    "\n",
    "T_iterations = harness.n_items * 20 # Adjust iterations as needed\n",
    "results[\"TMC\"] = harness.compute_tmc_shap(num_iterations=T_iterations, performance_tolerance=0.001, seed=seed)\n",
    "if beta_dist: \n",
    "    results[\"BetaShap (U)\"] = harness.compute_beta_shap(num_iterations=T_iterations, beta_a=0.5, beta_b=0.5, seed=seed)\n",
    "\n",
    "\n",
    "results[\"LOO\"] = harness.compute_loo()\n",
    "results[\"Exact\"] = harness.compute_exact_shap() # n=6 is 64 calls, feasible\n",
    "\n",
    "# 4. Display Results\n",
    "print(\"\\n\\n--- Attribution Scores (from Harness) ---\")\n",
    "# Create item labels\n",
    "item_labels = [f'Doc {i}' for i in range(harness.n_items)]\n",
    "\n",
    "# Filter out None results before creating DataFrame\n",
    "valid_results = {k:v for k, v in results.items() if v is not None and isinstance(v, np.ndarray) and len(v) == harness.n_items}\n",
    "\n",
    "if valid_results:\n",
    "    results_df = pd.DataFrame(valid_results, index=item_labels)\n",
    "    print(results_df.round(4))\n",
    "\n",
    "    if \"Exact\" in valid_results:\n",
    "        print(\"\\n--- Evaluation Metrics vs Exact Shapley ---\")\n",
    "        metrics_data = []\n",
    "        exact_scores = valid_results[\"Exact\"]\n",
    "        for method, approx_scores in valid_results.items():\n",
    "            if method != \"Exact\":\n",
    "\n",
    "                # Handle potential constant arrays for correlation\n",
    "                if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                    pearson_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                    spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                else:\n",
    "                    pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                    spearman_c, _ = spearmanr(exact_scores, approx_scores)\n",
    "                \n",
    "                metrics_data.append({\n",
    "                    \"Method\": method,\n",
    "                    \"Pearson\": pearson_c,\n",
    "                    \"Spearman\": spearman_c\n",
    "                })\n",
    "        \n",
    "        if metrics_data:\n",
    "            metrics_df = pd.DataFrame(metrics_data).set_index(\"Method\")\n",
    "            print(metrics_df.round(4))\n",
    "        else:\n",
    "            print(\"No approximate methods to compare against Exact.\")\n",
    "else:\n",
    "    print(\"No valid attribution results were computed by the harness.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "evaluate_shap(query, n_rag=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)  # Use GPU1, or 2, or 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Initializing Experiment Harness (Pre-computing all utilities) ---\")\n",
    "start_harness_time = time.time()\n",
    "harness = ShapleyExperimentHarness(\n",
    "    items=docs,\n",
    "    query=query,\n",
    "    target_response=target_response,\n",
    "    llm_caller=compute_logprob# Use placeholder for speed\n",
    ")\n",
    "print(f\"Harness initialized in {time.time() - start_harness_time:.2f}s. \"\n",
    "        f\"{len(harness.all_true_utilities)} utilities computed.\")\n",
    "\n",
    "results_exp = {}\n",
    "seed_exp = 42\n",
    "\n",
    "print(\"\\n--- Running Methods using Pre-computed Utilities ---\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"Exact\"] = harness.compute_exact_shap()\n",
    "print(f\"Exact Shapley (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"ContextCite\"] = harness.compute_contextcite_weights(num_samples=16, lasso_alpha=0.0, seed=seed_exp)\n",
    "print(f\"ContextCite (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"WSS\"] = harness.compute_wss(num_samples=16, lasso_alpha=0.0, seed=seed_exp)\n",
    "print(f\"WSS (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"TMC\"] = harness.compute_tmc_shap(num_iterations=harness.n_items * 10, performance_tolerance=0.001, seed=seed_exp)\n",
    "print(f\"TMC (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "if beta_dist:\n",
    "    start_method_time = time.time()\n",
    "    results_exp[\"BetaShap (U)\"] = harness.compute_beta_shap(num_iterations=harness.n_items * 10, beta_a=0.5, beta_b=0.5, seed=seed_exp)\n",
    "    print(f\"BetaShap (U, from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"LOO\"] = harness.compute_loo()\n",
    "print(f\"LOO (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "# Display Results\n",
    "print(\"\\n\\n--- Experiment Harness: Comparison Table ---\")\n",
    "valid_results_exp = {k:v for k, v in results_exp.items() if v is not None}\n",
    "if valid_results_exp:\n",
    "    results_df_exp = pd.DataFrame(valid_results_exp, index=[f'Item {i}' for i in range(harness.n_items)])\n",
    "    print(results_df_exp.round(4))\n",
    "\n",
    "else:\n",
    "    print(\"No valid results were computed by the harness.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "if \"Exact\" in valid_results_exp:\n",
    "            print(\"\\n--- Experiment Harness: Metrics vs Exact Shapley ---\")\n",
    "            exact_scores = valid_results_exp[\"Exact\"]\n",
    "            for method, approx_scores in valid_results_exp.items():\n",
    "                if method != \"Exact\" and approx_scores is not None and len(approx_scores) == len(exact_scores):\n",
    "                    mae = np.mean(np.abs(exact_scores - approx_scores))\n",
    "                    \n",
    "                    # Calculate Pearson and Spearman, handle potential constant arrays\n",
    "                    if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                        # If one or both are constant, correlation might be ill-defined or 0/1\n",
    "                        # pearsonr/spearmanr might return NaN or raise warning for constant input.\n",
    "                        pearson_corr_val = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                        spearman_corr_val = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                    else:\n",
    "                        try:\n",
    "                            pearson_corr_val, _ = pearsonr(exact_scores, approx_scores)\n",
    "                            spearman_corr_val, _ = spearmanr(exact_scores, approx_scores)\n",
    "                        except ValueError: # e.g. if NaNs are present or other issues\n",
    "                            pearson_corr_val = np.nan\n",
    "                            spearman_corr_val = np.nan\n",
    "                            \n",
    "                    print(f\"{method}: MAE={mae:.4f}, Pearson={pearson_corr_val:.4f}, Spearman={spearman_corr_val:.4f}\")\n",
    "                elif method != \"Exact\":\n",
    "                    print(f\"{method}: Could not compute metrics (scores missing or length mismatch).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "if exact_values is not None:\n",
    "    print(\"Spearman rank correlation score:\")\n",
    "    if wss_values is not None:\n",
    "        mae_wss,_ = spearmanr(exact_values, wss_values)\n",
    "        print(f\" WSS vs Exact: {mae_wss}\")\n",
    "    if tmc_values is not None:\n",
    "        mae_tmc,_ = spearmanr(exact_values, tmc_values)\n",
    "        print(f\" TMC vs Exact: {mae_tmc}\")\n",
    "    if beta_dist and beta_u_values is not None:\n",
    "        mae_beta_u,_ = spearmanr(exact_values, beta_u_values)\n",
    "        print(f\" BetaShap (U) vs Exact: {mae_beta_u}\")\n",
    "    # Compare LOO to Exact if available\n",
    "    if exact_values is not None and loo_values is not None:\n",
    "        mae_loo ,_= spearmanr(exact_values, loo_values)\n",
    "        print(f\" LOO vs Exact: {mae_loo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exact_values is not None:\n",
    "    print(\"Pearson correlation score:\")\n",
    "    if wss_values is not None:\n",
    "        mae_wss,_ = pearsonr(exact_values, wss_values)\n",
    "        print(f\" WSS vs Exact: {mae_wss}\")\n",
    "    if tmc_values is not None:\n",
    "        mae_tmc,_ = pearsonr(exact_values, tmc_values)\n",
    "        print(f\" TMC vs Exact: {mae_tmc}\")\n",
    "    if beta_dist and beta_u_values is not None:\n",
    "        mae_beta_u,_ = pearsonr(exact_values, beta_u_values)\n",
    "        print(f\" BetaShap (U) vs Exact: {mae_beta_u}\")\n",
    "    # Compare LOO to Exact if available\n",
    "    if exact_values is not None and loo_values is not None:\n",
    "        mae_loo ,_= pearsonr(exact_values, loo_values)\n",
    "        print(f\" LOO vs Exact: {mae_loo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub='ollama'\n",
    "model=\"llama2\"\n",
    "respin=query_rag(query=query, retrieved_docs=docs, hub=hub, model=model)\n",
    "print(respin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_precision_recall(k_max, query, query_idx):\n",
    "    # Get initial documents and shapley scores\n",
    "    docs, _ = retrieve_documents(query=query, k=k_max)\n",
    "    shap = shapley_values(docs, query)\n",
    "    \n",
    "    # Select new query based on highest Shapley value\n",
    "    new_query = docs[np.argmax(shap)]\n",
    "\n",
    "    # Initialize metrics\n",
    "    precision, recall, f1score = [], [], []\n",
    "    precision_new, recall_new, f1score_new = [], [], []\n",
    "\n",
    "    relevant_ids = set(df['relevant_passage_ids'][query_idx])\n",
    "\n",
    "    for k in range(1, k_max):\n",
    "        # Retrieve documents with both original and shap-reformulated queries\n",
    "        docs_old, _ = retrieve_documents(query=query, k=k)\n",
    "        docs_new, _ = retrieve_documents(query=new_query, k=k)\n",
    "\n",
    "        # Convert passages to IDs\n",
    "        try:\n",
    "            retrieved_ids_old = {\n",
    "                df1[df1['passage'] == passage]['id'].values.item() \n",
    "                for passage in docs_old \n",
    "                if not df1[df1['passage'] == passage]['id'].empty\n",
    "            }\n",
    "            retrieved_ids_new = {\n",
    "                df1[df1['passage'] == passage]['id'].values.item() \n",
    "                for passage in docs_new \n",
    "                if not df1[df1['passage'] == passage]['id'].empty\n",
    "            }\n",
    "        except ValueError:\n",
    "            # Handles cases where .item() fails due to multiple matches\n",
    "            continue\n",
    "\n",
    "        # Precision and Recall calculations\n",
    "        tp_old = len(relevant_ids.intersection(retrieved_ids_old))\n",
    "        tp_new = len(relevant_ids.intersection(retrieved_ids_new))\n",
    "\n",
    "        p_old = tp_old / k\n",
    "        r_old = tp_old / len(relevant_ids) if relevant_ids else 0\n",
    "\n",
    "        p_new = tp_new / k\n",
    "        r_new = tp_new / len(relevant_ids) if relevant_ids else 0\n",
    "\n",
    "        # F1 score with zero division check\n",
    "        f1_old = 2 * p_old * r_old / (p_old + r_old) if (p_old + r_old) > 0 else 0\n",
    "        f1_new = 2 * p_new * r_new / (p_new + r_new) if (p_new + r_new) > 0 else 0\n",
    "\n",
    "        # Append metrics\n",
    "        precision.append(p_old)\n",
    "        recall.append(r_old)\n",
    "        f1score.append(f1_old)\n",
    "\n",
    "        precision_new.append(p_new)\n",
    "        recall_new.append(r_new)\n",
    "        f1score_new.append(f1_new)\n",
    "\n",
    "    return precision, recall, f1score, precision_new, recall_new, f1score_new\n",
    "\n",
    "\n",
    "# Run evaluation on top-10 questions\n",
    "precision, recall, f1score = [], [], []\n",
    "precision_new, recall_new, f1score_new = [], [], []\n",
    "\n",
    "for j, query in enumerate(df['question'][:10]):\n",
    "    prec, rec, f1, prec_new, rec_new, f1_new = compute_precision_recall(5, query, j)\n",
    "    precision.append(prec)\n",
    "    recall.append(rec)\n",
    "    f1score.append(f1)\n",
    "    precision_new.append(prec_new)\n",
    "    recall_new.append(rec_new)\n",
    "    f1score_new.append(f1_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert lists of lists to numpy arrays for easier manipulation\n",
    "precision = np.array(precision)\n",
    "recall = np.array(recall)\n",
    "f1score = np.array(f1score)\n",
    "\n",
    "precision_new = np.array(precision_new)\n",
    "recall_new = np.array(recall_new)\n",
    "f1score_new = np.array(f1score_new)\n",
    "\n",
    "# Calculate average across queries\n",
    "avg_precision = precision.mean(axis=0)\n",
    "avg_recall = recall.mean(axis=0)\n",
    "avg_f1score = f1score.mean(axis=0)\n",
    "\n",
    "avg_precision_new = precision_new.mean(axis=0)\n",
    "avg_recall_new = recall_new.mean(axis=0)\n",
    "avg_f1score_new = f1score_new.mean(axis=0)\n",
    "\n",
    "k_values = list(range(1, precision.shape[1] + 1))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(k_values, avg_precision, label='Original', marker='o')\n",
    "plt.plot(k_values, avg_precision_new, label='Shap Retriever', marker='s')\n",
    "plt.title('Average Precision')\n",
    "plt.xlabel('Top-k')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(k_values, avg_recall, label='Original', marker='o')\n",
    "plt.plot(k_values, avg_recall_new, label='Shap Retriever', marker='s')\n",
    "plt.title('Average Recall')\n",
    "plt.xlabel('Top-k')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(k_values, avg_f1score, label='Original', marker='o')\n",
    "plt.plot(k_values, avg_f1score_new, label='Shap Retriever', marker='s')\n",
    "plt.title('Average F1 Score')\n",
    "plt.xlabel('Top-k')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(scores[0], docs):\n",
    "    print(f\"{a}\\t{b[:230]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(scores[0], docs):\n",
    "    print(f\"{a}\\t{b[:230]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub='google'\n",
    "model=\"publishers/meta/models/llama-3.3-70b-instruct-maas\"\n",
    "respin=query_rag(query=query, retrieved_docs=docs, hub=hub, model=model)\n",
    "print(respin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub='google'\n",
    "model=\"gemini-2.5-pro-exp-03-25\"\n",
    "resp=query_rag(query=query, retrieved_docs=docs, hub=hub, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(normalize_embeddings(gen_embs(df[df['question']==query]['answer'].values[0], model='medemb').reshape(1, -1)), normalize_embeddings(gen_embs(resp1, model='medemb').reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(normalize_embeddings(gen_embs(df[df['question']==query]['answer'].values[0], model='medemb').reshape(1, -1)), normalize_embeddings(gen_embs(shapmax, model='medemb').reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(normalize_embeddings(gen_embs(df[df['question']==query]['answer'].values[0], model='medemb').reshape(1, -1)), normalize_embeddings(gen_embs(shapres, model='medemb').reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap=shapley_values(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapres=ragshap(shap, retrival_type='reponse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapmax=ragshap(shap, retrival_type='max_shap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
