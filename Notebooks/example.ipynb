{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from SHapRAG import*\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import time\n",
    "# torch.cuda.set_device(1)  # Use GPU1, or 2, or 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The weather in Paris is sunny today.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"The sun is shining in Paris today\",\n",
    "    \"Berlin is the capital of Germany.\", # Irrelevant\n",
    "    # \"The Eiffel Tower is located in Paris, France.\",\n",
    "    # \"France borders several countries including Germany.\",\n",
    "    \"The currency used in France is the Euro.\",\n",
    "    \"Paris hosted the Summer Olympics in 1900 and 1924.\",\n",
    "    \"Germany uses the Euro as well.\", # Redundant info\n",
    "    \"It is cloudy in Berlin today.\"\n",
    "]\n",
    "queries = [\"What is the weather like in the capital of France?\", \"Germany is good?\", \"Paris is the capital of Germany?\"]\n",
    "# target_response = \"Paris is sunny.\" # The ideal answer fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instantiating ShapleyExperimentHarness (will pre-compute all utilities)...\n",
      "Loading LLM 'meta-llama/Meta-Llama-3.1-8B-Instruct' on device 'cuda'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s]\n",
      "/transformers/src/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/transformers/src/transformers/generation/configuration_utils.py:641: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded successfully.\n",
      "Generating target_response using all items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 12945.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated target_response: 'The weather in the capital of France (Paris) is sunny today.'\n",
      "Pre-computing utilities for all 256 subsets (n=8)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-computing Utilities: 100%|██████████| 256/256 [02:53<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computation complete. Made 256 LLM calls.\n",
      "Harness target_response automatically generated: 'The weather in the capital of France (Paris) is sunny today.'\n",
      "Number of items (n): 8\n",
      "\n",
      "--- Computing Attributions using Harness (from pre-computed utilities) ---\n",
      "Computing ContextCite Weights (m=64, using pre-computed utilities)...\n",
      "Computing Weakly Supervised Shapley (m=64, using pre-computed utilities)...\n",
      "Computing TMC-Shapley (T=160, using pre-computed utilities)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Beta-Shapley (T=160, α=0.5, β=0.5, using pre-computed utilities)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing LOO (n=8, using pre-computed utilities)...\n",
      "Computing Exact Shapley (using pre-computed utilities)...\n",
      "\n",
      "\n",
      "--- Attribution Scores (from Harness) ---\n",
      "       ContextCite      WSS      TMC  BetaShap (U)     LOO    Exact\n",
      "Doc 0      13.9144  14.9906  14.5110       14.8129  2.0640  14.9513\n",
      "Doc 1       1.2630   2.1980   2.8406        5.3029  1.9820   2.5517\n",
      "Doc 2      13.8312  15.3550  13.4659       15.8340  0.1303  13.2135\n",
      "Doc 3      -1.2159  -0.0944   0.2742        2.1902 -0.1183  -0.1139\n",
      "Doc 4      -0.7668  -0.9923  -0.2747        0.0036 -0.1946  -0.2381\n",
      "Doc 5      -2.5924  -0.9990  -0.2695        0.4441 -0.0644  -0.1724\n",
      "Doc 6       0.6996   0.9123   0.3246        1.1609 -0.2198   0.4397\n",
      "Doc 7      -1.6877  -0.4979   0.0000        2.4306 -0.0896   0.2404\n",
      "\n",
      "--- Evaluation Metrics vs Exact Shapley ---\n",
      "                 MAE      MSE  Pearson  Spearman\n",
      "Method                                          \n",
      "ContextCite   1.1477   1.7816   0.9906    0.8333\n",
      "WSS           0.6682   0.8417   0.9944    0.9286\n",
      "TMC           0.2324   0.0717   0.9990    0.9762\n",
      "BetaShap (U)  1.4480   3.1900   0.9834    0.9048\n",
      "LOO           3.4607  42.2669   0.5474    0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\nInstantiating ShapleyExperimentHarness (will pre-compute all utilities)...\")\n",
    "harness = ShapleyExperimentHarness(\n",
    "    items=documents,\n",
    "    query=\"What is the weather like in the capital of France?\",\n",
    "    # llm_model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", # Use a smaller model for faster demo if preferred\n",
    "    llm_model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", # Or your chosen model\n",
    "    verbose=True\n",
    ")\n",
    "print(f\"Harness target_response automatically generated: '{harness.target_response}'\")\n",
    "print(f\"Number of items (n): {harness.n_items}\")\n",
    "\n",
    "# 3. Compute Attributions using different methods from the harness\n",
    "results = {}\n",
    "seed = 42 # For reproducibility of stochastic methods\n",
    "\n",
    "print(\"\\n--- Computing Attributions using Harness (from pre-computed utilities) ---\")\n",
    "\n",
    "# Adjust num_samples for WSS/ContextCite if n is very small\n",
    "m_samples_for_approx = 64\n",
    "results[\"ContextCite\"] = harness.compute_contextcite_weights(num_samples=m_samples_for_approx, lasso_alpha=0.0, seed=seed) # LinReg\n",
    "results[\"WSS\"] = harness.compute_wss(num_samples=m_samples_for_approx, lasso_alpha=0.0, seed=seed) # LinReg\n",
    "\n",
    "T_iterations = harness.n_items * 20 # Adjust iterations as needed\n",
    "results[\"TMC\"] = harness.compute_tmc_shap(num_iterations=T_iterations, performance_tolerance=0.001, seed=seed)\n",
    "if beta_dist: \n",
    "    results[\"BetaShap (U)\"] = harness.compute_beta_shap(num_iterations=T_iterations, beta_a=0.5, beta_b=0.5, seed=seed)\n",
    "\n",
    "\n",
    "results[\"LOO\"] = harness.compute_loo()\n",
    "results[\"Exact\"] = harness.compute_exact_shap() # n=6 is 64 calls, feasible\n",
    "\n",
    "# 4. Display Results\n",
    "print(\"\\n\\n--- Attribution Scores (from Harness) ---\")\n",
    "# Create item labels\n",
    "item_labels = [f'Doc {i}' for i in range(harness.n_items)]\n",
    "\n",
    "# Filter out None results before creating DataFrame\n",
    "valid_results = {k:v for k, v in results.items() if v is not None and isinstance(v, np.ndarray) and len(v) == harness.n_items}\n",
    "\n",
    "if valid_results:\n",
    "    results_df = pd.DataFrame(valid_results, index=item_labels)\n",
    "    print(results_df.round(4))\n",
    "\n",
    "    if \"Exact\" in valid_results:\n",
    "        print(\"\\n--- Evaluation Metrics vs Exact Shapley ---\")\n",
    "        metrics_data = []\n",
    "        exact_scores = valid_results[\"Exact\"]\n",
    "        for method, approx_scores in valid_results.items():\n",
    "            if method != \"Exact\":\n",
    "\n",
    "                # Handle potential constant arrays for correlation\n",
    "                if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                    pearson_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                    spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                else:\n",
    "                    pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                    spearman_c, _ = spearmanr(exact_scores, approx_scores)\n",
    "                \n",
    "                metrics_data.append({\n",
    "                    \"Method\": method,\n",
    "                    \"Pearson\": pearson_c,\n",
    "                    \"Spearman\": spearman_c\n",
    "                })\n",
    "        \n",
    "        if metrics_data:\n",
    "            metrics_df = pd.DataFrame(metrics_data).set_index(\"Method\")\n",
    "            print(metrics_df.round(4))\n",
    "        else:\n",
    "            print(\"No approximate methods to compare against Exact.\")\n",
    "else:\n",
    "    print(\"No valid attribution results were computed by the harness.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_shap(query, n_rag=5):\n",
    "    # docs, scores=retrieve_documents(query, n_rag)\n",
    "    target_response=compute_logprob(query, model, tokenizer, context=documents, max_new_tokens=50, response=True)\n",
    "    harness = ShapleyExperimentHarness(\n",
    "                                        items=documents,\n",
    "                                        query=query,\n",
    "                                        target_response=target_response,\n",
    "                                        llm_caller=compute_logprob,\n",
    "                                        verbose=False,\n",
    "                                        utility_cache_file=None\n",
    "                                    )\n",
    "    query_metrics = []\n",
    "    exact_s = harness.compute_exact_shap()\n",
    "    methods_to_run = {\n",
    "        \"ContextCite\": lambda: harness.compute_contextcite_weights(num_samples=64, lasso_alpha=0.0),\n",
    "        \"WSS\": lambda: harness.compute_wss(num_samples=64, lasso_alpha=0.0),\n",
    "        \"TMC\": lambda: harness.compute_tmc_shap(num_iterations=10*n_rag, performance_tolerance=0.001),\n",
    "        \"LOO\": lambda: harness.compute_loo()\n",
    "    }\n",
    "    if beta_dist: # Only add if scipy was imported successfully\n",
    "        methods_to_run[\"BetaShap (U)\"] = lambda: harness.compute_beta_shap(num_iterations=10*n_rag, beta_a=0.5, beta_b=0.5)\n",
    "\n",
    "    for method_name, method_func in methods_to_run.items():\n",
    "        approx_scores = method_func()\n",
    "        \n",
    "        mae_val, pearson_val, spearman_val = np.nan, np.nan, np.nan # Default to NaN\n",
    "\n",
    "        if approx_scores is not None and len(approx_scores) == len(exact_s):\n",
    "            mae_val = np.mean(np.abs(exact_s - approx_scores))\n",
    "            if pearsonr and spearmanr: # Check if scipy functions were imported\n",
    "                # Handle constant arrays for correlation\n",
    "                if np.all(exact_s == exact_s[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                    is_close = np.allclose(exact_s, approx_scores)\n",
    "                    pearson_val = 1.0 if is_close else 0.0\n",
    "                    spearman_val = 1.0 if is_close else 0.0\n",
    "                else:\n",
    "                    pearson_val, _ = pearsonr(exact_s, approx_scores)\n",
    "\n",
    "\n",
    "        query_metrics.append({\n",
    "            \"method\": method_name,\n",
    "            \"mae\": mae_val,\n",
    "            \"pearson\": pearson_val,\n",
    "            \"spearman\": spearman_val\n",
    "        })\n",
    "    return query_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "model.eval()\n",
    "model = accelerator.prepare(model)\n",
    "evaluate_shap(queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Initializing Experiment Harness (Pre-computing all utilities) ---\")\n",
    "start_harness_time = time.time()\n",
    "harness = ShapleyExperimentHarness(\n",
    "    items=documents,\n",
    "    query=query,\n",
    "    target_response=target_response,\n",
    "    llm_caller=compute_logprob# Use placeholder for speed\n",
    ")\n",
    "print(f\"Harness initialized in {time.time() - start_harness_time:.2f}s. \"\n",
    "        f\"{len(harness.all_true_utilities)} utilities computed.\")\n",
    "\n",
    "results_exp = {}\n",
    "seed_exp = 42\n",
    "\n",
    "print(\"\\n--- Running Methods using Pre-computed Utilities ---\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"Exact\"] = harness.compute_exact_shap()\n",
    "print(f\"Exact Shapley (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"ContextCite\"] = harness.compute_contextcite_weights(num_samples=64, lasso_alpha=0.0, seed=seed_exp)\n",
    "print(f\"ContextCite (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"WSS\"] = harness.compute_wss(num_samples=64, lasso_alpha=0.0, seed=seed_exp)\n",
    "print(f\"WSS (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"TMC\"] = harness.compute_tmc_shap(num_iterations=harness.n_items * 15, performance_tolerance=0.001, seed=seed_exp)\n",
    "print(f\"TMC (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "if beta_dist:\n",
    "    start_method_time = time.time()\n",
    "    results_exp[\"BetaShap (U)\"] = harness.compute_beta_shap(num_iterations=harness.n_items * 15, beta_a=0.5, beta_b=0.5, seed=seed_exp)\n",
    "    print(f\"BetaShap (U, from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "start_method_time = time.time()\n",
    "results_exp[\"LOO\"] = harness.compute_loo()\n",
    "print(f\"LOO (from cache) took {time.time() - start_method_time:.4f}s\")\n",
    "\n",
    "# Display Results\n",
    "print(\"\\n\\n--- Experiment Harness: Comparison Table ---\")\n",
    "valid_results_exp = {k:v for k, v in results_exp.items() if v is not None}\n",
    "if valid_results_exp:\n",
    "    results_df_exp = pd.DataFrame(valid_results_exp, index=[f'Item {i}' for i in range(harness.n_items)])\n",
    "    print(results_df_exp.round(4))\n",
    "else:\n",
    "    print(\"No valid results were computed by the harness.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Exact\" in valid_results_exp:\n",
    "            print(\"\\n--- Experiment Harness: Metrics vs Exact Shapley ---\")\n",
    "            exact_scores = valid_results_exp[\"Exact\"]\n",
    "            for method, approx_scores in valid_results_exp.items():\n",
    "                if method != \"Exact\" and approx_scores is not None and len(approx_scores) == len(exact_scores):\n",
    "                    \n",
    "                    # Calculate Pearson and Spearman, handle potential constant arrays\n",
    "                    if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                        # If one or both are constant, correlation might be ill-defined or 0/1\n",
    "                        # pearsonr/spearmanr might return NaN or raise warning for constant input.\n",
    "                        pearson_corr_val = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                        spearman_corr_val = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "                    else:\n",
    "                        try:\n",
    "                            pearson_corr_val, _ = pearsonr(exact_scores, approx_scores)\n",
    "                            spearman_corr_val, _ = spearmanr(exact_scores, approx_scores)\n",
    "                        except ValueError: # e.g. if NaNs are present or other issues\n",
    "                            pearson_corr_val = np.nan\n",
    "                            spearman_corr_val = np.nan\n",
    "                            \n",
    "                    print(f\"{method}: Pearson={pearson_corr_val:.4f}, Spearman={spearman_corr_val:.4f}\")\n",
    "                elif method != \"Exact\":\n",
    "                    print(f\"{method}: Could not compute metrics (scores missing or length mismatch).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Experiment Harness: Metrics vs Exact Shapley ---\n",
    "ContextCite: Pearson=0.9978, Spearman=1.0000\n",
    "WSS: Pearson=0.9926, Spearman=1.0000\n",
    "TMC: Pearson=0.9995, Spearman=1.0000\n",
    "BetaShap (U): Pearson=0.9864, Spearman=0.9429\n",
    "LOO: Pearson=0.9678, Spearman=0.9429"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
