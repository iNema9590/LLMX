{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score, average_precision_score, precision_recall_curve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *\n",
    "from SHapRAG.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed color palette per method family/name\n",
    "METHOD_COLORS = {\n",
    "    \"FM\": \"#ff7f0e\",\n",
    "    \"Spex\": \"#d62728\",\n",
    "    \"Shapiq\":\"#2ca02c\",\n",
    "    \"ProxySpex\": \"#9467bd\",\n",
    "    \"FR_2\": \"#8c564b\",\n",
    "    \"FR_3\": \"#e377c2\",\n",
    "    \"FR_4\": \"#7f7f7f\",\n",
    "    \"FR_5\": \"#bcbd22\",\n",
    "    \"ContextCite\": \"#1f77b4\",\n",
    "    \"default\": \"#17becf\",\n",
    "}\n",
    "\n",
    "def family_of(method_key: str) -> str:\n",
    "    \"\"\"Map a notebook method key to a color family key.\"\"\"\n",
    "    if method_key.startswith(\"FM\"):\n",
    "        return \"FM-Shapley\"\n",
    "    if method_key.startswith(\"Spex\"):\n",
    "        return \"Spex\"\n",
    "    if method_key.startswith(\"Shapiq\"):\n",
    "        return \"Shapiq\"\n",
    "    if method_key.startswith(\"ProxySpex\"):\n",
    "        return \"ProxySpex\"\n",
    "    if method_key.startswith(\"ContextCite\"):\n",
    "        return \"ContextCite\"\n",
    "    if method_key in (\"FR_1\", \"FR_2\", \"FR_3\", \"FR_4\", \"FR_5\"):\n",
    "        return method_key\n",
    "    return \"default\"\n",
    "\n",
    "def get_color(method_key: str) -> str:\n",
    "    return METHOD_COLORS.get(family_of(method_key))\n",
    "\n",
    "\n",
    "# A colormap to pick new unique colors from\n",
    "# cmap = plt.cm.tab20\n",
    "# unknown_colors = {}\n",
    "# unknown_index = 0\n",
    "# def get_color(method):\n",
    "#     global unknown_index\n",
    "#     if method in METHOD_COLORS:\n",
    "#         return METHOD_COLORS[method]\n",
    "    \n",
    "#     if method not in unknown_colors:\n",
    "#         unknown_colors[method] = cmap(unknown_index % cmap.N)\n",
    "#         unknown_index += 1\n",
    "#     return unknown_colors[method]\n",
    "\n",
    "\n",
    "# Set matplotlib default cycle to the custom palette (keeps plotting consistent)\n",
    "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(color=list(METHOD_COLORS.values()))\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfin=pd.read_csv(\"../data/complementary.csv\")\n",
    "dfin.context=dfin.context.apply(ast.literal_eval)\n",
    "# dfin[\"paragraphs\"] = dfin[\"paragraphs\"].apply(lambda p: p[:5]+ [p[1]] + p[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GT():\n",
    "    return [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()\n",
    "utility_cache_base_dir = f\"../Experiment_data/complementary/{model_path.split('/')[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"answered.txt\", \"r\") as f:\n",
    "#     res = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions_to_run = len(dfin)\n",
    "K_VALUES = [1, 2, 3, 4, 5]\n",
    "all_results = []\n",
    "extras = []\n",
    "\n",
    "for i in range(num_questions_to_run):\n",
    "    query = dfin.question[i]\n",
    "    # if res[i]==\"True\":\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs = dfin.context[i]\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\"\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "\n",
    "    if accelerator_main.is_main_process:\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model=prepared_model,\n",
    "        prepared_tokenizer=tokenizer,\n",
    "        accelerator=accelerator_main,\n",
    "        utility_cache_path=current_utility_path,\n",
    "        utility_mode='log-perplexity'\n",
    "    )\n",
    "    \n",
    "    full_budget=pow(2,harness.n_items)\n",
    "    # res = evaluate(df.question[i], harness.target_response, df.answer[i])\n",
    "    # print(res)\n",
    "    if accelerator_main.is_main_process:\n",
    "        methods_results = {}\n",
    "        metrics_results = {}\n",
    "        extra_results = {}\n",
    "\n",
    "        m_samples_map = {\"XS\":32, \"S\":64, \"M\":128, \"L\":264, \"XL\":528, \"XXL\":724}\n",
    "\n",
    "        # Store FM models for later R²/MSE\n",
    "        fm_models = {}\n",
    "        methods_results['Exact-Shap']=harness._calculate_shapley()\n",
    "        harness.save_utility_cache(current_utility_path)\n",
    "        for size_key, actual_samples in m_samples_map.items():\n",
    "            print(f\"Running sample size: {actual_samples}\")\n",
    "            methods_results[f\"ContextCite_{actual_samples}\"], fm_models[f\"ContextCite_{actual_samples}\"] = harness.compute_contextcite(\n",
    "                num_samples=actual_samples, seed=SEED\n",
    "            )\n",
    "            # FM Weights (loop over ranks 0–5)\n",
    "            # for rank in [1, 2, 3, 4, 5, 8]:\n",
    "            #     methods_results[f\"FR_{rank}_{actual_samples}\"], extra_results[f\"FR_{rank}_{actual_samples}\"], fm_models[f\"FR_{rank}_{actual_samples}\"] = harness.compute_wss(\n",
    "            #         num_samples=actual_samples,\n",
    "            #         seed=SEED,\n",
    "            #         sampling=\"kernelshap\",\n",
    "            #         sur_type=\"fm\",\n",
    "            #         rank=rank\n",
    "            #     )\n",
    "            # methods_results[f\"FMW_{actual_samples}\"], extra_results[f\"FMW_{actual_samples}\"], fm_models[f\"FMW_{actual_samples}\"] = harness.compute_wss(\n",
    "            #         num_samples=actual_samples,\n",
    "            #         seed=SEED,\n",
    "            #         sampling=\"kernelshap\",\n",
    "            #         sur_type=\"fm\")\n",
    "            methods_results[f\"FM_{actual_samples}\"], extra_results[f\"FM_{actual_samples}\"], fm_models[f\"FM_{actual_samples}\"] = harness.compute_wss(\n",
    "                    num_samples=actual_samples,\n",
    "                    seed=SEED,\n",
    "                    sampling=\"kernelshap\",\n",
    "                    sur_type=\"fm_tuning\")\n",
    "                    \n",
    "            # FM models with dynamic k pruning\n",
    "            # methods_results[f\"FM_k_dynamic_{actual_samples}\"], extra_results[f\"FM_k_dynamic_{actual_samples}\"], fm_models[f\"FM_k_dynamic_{actual_samples}\"] = harness.compute_wss_dynamic_pruning_reuse_utility(\n",
    "            #     num_samples=actual_samples, \n",
    "            #     initial_rank=1, \n",
    "            #     final_rank=2,\n",
    "            # )\n",
    "            attributionshapiq, interactionshapiq, fm_models[f\"Shapiq_{actual_samples}\"] = harness.compute_shapiq_fsii(budget=actual_samples)\n",
    "            methods_results[f\"Shapiq_{actual_samples}\"] = attributionshapiq\n",
    "            extra_results.update({\n",
    "                f\"Shapiq_{actual_samples}\":interactionshapiq\n",
    "                                                                        })\n",
    "            try:\n",
    "                attributionshap, interactionshap, fm_models[f\"Spex_{actual_samples}\"] = harness.compute_fsii(sample_budget=actual_samples, max_order=harness.n_items)\n",
    "                # attributionban, interactionban, fm_models[f\"FBII_{actual_samples}\"] = harness.compute_fbii(sample_budget=actual_samples, max_order=harness.n_items)\n",
    "                # methods_results[f\"FBII_{actual_samples}\"] = attributionban\n",
    "                methods_results[f\"Spex_{actual_samples}\"] = attributionshap\n",
    "\n",
    "                extra_results.update({\n",
    "                f\"Spex_{actual_samples}\":interactionshap\n",
    "                                                                        })\n",
    "            except Exception: pass\n",
    "\n",
    "            try:\n",
    "                attributionban, interactionban, fm_models[f\"ProxySpex_{actual_samples}\"] = harness.compute_fbii(sample_budget=actual_samples, max_order=harness.n_items)\n",
    "                methods_results[f\"ProxySpex_{actual_samples}\"] = attributionban\n",
    "                extra_results.update({\n",
    "                    f\"ProxySpex_{actual_samples}\":interactionban\n",
    "                                                                        })\n",
    "            except Exception: \n",
    "                pass\n",
    "\n",
    "    #     methods_results[\"LOO\"] = harness.compute_loo()\n",
    "    #     methods_results[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "        attributionxs, interactionxs, fm_models[\"Exact-FSII\"] = harness.compute_exact_fsii(max_order=2)\n",
    "\n",
    "        extra_results.update({\n",
    "        \"Exact-FSII\": interactionxs\n",
    "    })\n",
    "        methods_results[\"Exact-FSII\"]=attributionxs\n",
    "\n",
    "        # --- Evaluation Metrics ---\n",
    "        metrics_results[\"topk_probability\"] = harness.evaluate_topk_performance(\n",
    "            methods_results, fm_models, K_VALUES\n",
    "        )\n",
    "\n",
    "        # R² and Delta R²\n",
    "        metrics_results[\"R2\"] = harness.r2(methods_results,50, models=fm_models)\n",
    "        metrics_results[\"Delta_R2\"] = harness.delta_r2(methods_results,50, models=fm_models)\n",
    "        metrics_results['Recall']=harness.recall_at_k(GT(), methods_results, K_VALUES)\n",
    "\n",
    "        # LDS per method\n",
    "        metrics_results[\"LDS\"] = harness.lds(methods_results,20, models=fm_models)\n",
    "\n",
    "\n",
    "\n",
    "        all_results.append({\n",
    "            \"query_index\": i,\n",
    "            \"query\": query,\n",
    "            \"ground_truth\": dfin.answer[i],\n",
    "            \"response\": harness.target_response,\n",
    "            \"methods\": methods_results,\n",
    "            \"metrics\": metrics_results\n",
    "        })\n",
    "        extras.append(extra_results)\n",
    "\n",
    "        # Save utility cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[1]['methods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{utility_cache_base_dir}/results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "with open(f\"{utility_cache_base_dir}/extras.pkl\", \"wb\") as f:\n",
    "    pickle.dump(extras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{utility_cache_base_dir}/results.pkl\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "with open(f\"{utility_cache_base_dir}/extras.pkl\", \"rb\") as f:\n",
    "    extras = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "spearmans = {i: [] for i in all_results[0]['methods'] if \"Exact\" not in i}\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for method_res in all_results:\n",
    "    for method, attribution in method_res['methods'].items():\n",
    "        if \"Exact\" not in method:\n",
    "            # Convert to numpy arrays for scaling\n",
    "            ref = np.array(method_res['methods'][\"Exact-Shap\"]).reshape(-1, 1)\n",
    "            att = np.array(attribution).reshape(-1, 1)\n",
    "            \n",
    "            # Scale both reference and attribution to [0, 1]\n",
    "            ref_scaled = scaler.fit_transform(ref).flatten()\n",
    "            att_scaled = scaler.fit_transform(att).flatten()\n",
    "            \n",
    "            # Compute NDCG score\n",
    "            spear = ndcg_score([ref_scaled], [att_scaled], k=5)\n",
    "            spearmans[method].append(spear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse methods and budgets\n",
    "parsed = {}\n",
    "budgets = set()\n",
    "for key, values in spearmans.items():\n",
    "    avg_val = np.mean(values)\n",
    "    \n",
    "    match = re.match(r\"(.+?)_(\\d+)$\", key)  # method_budget pattern\n",
    "    if match:\n",
    "        method, budget = match.groups()\n",
    "        budget = int(budget)\n",
    "        budgets.add(budget)\n",
    "        parsed.setdefault(method, {})[budget] = avg_val\n",
    "    else:\n",
    "        # constant methods (no budget)\n",
    "        parsed.setdefault(key, {})[None] = avg_val\n",
    "\n",
    "budgets = sorted(budgets)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for method, results in parsed.items():\n",
    "    if None in results:  # constant method\n",
    "        plt.hlines(results[None], xmin=min(budgets), xmax=max(budgets), \n",
    "                   linestyles='--', label=method)\n",
    "    else:\n",
    "        xs = sorted(results.keys())\n",
    "        ys = [results[b] for b in xs]\n",
    "\n",
    "        if \"FR\" not in method:\n",
    "            plt.plot(xs, ys, marker='o', label=method, color=get_color(method))\n",
    "        else:\n",
    "            plt.scatter(xs, ys, marker='x', label=method, color=get_color(method))\n",
    "plt.xlabel(\"Budget\")\n",
    "plt.ylabel(\"NDCG\")\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(f\"../Figures/{model_path.split('/')[1].split('-')[0]}/ndcg_vs_budget_marginal.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmans_per_k = {k: {i: [] for i in all_results[0]['methods'] if \"Exact\" not in i}\n",
    "                   for k in K_VALUES}\n",
    "\n",
    "# Compute NDCG@k for each k, for each method, across experiments\n",
    "for method_res in all_results:\n",
    "    for method, attribution in method_res['methods'].items():\n",
    "        if \"Exact\" in method:\n",
    "            continue\n",
    "        \n",
    "        # Convert to numpy arrays for scaling\n",
    "        ref = np.array(method_res['methods'][\"Exact-Shap\"]).reshape(-1, 1)\n",
    "        att = np.array(attribution).reshape(-1, 1)\n",
    "\n",
    "        # Scale both reference and attribution to [0, 1]\n",
    "        ref_scaled = scaler.fit_transform(ref).flatten()\n",
    "        att_scaled = scaler.fit_transform(att).flatten()\n",
    "\n",
    "        # Compute NDCG@k for all K_VALUES\n",
    "        for k in K_VALUES:\n",
    "            spear = ndcg_score([ref_scaled], [att_scaled], k=k)\n",
    "            spearmans_per_k[k][method].append(spear)\n",
    "\n",
    "# Average over experiments: avg_spearmans_per_k[k][method] = mean NDCG@k\n",
    "avg_spearmans_per_k = {\n",
    "    k: {\n",
    "        m: float(np.mean(vals)) for m, vals in methods_scores.items() if len(vals) > 0\n",
    "    }\n",
    "    for k, methods_scores in spearmans_per_k.items()\n",
    "}\n",
    "\n",
    "parsed_per_k = {}  # parsed_per_k[k][base_method][budget_or_None] = avg_val\n",
    "all_budgets = set()\n",
    "\n",
    "for k, method_dict in avg_spearmans_per_k.items():\n",
    "    parsed_per_k[k] = {}\n",
    "    for key, avg_val in method_dict.items():\n",
    "        match = re.match(r\"(.+?)_(\\d+)$\", key)  # method_budget pattern\n",
    "        if match:\n",
    "            method, budget = match.groups()\n",
    "            budget = int(budget)\n",
    "            all_budgets.add(budget)\n",
    "            parsed_per_k[k].setdefault(method, {})[budget] = avg_val\n",
    "        else:\n",
    "            # constant methods (no budget)\n",
    "            parsed_per_k[k].setdefault(key, {})[None] = avg_val\n",
    "\n",
    "all_budgets = sorted(all_budgets)\n",
    "\n",
    "# ---- Build data for fixed budget: metric vs k ----\n",
    "\n",
    "# fixed_budget_results[method][k] = value at that budget\n",
    "fixed_budget_results = {}\n",
    "FIXED_BUDGET=264\n",
    "for k in K_VALUES:\n",
    "    for method, results in parsed_per_k[k].items():\n",
    "        # budgeted methods\n",
    "        if FIXED_BUDGET in results:\n",
    "            fixed_budget_results.setdefault(method, {})[k] = results[FIXED_BUDGET]\n",
    "        # constant methods (no budget) -> same value for all k, if you want them\n",
    "        elif None in results:\n",
    "            fixed_budget_results.setdefault(method, {})[k] = results[None]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for method, k_dict in fixed_budget_results.items():\n",
    "    ks_sorted = sorted(k_dict.keys())\n",
    "    ys = [k_dict[kk] for kk in ks_sorted]\n",
    "    label = f\"{method} (budget={FIXED_BUDGET})\" if any(b.isdigit() for b in method) is False else method\n",
    "    # You can simplify label if you want; here we keep \"method\" plus budget in the title\n",
    "    plt.plot(ks_sorted, ys, marker='o', label=method, color=get_color(method))\n",
    "\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"NDCG\")\n",
    "plt.grid(True)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_for_query(scores, true_indices):\n",
    "    true_set = set(true_indices)\n",
    "    if len(true_set) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Rank items by score (descending)\n",
    "    ranked_indices = np.argsort(-scores)\n",
    "\n",
    "    num_relevant = len(true_set)\n",
    "    num_hits = 0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for rank, idx in enumerate(ranked_indices, start=1):\n",
    "        if idx in true_set:\n",
    "            num_hits += 1\n",
    "        precision = num_hits / rank\n",
    "        recall = num_hits / num_relevant\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    recalls = np.array(recalls)\n",
    "    precisions = np.array(precisions)\n",
    "\n",
    "    # Average Precision: area under PR via step approximation\n",
    "    order = np.argsort(recalls)\n",
    "    recalls = recalls[order]\n",
    "    precisions = precisions[order]\n",
    "\n",
    "    recalls_ext = np.concatenate(([0.0], recalls))\n",
    "    precisions_ext = np.concatenate(([precisions[0]], precisions))\n",
    "\n",
    "    ap = 0.0\n",
    "    for i in range(1, len(recalls_ext)):\n",
    "        delta_r = recalls_ext[i] - recalls_ext[i - 1]\n",
    "        ap += delta_r * precisions_ext[i]\n",
    "\n",
    "    return ap\n",
    "\n",
    "\n",
    "def mean_pr_auc(all_scores):\n",
    "\n",
    "    aps = []\n",
    "    for i, scores in enumerate(all_scores):\n",
    "        ap = average_precision_for_query(scores, GT(i))\n",
    "        aps.append(ap)\n",
    "    return float(np.mean(aps)) if aps else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_from_scores(all_scores, gtset_k):\n",
    "    n = all_scores.shape[0]\n",
    "\n",
    "    # Build binary relevance vector y_true\n",
    "    y_true = np.zeros(n, dtype=int)\n",
    "    y_true[list(gtset_k)] = 1\n",
    "\n",
    "    # y_scores is just the scores\n",
    "    y_scores = all_scores\n",
    "\n",
    "    if y_true.sum() == 0:\n",
    "        # no relevant docs; convention: skip or count AP=0\n",
    "        ap = 0.0\n",
    "    else:\n",
    "        ap = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps={i:[] for i in all_results[0][\"methods\"].keys()}\n",
    "for i, rw in enumerate(all_results):\n",
    "    for method in aps:\n",
    "        aps[method].append(map_from_scores(rw[\"methods\"][method], GT()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_methods = ['Exact-Shap', 'Exact-FSII']  # adjust if needed\n",
    "\n",
    "# Containers\n",
    "budgeted_data = {}   # family -> list of (budget, mean_value)\n",
    "constant_data = {}   # method -> mean_value\n",
    "\n",
    "for method, values in aps.items():\n",
    "    mean_val = float(np.mean(values))\n",
    "\n",
    "    if method in constant_methods:\n",
    "        constant_data[method] = mean_val\n",
    "    else:\n",
    "        # split family and budget: e.g. \"Shapiq_528\" -> (\"Shapiq\", 528)\n",
    "        parts = method.split(\"_\")\n",
    "        family = \"_\".join(parts[:-1])\n",
    "        budget = int(parts[-1])\n",
    "\n",
    "        if family not in budgeted_data:\n",
    "            budgeted_data[family] = []\n",
    "        budgeted_data[family].append((budget, mean_val))\n",
    "\n",
    "# sort by budget\n",
    "for family in budgeted_data:\n",
    "    budgeted_data[family] = sorted(budgeted_data[family], key=lambda x: x[0])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot budgeted families\n",
    "for family, items in budgeted_data.items():\n",
    "    budgets = [b for b, _ in items]\n",
    "    means = [m for _, m in items]\n",
    "    if \"FR\" not in family:\n",
    "        plt.plot(budgets, means, marker='o', label=family, color=get_color(family))\n",
    "    else:   \n",
    "        plt.scatter(budgets, means, marker='X', label=family, color=get_color(family))\n",
    "\n",
    "# Plot constant methods as horizontal lines\n",
    "# for method, mean_val in constant_data.items():\n",
    "#     plt.axhline(y=mean_val, linestyle='--', label=method)\n",
    "\n",
    "plt.xlabel(\"Budget\")\n",
    "plt.ylabel(\"PR-AUC\")\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"../Figures/{model_path.split('/')[1].split('-')[0]}/prauc_vs_budget_marginal.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def summarize_and_print(all_results, k_values=[1, 2, 3,4,5]):\n",
    "    table_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Mapping for consistency\n",
    "    method_name_map = {\n",
    "        \n",
    "    }\n",
    "\n",
    "    for res in all_results:\n",
    "        metrics = res[\"metrics\"]\n",
    "        # LDS and R2\n",
    "        for method_name, lds_val in metrics.get(\"LDS\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"LDS\"].append(lds_val)\n",
    "\n",
    "        for method_name, r2_val in metrics.get(\"R2\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"R2\"].append(r2_val)\n",
    "\n",
    "        # Delta R2 (new)\n",
    "        for method_name, delta_val in metrics.get(\"Delta_R2\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            table_data[method][\"Delta_R2\"].append(delta_val)\n",
    "\n",
    "        # Top-k\n",
    "        for method_name, k_dict in metrics.get(\"topk_probability\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            for k in k_values:\n",
    "                if k in k_dict:\n",
    "                    col_name = f\"topk_probability_k{k}\"\n",
    "                    table_data[method][col_name].append(k_dict[k])\n",
    "        \n",
    "        for method_name, k_dict in metrics.get(\"Recall\", {}).items():\n",
    "            method = method_name_map.get(method_name, method_name)\n",
    "            for k in k_values:\n",
    "                col_name = f\"Recall@{k}\"\n",
    "                table_data[method][col_name].append(k_dict[k-1])\n",
    "\n",
    "    # Averages\n",
    "    avg_table = {\n",
    "        method: {metric: np.nanmean(values) for metric, values in metric_dict.items()}\n",
    "        for method, metric_dict in table_data.items()\n",
    "    }\n",
    "\n",
    "    # Standard deviations for LDS, R², and Delta_R2\n",
    "    for method, metric_dict in table_data.items():\n",
    "        for metric in [\"LDS\", \"R2\", \"Delta_R2\"]:\n",
    "            if metric in metric_dict:\n",
    "                avg_table[method][f\"{metric}_std\"] = np.nanstd(metric_dict[metric])\n",
    "\n",
    "    df_summary = pd.DataFrame.from_dict(avg_table, orient=\"index\").sort_index()\n",
    "\n",
    "    print(\"\\n=== Metrics Summary Across All Queries ===\")\n",
    "    print(df_summary.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "    return df_summary\n",
    "df_res=summarize_and_print(all_results, k_values=[1, 2, 3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df_reset = df_res.reset_index().rename(columns={'index': 'method'})\n",
    "\n",
    "# Separate constant methods (no budget) and budgeted methods\n",
    "constant_methods = ['LOO', 'ARC-JSD', 'Exact-FSII', 'Exact-Shap']\n",
    "df_const = df_reset[df_reset['method'].isin(constant_methods)]\n",
    "df_budgeted = df_reset[~df_reset['method'].isin(constant_methods)]\n",
    "\n",
    "# Extract family and budget for budgeted methods\n",
    "df_budgeted['family'] = df_budgeted['method'].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "df_budgeted['budget'] = df_budgeted['method'].apply(lambda x: int(x.split(\"_\")[-1]))\n",
    "df_budgeted = df_budgeted.sort_values(by=['family', 'budget'])\n",
    "\n",
    "# Function to plot metric\n",
    "def plot_metric(metric, ylabel):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot budgeted families\n",
    "    families = df_budgeted['family'].unique()\n",
    "    for fam in families:\n",
    "        subset = df_budgeted[df_budgeted['family'] == fam]\n",
    "        if \"FR\" in fam:\n",
    "            plt.scatter(subset['budget'], subset[metric], marker='x', color=get_color(fam), label=fam)  # Dummy for legend\n",
    "        else:\n",
    "            plt.plot(subset['budget'], subset[metric], marker='o', label=fam, color=get_color(fam))\n",
    "\n",
    "    # Plot constant methods as horizontal lines\n",
    "    colors = plt.cm.tab10.colors  # categorical palette\n",
    "    # for idx, (_, row) in enumerate(df_const.iterrows()):\n",
    "    #     plt.axhline(y=row[metric], color=colors[idx % len(colors)],marker='x', label=row['method'])\n",
    "\n",
    "    plt.xlabel(\"Budget\")\n",
    "    plt.ylabel(ylabel)\n",
    "    # plt.title(f\"Evolution of {ylabel} with Increasing Budget\")\n",
    "    # plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"../Figures/{model_path.split('/')[1].split('-')[0]}/{metric.lower()}_vs_budget_marginal.pdf\", bbox_inches='tight')  \n",
    "    plt.show()\n",
    "\n",
    "plot_metric(\"R2\", \"R2\")\n",
    "plot_metric(\"LDS\", \"LDS\")\n",
    "plot_metric(\"Delta_R2\", \"Delta R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter budgeted methods at budget = 274\n",
    "df_budgeted_264 = df_budgeted[df_budgeted['budget'] == 264]\n",
    "\n",
    "# Metrics to plot\n",
    "recall_metrics = [f\"Recall@{k}\" for k in range(1, 6)]\n",
    "k_values = list(range(1, 6))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot budgeted families at budget 274\n",
    "families = df_budgeted_264['family'].unique()\n",
    "for fam in families:\n",
    "    # if 'LK' not in fam:\n",
    "    subset = df_budgeted_264[df_budgeted_264['family'] == fam]\n",
    "    if not subset.empty:\n",
    "        recalls = subset[recall_metrics].values.flatten()\n",
    "        plt.plot(k_values, recalls, marker='o', label=fam, color=get_color(fam))\n",
    "\n",
    "# Plot constant methods\n",
    "# for idx, (_, row) in enumerate(df_const.iterrows()):\n",
    "#     recalls = [row[m] for m in recall_metrics]\n",
    "#     plt.plot(k_values, recalls, marker='x', linestyle=\"--\", label=row['method'])\n",
    "\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall@k\")\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"../Figures/{model_path.split('/')[1].split('-')[0]}/recall_at_k.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for method in df_res.index:\n",
    "    if \"264\" in method :\n",
    "        plt.plot(\n",
    "            [1, 2, 3,4,5],\n",
    "            df_res.loc[method, ['topk_probability_k1', 'topk_probability_k2', 'topk_probability_k3', 'topk_probability_k4', 'topk_probability_k5']],\n",
    "            marker='o',\n",
    "            label=method.split(\"_\")[0]\n",
    "        )\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Top-k Removal Drop')\n",
    "# plt.title('Top-k log-perplexityability Drop')\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"../Figures/{model_path.split('/')[1].split('-')[0]}/topk_removal.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recall@k wrt ground truth (positive and negative interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(similarity_dict, ground_truth, k):\n",
    "    \"\"\"\n",
    "    sorted_pairs_desc: [((i, j), value), ...] sorted by value desc\n",
    "    ground_truth: list of [i, j] or (i, j), all with i < j\n",
    "    k: int\n",
    "    \"\"\"\n",
    "    sorted_pairs_desc = sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    gt_set = {tuple(pair) for pair in ground_truth}\n",
    "    top_k_pairs = [pair for (pair, _) in sorted_pairs_desc[:k]]\n",
    "    top_k_set = set(top_k_pairs)\n",
    "    \n",
    "    hits = sum(1 for gt in gt_set if gt in top_k_set)\n",
    "    return hits / len(gt_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "k=5\n",
    "recall_at_k_average={i:[] for i in extras[1].keys()}\n",
    "for en, A in enumerate(extras):\n",
    "    pos_int_gt=[list(pair) for pair in combinations(GT(en), 2)]\n",
    "    for name, inter_value in A.items():\n",
    "        if \"FM\" in name:\n",
    "            similarity_dict = {\n",
    "            (i, j): inter_value[i][j]\n",
    "            for i in range(len(inter_value))\n",
    "            for j in range(i + 1, len(inter_value))\n",
    "            }\n",
    "            recall_at_k_average[name].append(recall_at_k(similarity_dict, pos_int_gt, k))\n",
    "        else:\n",
    "            recall_at_k_average[name].append(recall_at_k(inter_value, pos_int_gt, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{n:sum(l)/len(l)} for n, l in recall_at_k_average.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RR@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse RR@k for all experiments and budgets\n",
    "def compute_rr_at_k(interaction, ground_truth, k):\n",
    "    \"\"\"\n",
    "    Compute Recovery@k for a method's interaction dict or matrix.\n",
    "    interaction: dict {(i, j): value} or 2D numpy array/matrix\n",
    "    ground_truth: set of ground-truth indices (R^*)\n",
    "    k: number of top interactions to consider\n",
    "    Returns: RR@k value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert matrix to dict if needed\n",
    "    if isinstance(interaction, (np.ndarray, list)):\n",
    "        mat = np.array(interaction)\n",
    "        pairs = {(i, j): mat[i][j] for i in range(mat.shape[0]) for j in range(mat.shape[1]) if i != j}\n",
    "    else:\n",
    "        pairs = interaction\n",
    "\n",
    "    # Sort pairs by value (descending)\n",
    "    sorted_pairs = sorted(pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    rr_sum = 0.0\n",
    "    for i in range(min(k, len(sorted_pairs))):\n",
    "        pair_indices = set(sorted_pairs[i][0])\n",
    "        rr_sum += len(ground_truth & pair_indices) / len(pair_indices)\n",
    "    return rr_sum / k if k > 0 else 0.0\n",
    "\n",
    "def extract_budget(key):\n",
    "    match = re.search(r'_(\\d+)$', key)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_family(key):\n",
    "    if key.startswith(\"FR\"):\n",
    "        return key\n",
    "    elif key.startswith(\"FM\"):\n",
    "        return \"FM\"\n",
    "    elif key.startswith(\"Spex\"):\n",
    "        return \"Spex\"\n",
    "    elif key.startswith(\"Proxy\"):\n",
    "        return \"ProxySpex\"\n",
    "    elif key.startswith(\"Shapiq\"):\n",
    "        return \"Shapiq\"\n",
    "    return None\n",
    "\n",
    "def collect_rr_at_k_over_budgets(extras, k):\n",
    "    # For each experiment, for each method, collect RR@k by budget\n",
    "    rr_by_method_budget = defaultdict(lambda: defaultdict(list))\n",
    "    for i, exp in enumerate(extras):\n",
    "        for method, interaction in exp.items():\n",
    "            budget = extract_budget(method)\n",
    "            family = extract_family(method)\n",
    "            if budget and family:\n",
    "                rr = compute_rr_at_k(interaction, set(GT()), k)\n",
    "                rr_by_method_budget[family][budget].append(rr)\n",
    "    # Average over experiments\n",
    "    rr_avg = defaultdict(dict)\n",
    "    for family, budgets in rr_by_method_budget.items():\n",
    "        for budget, vals in budgets.items():\n",
    "            rr_avg[family][budget] = np.mean(vals)\n",
    "    return rr_avg\n",
    "rr_avg = collect_rr_at_k_over_budgets(extras, 5)\n",
    "# Plot RR@k as line chart for each method family, with constant methods as parallel lines\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot budgeted families\n",
    "for family, budget_rrs in rr_avg.items():\n",
    "    budgets = sorted(budget_rrs.keys())\n",
    "    values = [budget_rrs[b] for b in budgets]\n",
    "    if \"FR\" in family:\n",
    "        plt.scatter(budgets, values, marker='x', label=family, color=get_color(family))\n",
    "    else:\n",
    "        plt.plot(budgets, values, marker='o', label=family, color=get_color(family))\n",
    "\n",
    "# Plot constant methods (e.g., Exact-FSII, LOO, ARC-JSD) as horizontal lines\n",
    "# constant_methods = ['Exact-FSII', 'LOO', 'ARC-JSD']\n",
    "# for method in constant_methods:\n",
    "#     # Collect RR@k for each experiment and average\n",
    "#     rr_vals = []\n",
    "#     for exp in extras:\n",
    "#         if method in exp:\n",
    "#             rr_vals.append(compute_rr_at_k(exp[method], ground_truth, k))\n",
    "#     if rr_vals:\n",
    "#         avg_rr = np.mean(rr_vals)\n",
    "#         plt.axhline(y=avg_rr, color=None, linestyle='--', label=method)\n",
    "\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel(f'RR@{k}')\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"../Figures/{model_path.split('/')[1].split('-')[0]}/rr@5_rank.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Create some fake \"methods\" with different styles\n",
    "handles = [\n",
    "    mlines.Line2D([], [], linestyle='-',  marker='o',  label='FM'),\n",
    "    mlines.Line2D([], [], linestyle='--', marker='s',  label='Spex'),\n",
    "    mlines.Line2D([], [], linestyle='-.', marker='^',  label='Shapiq'),\n",
    "]\n",
    "\n",
    "# Standalone legend figure\n",
    "fig, ax = plt.subplots(figsize=(4, 1.5))  # adjust size as needed\n",
    "ax.axis('off')  # no axes, just the legend\n",
    "\n",
    "ax.legend(\n",
    "    handles=handles,\n",
    "    loc='center',\n",
    "    ncol=3,        # put legend entries in 2 columns\n",
    "    frameon=False  # no box around legend\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"legend_methods_only.pdf\", bbox_inches='tight')\n",
    "# fig.savefig(\"legend_methods_only.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_by_family_budget_k = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for i, exp in enumerate(extras):\n",
    "    gt = set(GT())  # assuming GT(i) is defined as in your original code\n",
    "    for method, interaction in exp.items():\n",
    "        budget = extract_budget(method)\n",
    "        family = extract_family(method)\n",
    "        if budget is not None and family is not None:\n",
    "            for k in K_VALUES:\n",
    "                rr = compute_rr_at_k(interaction, gt, k)\n",
    "                rr_by_family_budget_k[family][budget][k].append(rr)\n",
    "\n",
    "# Average over experiments: rr_avg[family][budget][k] = mean RR@k\n",
    "rr_avg = defaultdict(lambda: defaultdict(dict))\n",
    "for family, budgets in rr_by_family_budget_k.items():\n",
    "    for budget, k_vals in budgets.items():\n",
    "        for k, vals in k_vals.items():\n",
    "            if vals:\n",
    "                rr_avg[family][budget][k] = float(np.mean(vals))\n",
    "\n",
    "# For plotting: at FIXED_BUDGET, collect RR@k for each family\n",
    "family_rr_at_k = defaultdict(dict)   # family_rr_at_k[family][k] = RR@k\n",
    "\n",
    "for family, budgets in rr_avg.items():\n",
    "    if FIXED_BUDGET in budgets:\n",
    "        for k in K_VALUES:\n",
    "            if k in budgets[FIXED_BUDGET]:\n",
    "                family_rr_at_k[family][k] = budgets[FIXED_BUDGET][k]\n",
    "\n",
    "# ---------- Plot: RR@k vs k at fixed budget ----------\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for family, k_dict in family_rr_at_k.items():\n",
    "    ks_sorted = sorted(k_dict.keys())\n",
    "    values = [k_dict[kk] for kk in ks_sorted]\n",
    "    if \"FR\" in family:\n",
    "        plt.scatter(ks_sorted, values, marker='x', label=f'{family} (budget={FIXED_BUDGET})', color=get_color(family))\n",
    "    else:\n",
    "        plt.plot(ks_sorted, values, marker='o', label=f'{family} (budget={FIXED_BUDGET})', color=get_color(family))\n",
    "\n",
    "plt.xlabel('k (RR@k)')\n",
    "plt.ylabel('RR')\n",
    "plt.grid(True)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_rr_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NDCG to Exact-FSII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_budget(key):\n",
    "    m = re.search(r'_(\\d+)$', key)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def extract_family(key):\n",
    "    # Map method keys to families used in earlier plots\n",
    "    if key.startswith(\"FR\"):\n",
    "        return key.split(\"_\")[0] + \"_\" + key.split(\"_\")[1]\n",
    "    elif key.startswith(\"FM\"):\n",
    "        return \"FM\"\n",
    "    elif key.startswith(\"ProxySpex\"):\n",
    "        # collapse to Flu/Flk family prefix (keep as-is for plotting)\n",
    "        return 'ProxySpex'\n",
    "    elif key.startswith(\"Spex\"):\n",
    "        return \"Spex\"\n",
    "    elif key.startswith(\"Shapiq\"):\n",
    "        return \"Shapiq\"\n",
    "    return None\n",
    "\n",
    "def pairs_from_exact(exp_list):\n",
    "    # Find a canonical pair ordering from Exact-FSII of the first experiment that has it\n",
    "    for exp in exp_list:\n",
    "        exact = exp.get('Exact-FSII')\n",
    "        if exact and isinstance(exact, dict):\n",
    "            return sorted(exact.keys())\n",
    "    # fallback: try to infer from any dict-valued method\n",
    "    for exp in exp_list:\n",
    "        for v in exp.values():\n",
    "            if isinstance(v, dict) and v:\n",
    "                return sorted(v.keys())\n",
    "    return []\n",
    "\n",
    "def vector_for_pairs(val, pairs):\n",
    "    # val can be dict {(i,j):score} or a square matrix/list/ndarray\n",
    "    if isinstance(val, (list, np.ndarray)):\n",
    "        mat = np.array(val)\n",
    "        return [abs(mat[i][j]) if (0 <= i < mat.shape[0] and 0 <= j < mat.shape[1]) else 0.0 for (i,j) in pairs]\n",
    "    elif isinstance(val, dict):\n",
    "        return [abs(val.get(pair, 0.0)) for pair in pairs]\n",
    "    else:\n",
    "        # Unknown type -> zeros\n",
    "        return [0.0 for _ in pairs]\n",
    "\n",
    "# Build canonical pair list\n",
    "pairs = pairs_from_exact(extras)\n",
    "if not pairs:\n",
    "    print('No pair ordering could be inferred from Exact-FSII or other dicts in extras. Aborting NDCG computation.')\n",
    "else:\n",
    "    # Compute per-experiment NDCG scores for each method (relative to Exact-FSII)\n",
    "    per_method_ndcg = defaultdict(list)\n",
    "    for exp in extras:\n",
    "        exact = exp.get('Exact-FSII', {})\n",
    "        exact_vec = vector_for_pairs(exact, pairs)\n",
    "        # if exact vector is all zeros, skip this experiment for fairness\n",
    "        if np.allclose(exact_vec, 0.0):\n",
    "            continue\n",
    "        for method, val in exp.items():\n",
    "            if method == 'Exact-FSII':\n",
    "                continue\n",
    "            try:\n",
    "                vec = vector_for_pairs(val, pairs)\n",
    "                # ndcg_score expects shape (n_samples, n_labels) for both y_true and y_score\n",
    "                score = ndcg_score([exact_vec], [vec], k=3)\n",
    "                per_method_ndcg[method].append(score)\n",
    "            except Exception:\n",
    "                # skip methods we cannot convert\n",
    "                continue\n",
    "\n",
    "    # Average NDCG across experiments for each method\n",
    "    avg_ndcg = {m: float(np.mean(scores)) for m, scores in per_method_ndcg.items() if len(scores)>0}\n",
    "\n",
    "    # Group budgeted methods by family and budget\n",
    "    family_budget = defaultdict(lambda: defaultdict(list))\n",
    "    for method, score in avg_ndcg.items():\n",
    "        budget = extract_budget(method)\n",
    "        family = extract_family(method)\n",
    "        if budget is not None and family is not None:\n",
    "            family_budget[family][budget].append(score)\n",
    "\n",
    "    # Compute mean per family-budget (in case multiple variant keys map to same family-budget)\n",
    "    family_budget_avg = {}\n",
    "    for fam, bd in family_budget.items():\n",
    "        family_budget_avg[fam] = {b: float(np.mean(vals)) for b, vals in bd.items()}\n",
    "\n",
    "    # Plotting: line per family (budgeted), horizontal lines for constant methods\n",
    "    plt.figure(figsize=(8,6))\n",
    "    # Plot budgeted families\n",
    "    for fam, bd in family_budget_avg.items():\n",
    "        xs = sorted(bd.keys())\n",
    "        ys = [bd[x] for x in xs]\n",
    "        if \"FR\" in fam:\n",
    "            plt.scatter(xs, ys, marker='x', label=fam, color=get_color(fam))\n",
    "        else:\n",
    "            plt.plot(xs, ys, marker='o', label=fam, color=get_color(fam))\n",
    "\n",
    "    # Constant methods: plot as horizontal lines using avg_ndcg if available\n",
    "    constant_methods = ['Exact-FSII','Exact-Shap','LOO','ARC-JSD']\n",
    "    for cm in constant_methods:\n",
    "        if cm in avg_ndcg:\n",
    "            plt.axhline(y=avg_ndcg[cm], linestyle='--', label=cm)\n",
    "\n",
    "    plt.xlabel('Budget')\n",
    "    plt.ylabel('NDCG')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"../Figures/{model_path.split('/')[1].split('-')[0]}/ndcg@5_interactions.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_budget_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build canonical pair list\n",
    "pairs = pairs_from_exact(extras)\n",
    "if not pairs:\n",
    "    print('No pair ordering could be inferred from Exact-FSII or other dicts in extras. Aborting NDCG computation.')\n",
    "else:\n",
    "    # per_method_ndcg[k][method] = [scores across experiments]\n",
    "    per_method_ndcg = {k: defaultdict(list) for k in K_VALUES}\n",
    "\n",
    "    for exp in extras:\n",
    "        exact = exp.get('Exact-FSII', {})\n",
    "        exact_vec = vector_for_pairs(exact, pairs)\n",
    "        if np.allclose(exact_vec, 0.0):\n",
    "            continue\n",
    "\n",
    "        for method, val in exp.items():\n",
    "            if method == 'Exact-FSII':\n",
    "                continue\n",
    "            try:\n",
    "                vec = vector_for_pairs(val, pairs)\n",
    "                for k in K_VALUES:\n",
    "                    score = ndcg_score([exact_vec], [vec], k=k)\n",
    "                    per_method_ndcg[k][method].append(score)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # Average NDCG across experiments for each k and each method\n",
    "    avg_ndcg_per_k = {\n",
    "        k: {m: float(np.mean(scores)) for m, scores in md.items() if len(scores) > 0}\n",
    "        for k, md in per_method_ndcg.items()\n",
    "    }\n",
    "\n",
    "    # For budgeted methods: group by family at the FIXED_BUDGET\n",
    "    # family_ndcg_at_k[family][k] = mean NDCG for that family at that k and budget\n",
    "    family_ndcg_at_k = defaultdict(dict)\n",
    "\n",
    "    for k, avg_ndcg in avg_ndcg_per_k.items():\n",
    "        # group budgeted methods by family at the chosen budget\n",
    "        family_scores = defaultdict(list)\n",
    "        for method, score in avg_ndcg.items():\n",
    "            budget = extract_budget(method)\n",
    "            family = extract_family(method)\n",
    "            if budget == FIXED_BUDGET and family is not None:\n",
    "                family_scores[family].append(score)\n",
    "\n",
    "        # average over possible variants within family\n",
    "        for fam, scores in family_scores.items():\n",
    "            if scores:\n",
    "                family_ndcg_at_k[fam][k] = float(np.mean(scores))\n",
    "\n",
    "    # Constant methods (no budget) as a function of k\n",
    "    constant_methods = ['Exact-FSII', 'Exact-Shap', 'LOO', 'ARC-JSD']\n",
    "    constant_ndcg_per_k = {cm: {} for cm in constant_methods}\n",
    "    for k, avg_ndcg in avg_ndcg_per_k.items():\n",
    "        for cm in constant_methods:\n",
    "            if cm in avg_ndcg:\n",
    "                constant_ndcg_per_k[cm][k] = avg_ndcg[cm]\n",
    "\n",
    "    # ---- Plot: NDCG vs k at fixed budget ----\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot budgeted families at the fixed budget\n",
    "    for fam, kd in family_ndcg_at_k.items():\n",
    "        ks_sorted = sorted(kd.keys())\n",
    "        ys = [kd[kk] for kk in ks_sorted]\n",
    "        plt.plot(ks_sorted, ys, marker='o', label=f'{fam} (budget={FIXED_BUDGET})', color=get_color(fam))\n",
    "\n",
    "    # Plot constant methods as lines over k\n",
    "    for cm, kd in constant_ndcg_per_k.items():\n",
    "        if kd:\n",
    "            ks_sorted = sorted(kd.keys())\n",
    "            ys = [kd[kk] for kk in ks_sorted]\n",
    "            plt.plot(ks_sorted, ys, linestyle='--', marker='x', label=cm)\n",
    "\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('NDCG')\n",
    "    # plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "legend_methods = [\n",
    "    \"FM-Shapley\", \"Spex\", \"Shapiq\", \"ContextCite\"\n",
    "]\n",
    "\n",
    "# Create Line2D handles with fixed color and linestyle\n",
    "handles = [\n",
    "    mlines.Line2D(\n",
    "        [], [],\n",
    "        color=get_color(m),  # use your METHOD_COLORS mapping\n",
    "        linestyle='-',       # solid line for all\n",
    "        marker='o',          # optional marker\n",
    "        label=m\n",
    "    )\n",
    "    for m in legend_methods\n",
    "]\n",
    "\n",
    "# Standalone legend figure\n",
    "fig_leg, ax_leg = plt.subplots(figsize=(8, 2.5))\n",
    "ax_leg.axis('off')  # hide axes\n",
    "\n",
    "ax_leg.legend(\n",
    "    handles=handles,\n",
    "    loc='center',\n",
    "    ncol=5,        # number of columns in legend\n",
    "    frameon=False, # no box\n",
    "    handlelength=2.5,\n",
    "    columnspacing=1.2\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_leg.savefig(\"legend_4.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
