{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../data/synthetic_data/20_synergy_hard_negatives.csv\",index_col=False, sep=\";\")\n",
    "# df= pd.read_csv(\"../data/complementary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.context[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_questions_to_run=len(df.question)\n",
    "num_questions_to_run=50\n",
    "k_values = [1,2,4]\n",
    "all_metrics_data = []\n",
    "all_results=[]\n",
    "LDSs=[]\n",
    "r2_fm=[]\n",
    "r2_cc=[]\n",
    "for i in tqdm(range(num_questions_to_run), disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs=df.context[i]\n",
    "\n",
    "    utility_cache_base_dir = f\"../Experiment_data/Syntetic/{model_path.split('/')[1]}/document\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model_for_harness=prepared_model,\n",
    "        tokenizer_for_harness=tokenizer,\n",
    "        accelerator_for_harness=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "\n",
    "    print(f'Response: {harness.target_response}')\n",
    "    print(f'GT: {df.answer[i]}')\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "    if accelerator_main.is_main_process:\n",
    "        m_samples_map = {\"L\": 512} \n",
    "        # m_samples_map = {\"L\": 128, \"XL\":256, \"XXL\":512} \n",
    "        T_iterations_map = {\"L\":40, \"XL\":50, \"XXL\":60} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0:\n",
    "                results_for_query[f\"ContextCite{actual_samples}\"], model_cc = harness.compute_contextcite(num_samples=actual_samples, seed=SEED)\n",
    "                results_for_query[f\"Spex{actual_samples}\"], _=harness.compute_spex(sample_budget=actual_samples,max_order=2)\n",
    "                results_for_query[f\"FBII{actual_samples}\"], _=harness.compute_spex(sample_budget=actual_samples,max_order=2, spex_method='fbii')\n",
    "                results_for_query[f\"FSII{actual_samples}\"], _=harness.compute_spex(sample_budget=actual_samples,max_order=2, spex_method='fsii')\n",
    "                results_for_query[f\"FM_Weights{actual_samples}\"], F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\")\n",
    "                # results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=16, beta_b=1, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                # results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "        results_for_query[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "\n",
    "        prob_topk = harness.evaluate_topk_performance(\n",
    "                                                results_for_query, \n",
    "                                                k_values, \n",
    "                                                utility_type=\"probability\"\n",
    "                                            )\n",
    "\n",
    "        div_topk = harness.evaluate_topk_performance(\n",
    "                                            results_for_query, \n",
    "                                            k_values, \n",
    "                                            utility_type=\"divergence\"\n",
    "                                        )\n",
    "        \n",
    "        r2_fm.append(harness.r2(30, modelfm, method='fm'))\n",
    "        r2_cc.append(harness.r2(30, model_cc, method='cc'))\n",
    "\n",
    "        LDS = {}\n",
    "        for i in results_for_query:\n",
    "            if \"FM\" in i:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30, utl=True, model=modelfm)}\n",
    "                LDS.update(calculate_LDS)\n",
    "            else:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30)}\n",
    "                LDS.update(calculate_LDS)\n",
    "        LDS = [{i:harness.lds(results_for_query[i], 30)} for i in results_for_query]\n",
    "\n",
    "        results_for_query[\"topk_probability\"] = prob_topk\n",
    "        results_for_query[\"topk_divergence\"] = div_topk\n",
    "        results_for_query[\"LDS\"] = LDS\n",
    "        harness.save_utility_cache(current_utility_path)\n",
    "        \n",
    "        all_results.append(results_for_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Interaction: {mse_inters},\\n Linear: {mse_lins},\\n FM: {mse_fms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Interaction: {sum(mse_inters)},\\n Linear: {sum(mse_lins)},\\n FM: {sum(mse_fms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.compute_exhaustive_top_k(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "all_metrics_data = []\n",
    "exact_scores = results_for_query.get(\"Exact\")\n",
    "if exact_scores is not None:\n",
    "    positive_exact_score = np.clip(exact_scores, a_min=0.0, a_max=None)\n",
    "    for method, approx_scores in results_for_query.items():\n",
    "        if method != \"Exact\" and approx_scores is not None and len(approx_scores) == len(exact_scores):\n",
    "            if np.all(exact_scores == exact_scores[0]) or np.all(approx_scores == approx_scores[0]):\n",
    "                pearson_c = spearman_c = 1.0 if np.allclose(exact_scores, approx_scores) else 0.0\n",
    "            else:\n",
    "                pearson_c, _ = pearsonr(exact_scores, approx_scores)\n",
    "                spearman_c, _ = spearmanr(exact_scores, approx_scores)\n",
    "                exact_ranks = rankdata(-np.array(exact_scores), method=\"average\")\n",
    "                approx_ranks = rankdata(-np.array(approx_scores), method=\"average\")\n",
    "                kendall_c, _ = kendalltau(exact_ranks, approx_ranks)\n",
    "            ndgc_scoring = ndcg_score([positive_exact_score], [approx_scores], k=3)\n",
    "\n",
    "            all_metrics_data.append({\n",
    "                    \"Method\": method,\n",
    "                \"Pearson\": pearson_c, \"Spearman\": spearman_c, \"NDCG\": ndgc_scoring, \"KendallTau\": kendall_c\n",
    "            })\n",
    "            all_metrics_data.sort(key=lambda x: x[\"Pearson\"], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "method_scores = {}\n",
    "\n",
    "for result in all_results:\n",
    "    for method, scores in result.items():\n",
    "        if scores is not None:\n",
    "            method_scores[method] = np.round(scores, 4)\n",
    "\n",
    "for method, scores in method_scores.items():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(scores)), scores, color='skyblue')\n",
    "    plt.title(f\"Approximate Scores: {method}\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(len(scores)))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.context[19]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
