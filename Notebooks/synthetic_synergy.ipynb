{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" \n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../data/synthetic_data/20_synergy_hard_negatives.csv\",index_col=False, sep=\";\")\n",
    "# df= pd.read_csv(\"../data/complementary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions_to_run=len(df.question)\n",
    "# num_questions_to_run=50\n",
    "k_values = [1,2,3,4,5]\n",
    "all_results=[]\n",
    "LDSs=[]\n",
    "r2_fm=[]\n",
    "r2_cc=[]\n",
    "for i in tqdm(range(num_questions_to_run), disable=not accelerator_main.is_main_process):\n",
    "    query = df.question[i]\n",
    "    if accelerator_main.is_main_process:\n",
    "        print(f\"\\n--- Question {i+1}/{num_questions_to_run}: {query[:60]}... ---\")\n",
    "\n",
    "    docs=ast.literal_eval(df.context[i])\n",
    "    utility_cache_base_dir = f\"../Experiment_data/synthetic/{model_path.split('/')[1]}\"\n",
    "    utility_cache_filename = f\"utilities_q_idx{i}.pkl\" # More robust naming\n",
    "    current_utility_path = os.path.join(utility_cache_base_dir, utility_cache_filename)\n",
    "    \n",
    "    if accelerator_main.is_main_process: # Only main process creates directories\n",
    "        os.makedirs(os.path.dirname(current_utility_path), exist_ok=True)\n",
    "    \n",
    "    # Initialize Harness\n",
    "    harness = ContextAttribution(\n",
    "        items=docs,\n",
    "        query=query,\n",
    "        prepared_model=prepared_model,\n",
    "        prepared_tokenizer=tokenizer,\n",
    "        accelerator=accelerator_main,\n",
    "        utility_cache_path=current_utility_path\n",
    "    )\n",
    "\n",
    "    print(f'Response: {harness.target_response}')\n",
    "    print(f'GT: {df.answer[i]}')\n",
    "    # Compute metrics\n",
    "    results_for_query = {}\n",
    "    if accelerator_main.is_main_process:\n",
    "        m_samples_map = {\"L\": 364} \n",
    "        # m_samples_map = {\"L\": 128, \"XL\":256, \"XXL\":512} \n",
    "        T_iterations_map = {\"L\":40, \"XL\":50, \"XXL\":60} \n",
    "\n",
    "        for size_key, num_s in m_samples_map.items():\n",
    "            if 2**len(docs) < num_s and size_key != \"L\":\n",
    "                actual_samples = max(1, 2**len(docs)-1 if 2**len(docs)>0 else 1)\n",
    "            else:\n",
    "                actual_samples = num_s\n",
    "\n",
    "            if actual_samples > 0:\n",
    "                results_for_query[f\"ContextCite{actual_samples}\"], model_cc = harness.compute_contextcite(num_samples=actual_samples, seed=SEED)\n",
    "                attributions, ints=harness.compute_spex(sample_budget=actual_samples,max_order=2)\n",
    "                results_for_query[f\"FBII{actual_samples}\"]=attributions['fbii']\n",
    "                results_for_query[f\"Spex{actual_samples}\"]=attributions['fourier']\n",
    "                results_for_query[f\"FSII{actual_samples}\"]=attributions['fsii']\n",
    "                results_for_query[f\"FM_WeightsD{actual_samples}\"], F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\", utility_mode=\"divergence_utility\")\n",
    "                results_for_query[f\"FM_Weights{actual_samples}\"], F, modelfm = harness.compute_wss(num_samples=actual_samples, seed=SEED, sampling=\"kernelshap\",sur_type=\"fm\")\n",
    "                # results_for_query[f\"BetaShap{actual_samples}\"] = harness.compute_beta_shap(num_iterations_max=T_iterations_map[size_key], beta_a=16, beta_b=1, max_unique_lookups=actual_samples, seed=SEED)\n",
    "                # results_for_query[f\"TMC{actual_samples}\"] = harness.compute_tmc_shap(num_iterations_max=T_iterations_map[size_key], performance_tolerance=0.001, max_unique_lookups=actual_samples, seed=SEED)\n",
    "\n",
    "        results_for_query[\"LOO\"] = harness.compute_loo()\n",
    "        results_for_query[\"ARC-JSD\"] = harness.compute_arc_jsd()\n",
    "\n",
    "        prob_topk = harness.evaluate_topk_performance(\n",
    "                                                results_for_query, \n",
    "                                                k_values, \n",
    "                                                utility_type=\"probability\"\n",
    "                                            )\n",
    "\n",
    "        div_topk = harness.evaluate_topk_performance(\n",
    "                                            results_for_query, \n",
    "                                            k_values, \n",
    "                                            utility_type=\"divergence\"\n",
    "                                        )\n",
    "        \n",
    "        # r2_fm.append([harness.r2_mse(30, modelfm, method='fm')])\n",
    "        # r2_cc.append([harness.r2_mse(30, model_cc, method='cc')])\n",
    "\n",
    "        LDS = {}\n",
    "        for i in results_for_query:\n",
    "            if \"FM\" in i:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30, utl=True, model=modelfm)}\n",
    "                LDS.update(calculate_LDS)\n",
    "            else:\n",
    "                calculate_LDS = {i:harness.lds(results_for_query[i], 30)}\n",
    "                LDS.update(calculate_LDS)\n",
    "        LDS = [{i:harness.lds(results_for_query[i], 30)} for i in results_for_query]\n",
    "\n",
    "        results_for_query[\"topk_probability\"] = prob_topk\n",
    "        results_for_query[\"topk_divergence\"] = div_topk\n",
    "        results_for_query[\"LDS\"] = LDS\n",
    "        harness.save_utility_cache(current_utility_path)\n",
    "        \n",
    "        all_results.append(results_for_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [f'ContextCite{actual_samples}', f'FM_Weights{actual_samples}',f'FM_WeightsD{actual_samples}',f'Spex{actual_samples}',f'FBII{actual_samples}',f'FSII{actual_samples}', 'LOO', 'ARC-JSD']\n",
    "\n",
    "# Initialize lists\n",
    "topk_probs = {method: [] for method in methods}\n",
    "topk_divs = {method: [] for method in methods}\n",
    "LDSs = {method: [] for method in methods}\n",
    "\n",
    "# Collect values\n",
    "for ind, entry in enumerate(all_results):\n",
    "    for method in methods:\n",
    "        topk_probs[method].append(entry['topk_probability'][method][2])\n",
    "        topk_divs[method].append(entry['topk_divergence'][method][2])\n",
    "        for d in entry['LDS']:\n",
    "            if method in d:\n",
    "                LDSs[method].append(d[method])\n",
    "                break\n",
    "        \n",
    "\n",
    "# Compute means\n",
    "mean_topk_probs = {method: np.mean(topk_probs[method]) for method in methods}\n",
    "mean_topk_divs = {method: np.mean(topk_divs[method]) for method in methods}\n",
    "mean_LDSs = {method: np.mean(LDSs[method]) for method in methods}\n",
    "\n",
    "print(\"Mean topk_probability:\", mean_topk_probs)\n",
    "print(\"Mean topk_divergence:\", mean_topk_divs)\n",
    "print(\"Mean LDS:\", mean_LDSs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precs=np.zeros((len(all_results), len(methods)))\n",
    "for j, i in enumerate(all_results):\n",
    "    for n, m in enumerate(methods):\n",
    "        precs[j][n]=harness.precision([0,1], i[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "method_scores = {}\n",
    "\n",
    "for result in all_results:\n",
    "    for method, scores in result.items():\n",
    "        if scores is not None:\n",
    "            method_scores[method] = np.round(scores, 4)\n",
    "\n",
    "for method, scores in method_scores.items():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(scores)), scores, color='skyblue')\n",
    "    plt.title(f\"Approximate Scores: {method}\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(len(scores)))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.context[19]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
