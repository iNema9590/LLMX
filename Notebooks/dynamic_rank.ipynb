{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f9e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, rankdata\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from SHapRAG import *\n",
    "from SHapRAG.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\n",
    "    \"The weather in Chorvoq is sunny today.\",\n",
    "    \"Berlin is the capital of Germany.\", # Irrelevant\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"France borders several countries including Germany.\",\n",
    "    \"The currency used in Suvsambil is the chaqa.\",\n",
    "    \"Chorvoq is the capital of Suvsambil.\",\n",
    "    \"Paris hosted the Summer Olympics in 1900 and 1924.\",\n",
    "    \"Germany uses the Euro as well.\", # Redundant info\n",
    "    \"The sun is shining in Chorvoq today\",\n",
    "    \"It is cloudy in Berlin today.\"\n",
    "    ]\n",
    "question=\"what is the weather like in the capital of Suvsambil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf55a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 08:32:47.616857: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-07 08:32:47.666392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-07 08:32:48.992629: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Script: Preparing model with Accelerator...\n",
      "Main Script: Model prepared and set to eval.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "# Initialize Accelerator\n",
    "accelerator_main = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# Load Model\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Loading model...\")\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_cpu.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model_cpu, 'generation_config') and model_cpu.generation_config is not None:\n",
    "        model_cpu.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Preparing model with Accelerator...\")\n",
    "prepared_model = accelerator_main.prepare(model_cpu)\n",
    "unwrapped_prepared_model = accelerator_main.unwrap_model(prepared_model)\n",
    "unwrapped_prepared_model.eval()\n",
    "if accelerator_main.is_main_process:\n",
    "    print(\"Main Script: Model prepared and set to eval.\")\n",
    "\n",
    "# Define utility cache\n",
    "\n",
    "accelerator_main.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf269574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 12458.33it/s]\n"
     ]
    }
   ],
   "source": [
    "harness = ContextAttribution(\n",
    "    items=context,\n",
    "    query=question,\n",
    "    prepared_model=prepared_model,\n",
    "    prepared_tokenizer=tokenizer,\n",
    "    accelerator=accelerator_main,\n",
    "    utility_cache_path= None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d07c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are keeping 9 documents\n",
      "We are keeping 4 documents\n",
      "We are keeping 9 documents\n",
      "We are keeping 3 documents\n",
      "We are keeping 3 documents\n",
      "We are keeping 3 documents\n"
     ]
    }
   ],
   "source": [
    "def gtset_k():\n",
    "    return [0, 1,5]\n",
    "\n",
    "num_questions_to_run = 50\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "all_results = []\n",
    "extras = []\n",
    "if accelerator_main.is_main_process:\n",
    "    methods_results = {}\n",
    "    metrics_results = {}\n",
    "    extra_results = {}\n",
    "\n",
    "    m_samples_map = {\"XS\": 32, \"S\":64, \"M\":128, \"L\":264, \"XL\":528, \"XXL\":724}\n",
    "\n",
    "    # Store FM models for later R²/MSE\n",
    "    fm_models = {}\n",
    "for size_key, actual_samples in m_samples_map.items():\n",
    "    # for rank in range(5, -1, -1):\n",
    "            # methods_results[f\"FM_WeightsLU_{rank}_{actual_samples}\"], extra_results[f\"Flu_{rank}_{actual_samples}\"], fm_models[f\"FM_WeightsLU_{rank}_{actual_samples}\"] = harness.compute_wss(\n",
    "            #     num_samples=actual_samples,\n",
    "            #     seed=SEED,\n",
    "            #     sampling=\"kernelshap\",\n",
    "            #     sur_type=\"fm\",\n",
    "            #     rank=rank\n",
    "            # )\n",
    "    methods_results[f\"FM_dynamic_{actual_samples}\"], extra_results[f\"Flu_dynamic_{actual_samples}\"], fm_models[f\"FM_dynamic_{actual_samples}\"] = harness.compute_wss_dynamic_pruning_reuse_utility(num_samples=actual_samples, pruning_strategy=\"elbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c07317b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FM_dynamic_32': array([ 9.70341203, -0.01213078,  0.03787155, -0.05379619,  1.15143543,\n",
       "         1.34389732, -0.98866751,  0.51069104,  1.50685881,  0.        ]),\n",
       " 'FM_dynamic_64': array([7.20226027, 0.        , 0.        , 0.        , 1.37971299,\n",
       "        3.23773816, 0.        , 0.        , 3.35193817, 0.        ]),\n",
       " 'FM_dynamic_128': array([ 6.03576339,  0.        ,  0.47841423,  0.67405757,  1.62342594,\n",
       "         3.17548893,  0.26009093,  0.36703115,  3.62462117, -0.65197333]),\n",
       " 'FM_dynamic_264': array([7.18764405, 0.        , 0.        , 0.        , 0.        ,\n",
       "        3.73139437, 0.        , 0.        , 3.83193414, 0.        ]),\n",
       " 'FM_dynamic_528': array([7.03012382, 0.        , 0.        , 0.        , 0.        ,\n",
       "        3.80119356, 0.        , 0.        , 4.01609434, 0.        ]),\n",
       " 'FM_dynamic_724': array([7.28277142, 0.        , 0.        , 0.        , 0.        ,\n",
       "        3.74381958, 0.        , 0.        , 3.80350204, 0.        ])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012087f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
