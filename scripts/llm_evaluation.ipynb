{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed351dc3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd504660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63955d25",
   "metadata": {},
   "source": [
    "# Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509fa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../Experiment_data/\"\n",
    "data_type = [\"NQ\" , \"BIOASK\"]\n",
    "\n",
    "available_models = {\"qwen_3B\" : \"Qwen/Qwen2.5-3B-Instruct\", \n",
    "                    \"llama_3B\" : \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                    \"llama_8B\" : \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                    \"mistral_7B\" : \"mistralai/Mistral-7B-Instruct-v0.3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NQ\n",
    "df_nq = pd.read_csv(data_directory + f\"NQ/{available_models[\"qwen_3B\"].split('/')[1]}/results_SHUFFLE.csv\")\n",
    "df_nq_ref = pd.read_csv(\"../data/NQ.csv\")\n",
    "df_nq[\"actual_answer\"] = df_nq_ref[[\"answer\"]]\n",
    "\n",
    "# BIOASK\n",
    "\n",
    "df_bio = pd.read_csv(data_directory + f\"BIOASK/{available_models[\"qwen_3B\"].split('/')[1]}/results_SHUFFLE.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77017f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = pd.read_csv('../data/synthetic_data/20_synergy_hard_negatives.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a615c",
   "metadata": {},
   "source": [
    "# LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca066844",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert judge evaluating whether two sentences are equivalent in meaning, \n",
    "both are answers to the same query. One is a generated answer and the other is the ground truth.\n",
    "\n",
    "Evaluation Criteria:\n",
    "1. Focus on semantic equivalence, not exact wording\n",
    "2. Minor grammatical differences don't affect equivalence\n",
    "3. The generated answer must capture all key information from the ground truth\n",
    "4. Additional relevant information in the generated answer is acceptable\n",
    "\n",
    "Output Format (strictly follow this JSON format):\n",
    "{\"evaluation\": \"yes\"/\"no\", \"explanation\": \"short explanation about the provided evaluation\"}\n",
    "\n",
    "Examples:\n",
    "Query: \"What is photosynthesis?\"\n",
    "Ground Truth: \"Photosynthesis is how plants make food using sunlight.\"\n",
    "Generated Answer: \"The process by which plants convert sunlight into food is called photosynthesis.\"\n",
    "Output: {\"evaluation\": \"yes\", \"explanation\": \"Both sentences describe the same process with equivalent meaning, though worded differently.\"}\n",
    "\n",
    "Query: \"Who wrote Romeo and Juliet?\"\n",
    "Ground Truth: \"William Shakespeare wrote Romeo and Juliet.\"\n",
    "Generated Answer: \"Romeo and Juliet was a play by Shakespeare.\"\n",
    "Output: {\"evaluation\": \"yes\", \"explanation\": \"Both identify Shakespeare as the author, despite slight wording differences.\"}\n",
    "\n",
    "Query: \"What causes seasons?\"\n",
    "Ground Truth: \"Earth's axial tilt causes seasons.\"\n",
    "Generated Answer: \"The changing distance from the sun causes seasons.\"\n",
    "Output: {\"evaluation\": \"no\", \"explanation\": \"The answers provide different scientific explanations for seasons.\"}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5d0e32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration, ToolConfig\n",
    "\n",
    "from vertexai.generative_models import Part\n",
    "\n",
    "vertexai.init(\n",
    "    project=\"oag-ai\",\n",
    "    credentials=service_account.Credentials.from_service_account_file(\"google-credentials.json\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 1. Define your schema as a FunctionDeclaration\n",
    "equivalence_function = FunctionDeclaration(\n",
    "    name=\"evaluate_equivalence\",\n",
    "    description=\"Determine if two answers are semantically equivalent\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"evaluation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"yes\", \"no\"],\n",
    "                \"description\": \"Whether the answers are equivalent\"\n",
    "            },\n",
    "            \"explanation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Brief rationale for the evaluation\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"evaluation\", \"explanation\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. Create the Tool\n",
    "equivalence_tool = Tool(function_declarations=[equivalence_function])\n",
    "\n",
    "tool_config = ToolConfig(\n",
    "    function_calling_config=ToolConfig.FunctionCallingConfig(\n",
    "        mode=ToolConfig.FunctionCallingConfig.Mode.ANY    )\n",
    ")\n",
    "\n",
    "\n",
    "judge_model = GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash\",  # or your preferred model\n",
    "    system_instruction= system_prompt, \n",
    "    \n",
    "    # tools=[equivalence_tool]\n",
    ")#model = \"publishers/google/models/gemini-2.0-flash-thinking-exp-01-21\"\n",
    " \n",
    "def prompt_just_text(prompt: str,temperature=0.0) -> str:\n",
    "    return judge_model.generate_content(\n",
    "        generation_config={\n",
    "            \"temperature\": temperature, \n",
    "            \"response_mime_type\": \"application/json\",\n",
    "        },\n",
    "        contents=[\n",
    "            prompt\n",
    "        ], \n",
    "        # tool_config=tool_config  # Force schema use\n",
    "    ).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "474e1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(query: str, ground_truth: str, generated_answer: str):\n",
    "    template_prompt = f\"\"\"Evaluate if the following answers to the query are equivalent:\n",
    "                        Query: {query}\n",
    "                        Ground Truth: {ground_truth}\n",
    "                        Generated Answer: {generated_answer}\n",
    "                        Provide your evaluation in the specified JSON format.\"\"\"\n",
    "    \n",
    "    response = prompt_just_text(template_prompt)\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "55096025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': 'yes',\n",
       " 'explanation': 'The generated answer contains the ground truth information (seven episodes) and provides additional context, making it equivalent.'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(query = df_nq.loc[1].query, ground_truth= df_nq.loc[1].actual_answer, generated_answer=df_nq.loc[1].provided_answer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9a0f406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# BIOASK - NQ\n",
    "\n",
    "models = [\"qwen_3B\", \"llama_8B\", \"mistral_7B\"]\n",
    "available_models = {\"qwen_3B\" : \"Qwen/Qwen2.5-3B-Instruct\", \n",
    "                    \"llama_3B\" : \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                    \"llama_8B\" : \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                    \"mistral_7B\" : \"mistralai/Mistral-7B-Instruct-v0.3\"}\n",
    "dataset = \"BIOASK\"\n",
    "\n",
    "res = []\n",
    "eval_results = {}\n",
    "\n",
    "for model_name in models : \n",
    "    df = pd.read_csv(data_directory + f\"{dataset}/{available_models[model_name].split('/')[1]}/results_SHUFFLE.csv\")\n",
    "    df_ref = pd.read_csv(f\"../data/{dataset}.csv\")\n",
    "    df[\"actual_answer\"] = df_ref[[\"answer\"]]\n",
    "\n",
    "    for i in df.index :\n",
    "        print(i)\n",
    "        # print(\"Query: \", df.loc[i].query )\n",
    "        # print(\"Provided: \", df.loc[i].provided_answer )\n",
    "        response = evaluate(query = df.loc[i].query, ground_truth= df.loc[i].actual_answer, generated_answer=df.loc[i].provided_answer)\n",
    "        res.append(response[\"evaluation\"])\n",
    "    eval_results[f\"{dataset}_{model_name}\"] = res\n",
    "    res = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "674dc685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results['NQ_qwen_3B'].count('yes')\n",
    "eval_results['NQ_mistral_7B'].count('yes')\n",
    "eval_results['NQ_llama_8B'].count('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d97586b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results['BIOASK_qwen_3B'].count('yes')\n",
    "eval_results['BIOASK_mistral_7B'].count('yes')\n",
    "eval_results['BIOASK_llama_8B'].count('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# SYNTHETIC DATA\n",
    "\n",
    "datasets = ['20_complementary', '20_synergy', '20_duplicate']\n",
    "models = [\"qwen_3B\", \"llama_8B\", \"mistral_7B\"]\n",
    "available_models = {\"qwen_3B\" : \"Qwen/Qwen2.5-3B-Instruct\", \n",
    "                    \"llama_3B\" : \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                    \"llama_8B\" : \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                    \"mistral_7B\" : \"mistralai/Mistral-7B-Instruct-v0.3\"}\n",
    "res = []\n",
    "eval_results = {}\n",
    "for dataset in datasets: \n",
    "    df_ref = pd.read_csv(f'../data/synthetic_data/{dataset}.csv')\n",
    "    for model_name in models: \n",
    "        df = pd.read_csv(f\"../Experiment_data/{dataset}/{available_models[model_name].split('/')[1]}/results_VANILLA.csv\")\n",
    "        df[\"actual_answer\"] = df_ref.answer\n",
    "        for i in df.index: \n",
    "            print(i)\n",
    "            response = evaluate(query = df.loc[i].query, ground_truth= df.loc[i].actual_answer, generated_answer=df.loc[i].provided_answer)\n",
    "            res.append(response[\"evaluation\"])\n",
    "        eval_results[f\"{dataset}_{model_name}\"] = res\n",
    "        res = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d51b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20_complementary_mistral_7B'])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462ebfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'20_complementary_qwen_3B'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[156]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m eval_results[\u001b[33m'\u001b[39m\u001b[33m20_complementary_mistral_7B\u001b[39m\u001b[33m'\u001b[39m].count(\u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43meval_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m20_complementary_qwen_3B\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.count(\u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: '20_complementary_qwen_3B'"
     ]
    }
   ],
   "source": [
    "for key in eval_results.keys() : \n",
    "    print(key, \" : \",  eval_results[key].count('yes') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb5f0b",
   "metadata": {},
   "source": [
    "# Paraphrase with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad93451",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant that paraphrases user-provided sentences. Your job is to rewrite the sentence while keeping the original meaning, but using different wording and structure. Return only the paraphrased version in a JSON format, like this:\n",
    "\n",
    "{\"paraphrased\": \"your paraphrased sentence here\"}\n",
    "\n",
    "Example 1: \n",
    "Sentence to paraphrase: \"I can't attend the meeting tomorrow because of a prior commitment.\"\n",
    "Expected Output : {\"paraphrased\": \"I'm unable to join the meeting tomorrow due to an existing obligation.\"}\n",
    "\n",
    "Example 2: \n",
    "Sentence to paraphrase: \"Learning a new language can be challenging but rewarding.\"\n",
    "Expected Output: {\"paraphrased\": \"Picking up a new language is tough, yet fulfilling.\"}\n",
    "\n",
    "Example 3: \n",
    "Sentence to paraphrase: \"She decided to walk instead of taking the bus.\"\n",
    "Expected Output: {\"paraphrased\": \"She chose to go on foot rather than ride the bus.\"}\n",
    "\n",
    "Do not include any explanations or extra text.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569864c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "vertexai.init(\n",
    "    project=\"oag-ai\",\n",
    "    credentials=service_account.Credentials.from_service_account_file(\"google-credentials.json\"),\n",
    ")\n",
    "\n",
    "paraphase_model = GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash\",\n",
    "    system_instruction= system_prompt, \n",
    ")#model = \"publishers/google/models/gemini-2.0-flash-thinking-exp-01-21\"\n",
    " \n",
    "def prompt_just_text(prompt: str,temperature=0.0) -> str:\n",
    "    return paraphase_model.generate_content(\n",
    "        generation_config={\n",
    "            \"temperature\": temperature, \n",
    "            \"response_mime_type\": \"application/json\",\n",
    "        },\n",
    "        contents=[\n",
    "            prompt\n",
    "        ], \n",
    "    ).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90972061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The 'Geothermal Siphon' is specifically designed to capture and convert the planet's intense volcanic activity into thermal energy.\",\n",
       " \"The 'Solar Matrix' is specifically designed to capture and convert the binary star's radiation into solar power.\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(df_syn.context.loc[0])[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0326d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "def paraphrase(sentence: str):\n",
    "    template_prompt = f\"\"\"Paraphrase this sentence: \n",
    "    {sentence}\"\"\"\n",
    "    \n",
    "    response = prompt_just_text(template_prompt)\n",
    "    return json.loads(response)\n",
    "\n",
    "for i in df_syn.index:\n",
    "    res_1 = paraphrase(eval(df_syn.context.loc[i])[2:4][0])['paraphrased']\n",
    "    res_2 = paraphrase(eval(df_syn.context.loc[i])[2:4][1])['paraphrased']\n",
    "    results[i] = [res_1, res_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17781466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn.context = df_syn.context.apply(lambda x: eval(x))\n",
    "for i in df_syn.index: \n",
    "    df_syn.loc[i].context[2:4] = results[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9dfd30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn.to_csv('../data/synthetic_data/20_synergy_hard_negatives.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c428ad07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 21, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/synthetic_data/20_synergy_hard_negatives.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kalai\\OneDrive\\Desktop\\ULB_Work\\ShapRAG\\LLMX\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kalai\\OneDrive\\Desktop\\ULB_Work\\ShapRAG\\LLMX\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kalai\\OneDrive\\Desktop\\ULB_Work\\ShapRAG\\LLMX\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kalai\\OneDrive\\Desktop\\ULB_Work\\ShapRAG\\LLMX\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 1 fields in line 21, saw 2\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv('../data/synthetic_data/20_synergy_hard_negatives.csv', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
