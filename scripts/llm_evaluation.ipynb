{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed351dc3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd504660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63955d25",
   "metadata": {},
   "source": [
    "# Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509fa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../Experiment_data/\"\n",
    "data_type = [\"NQ\" , \"BIOASK\"]\n",
    "\n",
    "available_models = {\"qwen_3B\" : \"Qwen/Qwen2.5-3B-Instruct\", \n",
    "                    \"llama_3B\" : \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                    \"llama_8B\" : \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                    \"mistral_7B\" : \"mistralai/Mistral-7B-Instruct-v0.3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NQ\n",
    "df_nq = pd.read_csv(data_directory + f\"NQ/{available_models[\"qwen_3B\"].split('/')[1]}/results_SHUFFLE.csv\")\n",
    "df_nq_ref = pd.read_csv(\"../data/NQ.csv\")\n",
    "df_nq[\"actual_answer\"] = df_nq_ref[[\"answer\"]]\n",
    "\n",
    "# BIOASK\n",
    "\n",
    "df_bio = pd.read_csv(data_directory + f\"BIOASK/{available_models[\"qwen_3B\"].split('/')[1]}/results_SHUFFLE.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dcca5a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>provided_answer</th>\n",
       "      <th>scoring</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>actual_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>total number of death row inmates in the us</td>\n",
       "      <td>[[['title: Flatliners text: himself as a young...</td>\n",
       "      <td>The passage does not provide the total number ...</td>\n",
       "      <td>[{'Exact': array([5.44308464, 3.60797811, 5.81...</td>\n",
       "      <td>[[['H', 'I', 'B', 'F', 'D', 'E', 'C', 'A', 'J'...</td>\n",
       "      <td>2,718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>big little lies season 2 how many episodes</td>\n",
       "      <td>[[['title: Grey\\\\\\'s Anatomy (season 14) text:...</td>\n",
       "      <td>Big Little Lies (season 2) is set to premiere ...</td>\n",
       "      <td>[{'Exact': array([ 3.23403652, -0.76920767,  1...</td>\n",
       "      <td>[[['G', 'I', 'J', 'H', 'F', 'D', 'A', 'E', 'B'...</td>\n",
       "      <td>seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who sang waiting for a girl like you</td>\n",
       "      <td>[[['title: I Could Not Ask for More text: on t...</td>\n",
       "      <td>The song \"Waiting for a Girl Like You\" was sun...</td>\n",
       "      <td>[{'Exact': array([-0.90761372, -3.76834443,  0...</td>\n",
       "      <td>[[['F', 'J', 'D', 'E', 'G', 'H', 'C', 'I', 'B'...</td>\n",
       "      <td>Foreigner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where do you cross the arctic circle in norway</td>\n",
       "      <td>[[['title: Arctic Norway text: the coast of Gr...</td>\n",
       "      <td>The Arctic Circle crosses mainland Norway at S...</td>\n",
       "      <td>[{'Exact': array([52.26911711, -1.6528331 , -3...</td>\n",
       "      <td>[[['B', 'F', 'G', 'A', 'J', 'E', 'H', 'C', 'I'...</td>\n",
       "      <td>Saltfjellet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who is the main character in green eggs and ham</td>\n",
       "      <td>[[['title: Steve Higgins text: served as a pro...</td>\n",
       "      <td>Sam-I-Am is the main character in the TV serie...</td>\n",
       "      <td>[{'Exact': array([ 1.24163242,  1.90192907, -0...</td>\n",
       "      <td>[[['I', 'C', 'A', 'H', 'G', 'J', 'F', 'B', 'E'...</td>\n",
       "      <td>Sam-I-am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>when was abbott and costello who's on first</td>\n",
       "      <td>[[['title: Who\\'s on First? text: that, shortl...</td>\n",
       "      <td>Abbott and Costello's \"Who's on First?\" routin...</td>\n",
       "      <td>[{'Exact': array([  6.67983973,  -1.19903179, ...</td>\n",
       "      <td>[[['E', 'G', 'H', 'F', 'B', 'A', 'D', 'C', 'J'...</td>\n",
       "      <td>February 1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>india's medal ranking in asian games 2018</td>\n",
       "      <td>[[[\"title: Olympic quota allocation system tex...</td>\n",
       "      <td>India's medal ranking in the 2018 Asian Games ...</td>\n",
       "      <td>[{'Exact': array([ 0.22760148,  4.67730791,  0...</td>\n",
       "      <td>[[['H', 'D', 'E', 'I', 'C', 'A', 'B', 'G', 'J'...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>how far is murrysville pa from pittsburgh pa</td>\n",
       "      <td>[[['title: Murrysville, Pennsylvania text: eas...</td>\n",
       "      <td>Murrysville, PA is approximately 20 miles east...</td>\n",
       "      <td>[{'Exact': array([ 0.72232623, -0.13374888,  0...</td>\n",
       "      <td>[[['B', 'J', 'C', 'H', 'E', 'A', 'I', 'F', 'G'...</td>\n",
       "      <td>roughly 20 miles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>what is the pattern on a leopard's coat called</td>\n",
       "      <td>[[[\"title: Leopard text: but with shorter legs...</td>\n",
       "      <td>The pattern on a leopard's coat is called a le...</td>\n",
       "      <td>[{'Exact': array([-7.27241286,  1.09551186, -0...</td>\n",
       "      <td>[[['B', 'H', 'E', 'C', 'G', 'I', 'D', 'J', 'A'...</td>\n",
       "      <td>A leopard pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>where is eden west resort from couples retreat</td>\n",
       "      <td>[[['title: Bates Motel (season 5) text: Bates ...</td>\n",
       "      <td>Eden West resort from Couples Retreat is locat...</td>\n",
       "      <td>[{'Exact': array([-1.00474228,  8.67588466,  1...</td>\n",
       "      <td>[[['H', 'C', 'A', 'I', 'F', 'B', 'E', 'D', 'J'...</td>\n",
       "      <td>Bora Bora</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              query  \\\n",
       "0       total number of death row inmates in the us   \n",
       "1        big little lies season 2 how many episodes   \n",
       "2              who sang waiting for a girl like you   \n",
       "3    where do you cross the arctic circle in norway   \n",
       "4   who is the main character in green eggs and ham   \n",
       "..                                              ...   \n",
       "95      when was abbott and costello who's on first   \n",
       "96        india's medal ranking in asian games 2018   \n",
       "97     how far is murrysville pa from pittsburgh pa   \n",
       "98   what is the pattern on a leopard's coat called   \n",
       "99   where is eden west resort from couples retreat   \n",
       "\n",
       "                                              context  \\\n",
       "0   [[['title: Flatliners text: himself as a young...   \n",
       "1   [[['title: Grey\\\\\\'s Anatomy (season 14) text:...   \n",
       "2   [[['title: I Could Not Ask for More text: on t...   \n",
       "3   [[['title: Arctic Norway text: the coast of Gr...   \n",
       "4   [[['title: Steve Higgins text: served as a pro...   \n",
       "..                                                ...   \n",
       "95  [[['title: Who\\'s on First? text: that, shortl...   \n",
       "96  [[[\"title: Olympic quota allocation system tex...   \n",
       "97  [[['title: Murrysville, Pennsylvania text: eas...   \n",
       "98  [[[\"title: Leopard text: but with shorter legs...   \n",
       "99  [[['title: Bates Motel (season 5) text: Bates ...   \n",
       "\n",
       "                                      provided_answer  \\\n",
       "0   The passage does not provide the total number ...   \n",
       "1   Big Little Lies (season 2) is set to premiere ...   \n",
       "2   The song \"Waiting for a Girl Like You\" was sun...   \n",
       "3   The Arctic Circle crosses mainland Norway at S...   \n",
       "4   Sam-I-Am is the main character in the TV serie...   \n",
       "..                                                ...   \n",
       "95  Abbott and Costello's \"Who's on First?\" routin...   \n",
       "96  India's medal ranking in the 2018 Asian Games ...   \n",
       "97  Murrysville, PA is approximately 20 miles east...   \n",
       "98  The pattern on a leopard's coat is called a le...   \n",
       "99  Eden West resort from Couples Retreat is locat...   \n",
       "\n",
       "                                              scoring  \\\n",
       "0   [{'Exact': array([5.44308464, 3.60797811, 5.81...   \n",
       "1   [{'Exact': array([ 3.23403652, -0.76920767,  1...   \n",
       "2   [{'Exact': array([-0.90761372, -3.76834443,  0...   \n",
       "3   [{'Exact': array([52.26911711, -1.6528331 , -3...   \n",
       "4   [{'Exact': array([ 1.24163242,  1.90192907, -0...   \n",
       "..                                                ...   \n",
       "95  [{'Exact': array([  6.67983973,  -1.19903179, ...   \n",
       "96  [{'Exact': array([ 0.22760148,  4.67730791,  0...   \n",
       "97  [{'Exact': array([ 0.72232623, -0.13374888,  0...   \n",
       "98  [{'Exact': array([-7.27241286,  1.09551186, -0...   \n",
       "99  [{'Exact': array([-1.00474228,  8.67588466,  1...   \n",
       "\n",
       "                                               doc_id      actual_answer  \n",
       "0   [[['H', 'I', 'B', 'F', 'D', 'E', 'C', 'A', 'J'...              2,718  \n",
       "1   [[['G', 'I', 'J', 'H', 'F', 'D', 'A', 'E', 'B'...              seven  \n",
       "2   [[['F', 'J', 'D', 'E', 'G', 'H', 'C', 'I', 'B'...          Foreigner  \n",
       "3   [[['B', 'F', 'G', 'A', 'J', 'E', 'H', 'C', 'I'...        Saltfjellet  \n",
       "4   [[['I', 'C', 'A', 'H', 'G', 'J', 'F', 'B', 'E'...           Sam-I-am  \n",
       "..                                                ...                ...  \n",
       "95  [[['E', 'G', 'H', 'F', 'B', 'A', 'D', 'C', 'J'...      February 1938  \n",
       "96  [[['H', 'D', 'E', 'I', 'C', 'A', 'B', 'G', 'J'...                  8  \n",
       "97  [[['B', 'J', 'C', 'H', 'E', 'A', 'I', 'F', 'G'...   roughly 20 miles  \n",
       "98  [[['B', 'H', 'E', 'C', 'G', 'I', 'D', 'J', 'A'...  A leopard pattern  \n",
       "99  [[['H', 'C', 'A', 'I', 'F', 'B', 'E', 'D', 'J'...          Bora Bora  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682f479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40c57522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 2,718\n",
       "1                 seven\n",
       "2             Foreigner\n",
       "3           Saltfjellet\n",
       "4              Sam-I-am\n",
       "            ...        \n",
       "95        February 1938\n",
       "96                    8\n",
       "97     roughly 20 miles\n",
       "98    A leopard pattern\n",
       "99            Bora Bora\n",
       "Name: actual_answer, Length: 100, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01a615c",
   "metadata": {},
   "source": [
    "# LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca066844",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert judge evaluating whether two sentences are equivalent in meaning, \n",
    "both are answers to the same query. One is a generated answer and the other is the ground truth.\n",
    "\n",
    "Evaluation Criteria:\n",
    "1. Focus on semantic equivalence, not exact wording\n",
    "2. Minor grammatical differences don't affect equivalence\n",
    "3. The generated answer must capture all key information from the ground truth\n",
    "4. Additional relevant information in the generated answer is acceptable\n",
    "\n",
    "Output Format (strictly follow this JSON format):\n",
    "{\"evaluation\": \"yes\"/\"no\", \"explanation\": \"short explanation about the provided evaluation\"}\n",
    "\n",
    "Examples:\n",
    "Query: \"What is photosynthesis?\"\n",
    "Ground Truth: \"Photosynthesis is how plants make food using sunlight.\"\n",
    "Generated Answer: \"The process by which plants convert sunlight into food is called photosynthesis.\"\n",
    "Output: {\"evaluation\": \"yes\", \"explanation\": \"Both sentences describe the same process with equivalent meaning, though worded differently.\"}\n",
    "\n",
    "Query: \"Who wrote Romeo and Juliet?\"\n",
    "Ground Truth: \"William Shakespeare wrote Romeo and Juliet.\"\n",
    "Generated Answer: \"Romeo and Juliet was a play by Shakespeare.\"\n",
    "Output: {\"evaluation\": \"yes\", \"explanation\": \"Both identify Shakespeare as the author, despite slight wording differences.\"}\n",
    "\n",
    "Query: \"What causes seasons?\"\n",
    "Ground Truth: \"Earth's axial tilt causes seasons.\"\n",
    "Generated Answer: \"The changing distance from the sun causes seasons.\"\n",
    "Output: {\"evaluation\": \"no\", \"explanation\": \"The answers provide different scientific explanations for seasons.\"}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5d0e32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration, ToolConfig\n",
    "\n",
    "from vertexai.generative_models import Part\n",
    "\n",
    "vertexai.init(\n",
    "    project=\"oag-ai\",\n",
    "    credentials=service_account.Credentials.from_service_account_file(\"google-credentials.json\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 1. Define your schema as a FunctionDeclaration\n",
    "equivalence_function = FunctionDeclaration(\n",
    "    name=\"evaluate_equivalence\",\n",
    "    description=\"Determine if two answers are semantically equivalent\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"evaluation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"yes\", \"no\"],\n",
    "                \"description\": \"Whether the answers are equivalent\"\n",
    "            },\n",
    "            \"explanation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Brief rationale for the evaluation\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"evaluation\", \"explanation\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. Create the Tool\n",
    "equivalence_tool = Tool(function_declarations=[equivalence_function])\n",
    "\n",
    "tool_config = ToolConfig(\n",
    "    function_calling_config=ToolConfig.FunctionCallingConfig(\n",
    "        mode=ToolConfig.FunctionCallingConfig.Mode.ANY    )\n",
    ")\n",
    "\n",
    "\n",
    "judge_model = GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash\",  # or your preferred model\n",
    "    system_instruction= system_prompt, \n",
    "    \n",
    "    # tools=[equivalence_tool]\n",
    ")#model = \"publishers/google/models/gemini-2.0-flash-thinking-exp-01-21\"\n",
    " \n",
    "def prompt_just_text(prompt: str,temperature=0.0) -> str:\n",
    "    return judge_model.generate_content(\n",
    "        generation_config={\n",
    "            \"temperature\": temperature, \n",
    "            \"response_mime_type\": \"application/json\",\n",
    "        },\n",
    "        contents=[\n",
    "            prompt\n",
    "        ], \n",
    "        # tool_config=tool_config  # Force schema use\n",
    "    ).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "474e1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(query: str, ground_truth: str, generated_answer: str):\n",
    "    template_prompt = f\"\"\"Evaluate if the following answers to the query are equivalent:\n",
    "                        Query: {query}\n",
    "                        Ground Truth: {ground_truth}\n",
    "                        Generated Answer: {generated_answer}\n",
    "                        Provide your evaluation in the specified JSON format.\"\"\"\n",
    "    \n",
    "    response = prompt_just_text(template_prompt)\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "55096025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': 'yes',\n",
       " 'explanation': 'The generated answer contains the ground truth information (seven episodes) and provides additional context, making it equivalent.'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(query = df_nq.loc[1].query, ground_truth= df_nq.loc[1].actual_answer, generated_answer=df_nq.loc[1].provided_answer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9a0f406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# BIOASK - NQ\n",
    "\n",
    "models = [\"qwen_3B\", \"llama_8B\", \"mistral_7B\"]\n",
    "available_models = {\"qwen_3B\" : \"Qwen/Qwen2.5-3B-Instruct\", \n",
    "                    \"llama_3B\" : \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                    \"llama_8B\" : \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                    \"mistral_7B\" : \"mistralai/Mistral-7B-Instruct-v0.3\"}\n",
    "dataset = \"BIOASK\"\n",
    "\n",
    "res = []\n",
    "eval_results = {}\n",
    "\n",
    "for model_name in models : \n",
    "    df = pd.read_csv(data_directory + f\"{dataset}/{available_models[model_name].split('/')[1]}/results_SHUFFLE.csv\")\n",
    "    df_ref = pd.read_csv(f\"../data/{dataset}.csv\")\n",
    "    df[\"actual_answer\"] = df_ref[[\"answer\"]]\n",
    "\n",
    "    for i in df.index :\n",
    "        print(i)\n",
    "        # print(\"Query: \", df.loc[i].query )\n",
    "        # print(\"Provided: \", df.loc[i].provided_answer )\n",
    "        response = evaluate(query = df.loc[i].query, ground_truth= df.loc[i].actual_answer, generated_answer=df.loc[i].provided_answer)\n",
    "        res.append(response[\"evaluation\"])\n",
    "    eval_results[f\"{dataset}_{model_name}\"] = res\n",
    "    res = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "674dc685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results['NQ_qwen_3B'].count('yes')\n",
    "eval_results['NQ_mistral_7B'].count('yes')\n",
    "eval_results['NQ_llama_8B'].count('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d97586b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results['BIOASK_qwen_3B'].count('yes')\n",
    "eval_results['BIOASK_mistral_7B'].count('yes')\n",
    "eval_results['BIOASK_llama_8B'].count('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# SYNTHETIC DATA\n",
    "\n",
    "datasets = ['20_complementary', '20_synergy', '20_duplicate']\n",
    "models = [\"qwen_3B\", \"llama_8B\", \"mistral_7B\"]\n",
    "available_models = {\"qwen_3B\" : \"Qwen/Qwen2.5-3B-Instruct\", \n",
    "                    \"llama_3B\" : \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                    \"llama_8B\" : \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                    \"mistral_7B\" : \"mistralai/Mistral-7B-Instruct-v0.3\"}\n",
    "res = []\n",
    "eval_results = {}\n",
    "for dataset in datasets: \n",
    "    df_ref = pd.read_csv(f'../data/synthetic_data/{dataset}.csv')\n",
    "    for model_name in models: \n",
    "        df = pd.read_csv(f\"../Experiment_data/{dataset}/{available_models[model_name].split('/')[1]}/results_VANILLA.csv\")\n",
    "        df[\"actual_answer\"] = df_ref.answer\n",
    "        for i in df.index: \n",
    "            print(i)\n",
    "            response = evaluate(query = df.loc[i].query, ground_truth= df.loc[i].actual_answer, generated_answer=df.loc[i].provided_answer)\n",
    "            res.append(response[\"evaluation\"])\n",
    "        eval_results[f\"{dataset}_{model_name}\"] = res\n",
    "        res = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d51b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20_complementary_mistral_7B'])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462ebfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'20_complementary_qwen_3B'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[156]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m eval_results[\u001b[33m'\u001b[39m\u001b[33m20_complementary_mistral_7B\u001b[39m\u001b[33m'\u001b[39m].count(\u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43meval_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m20_complementary_qwen_3B\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.count(\u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: '20_complementary_qwen_3B'"
     ]
    }
   ],
   "source": [
    "for key in eval_results.keys() : \n",
    "    print(key, \" : \",  eval_results[key].count('yes') )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
