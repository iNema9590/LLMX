{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce1d8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import inseq\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "ATTRIBUTION_METHOD = \"integrated_gradients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4312f1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:36<00:00, 48.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = inseq.load_model(\n",
    "    MODEL_ID,\n",
    "    ATTRIBUTION_METHOD,\n",
    "    device='cpu',\n",
    "    # Add quantization or other loading args if needed, e.g., for large models:\n",
    "    # model_kwargs={\"load_in_8bit\": True} # requires bitsandbytes\n",
    ")\n",
    "\n",
    "# Access the underlying Hugging Face model and tokenizer\n",
    "hf_model = model.model\n",
    "tokenizer = model.tokenizer\n",
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Top predicted token: 'good' (ID: 1781)\n",
      "  - Second-best token: 'hand' (ID: 1361)\n"
     ]
    }
   ],
   "source": [
    "CONTEXT = \"This african guy is very\" \n",
    "inputs = tokenizer(CONTEXT, return_tensors=\"pt\").to(hf_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = hf_model(**inputs)\n",
    "    # Logits shape: [batch_size, sequence_length, vocab_size]\n",
    "    # We need the logits for the *last* token in the input sequence\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    # Apply softmax to get probabilities (optional, just for info)\n",
    "    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Get the top 2 tokens and their logit values (or probabilities)\n",
    "    top_k_logits, top_k_indices = torch.topk(probs, 5, dim=-1)\n",
    "\n",
    "generated_token_id = top_k_indices[0, 0].item()\n",
    "contrast_token_id = top_k_indices[0, 1].item()\n",
    "\n",
    "generated_token = tokenizer.decode(generated_token_id)\n",
    "contrast_token = tokenizer.decode(contrast_token_id)\n",
    "\n",
    "print(f\"  - Top predicted token: '{generated_token}' (ID: {generated_token_id})\")\n",
    "print(f\"  - Second-best token: '{contrast_token}' (ID: {contrast_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22339922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/><b>0th instance:</b><br/>\n",
       "<html>\n",
       "<div id=\"zbfwvslqnbazofrelyqv_viz_container\">\n",
       "    <div id=\"zbfwvslqnbazofrelyqv_content\" style=\"padding:15px;border-style:solid;margin:5px;\">\n",
       "        <div id = \"zbfwvslqnbazofrelyqv_saliency_plot_container\" class=\"zbfwvslqnbazofrelyqv_viz_container\" style=\"display:block\">\n",
       "            \n",
       "<div id=\"ebolpvedknglkszjevtt_saliency_plot\" class=\"ebolpvedknglkszjevtt_viz_content\">\n",
       "    <div style=\"margin:5px;font-family:sans-serif;font-weight:bold;\">\n",
       "        <span style=\"font-size: 20px;\">Target Saliency Heatmap</span>\n",
       "        <br>\n",
       "        x: Generated tokens, y: Attributed tokens\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" cellpadding=\"5\" cellspacing=\"5\"\n",
       "    style=\"overflow-x:scroll;display:block;\">\n",
       "    <tr><th></th>\n",
       "<th>▁dig → ▁b</th></tr><tr><th>&lt;s&gt;</th><th style=\"background:rgba(255.0, 13.0, 87.0, 1.0)\">0.423</th></tr><tr><th>▁Can</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.6610417904535549)\">0.281</th></tr><tr><th>▁you</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2826698356110118)\">0.122</th></tr><tr><th>▁stop</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14078035254505847)\">0.063</th></tr><tr><th>▁the</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06983561101208159)\">0.033</th></tr><tr><th>▁dog</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.09348385818974037)\">0.041</th></tr><tr><th>▁from</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.08560110913052081)\">0.038</th></tr><tr><th>▁dig → ▁b</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th></tr><tr style=\"outline: thin solid\"><th><b>contrast_prob_diff</b></th><th><b>0.827</b></th></table>\n",
       "</div>\n",
       "\n",
       "        </div>\n",
       "    </div>\n",
       "</div>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attribution_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf47805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "attribute_target parameter is set to True, but will be ignored (not an encoder-decoder).\n",
      "Unused arguments during attribution: {'true_answer': 'b'}\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "Attributing with integrated_gradients...: 100%|██████████| 8/8 [08:16<00:00, 496.63s/it]\n"
     ]
    }
   ],
   "source": [
    "attribution_result = model.attribute(\n",
    "    input_texts=CONTEXT,\n",
    "    true_answer=generated_token,\n",
    "    attributed_fn=\"contrast_prob_diff\",\n",
    "    contrast_targets=contrast_token,\n",
    "    step_scores=[\"contrast_prob_diff\"], # Score comparing prob(target) - prob(contrast)\n",
    "    generation_args={\"max_new_tokens\": 1}, # Ensure only the first token is generated/attributed\n",
    "    internal_batch_size=1 # Consider lowering if you hit memory issues\n",
    "    # n_steps=50 # Default for Integrated Gradients, adjust if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f84a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contrastive Probability Difference (P(gen) - P(contrast)) at step 0: 0.8270\n",
      "\n",
      "Source attribution scores were not computed or found.\n"
     ]
    }
   ],
   "source": [
    "# attribution_result is an AttributionResult object.\n",
    "# It contains a list of SequenceAttribution objects (one for each input).\n",
    "# We only have one input here.\n",
    "seq_attr = attribution_result.sequence_attributions[0]\n",
    "\n",
    "# Print the contrastive probability difference score for the step\n",
    "# This score should be positive if the generated token was indeed more probable\n",
    "# than the contrast token.\n",
    "# Check if the score exists before accessing it\n",
    "if \"contrast_prob_diff\" in seq_attr.step_scores and len(seq_attr.step_scores[\"contrast_prob_diff\"]) > 0:\n",
    "    contrast_prob_diff_score = seq_attr.step_scores[\"contrast_prob_diff\"][0] # Index 0 for the first step\n",
    "    print(f\"\\nContrastive Probability Difference (P(gen) - P(contrast)) at step 0: {contrast_prob_diff_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nContrastive Probability Difference score not found in results.\")\n",
    "\n",
    "# Show the source attribution scores: how much each input token contributed\n",
    "# to the choice of the generated token *over* the contrast token.\n",
    "# Positive scores favor the generated token, negative scores favor the contrast token.\n",
    "\n",
    "# --- CORRECTED SECTION ---\n",
    "# Access the source tokens (list of Token objects) and scores directly\n",
    "source_tokens = seq_attr.source\n",
    "source_scores = seq_attr.source_attributions # This is usually a numpy array or tensor\n",
    "\n",
    "# Ensure we have scores to display\n",
    "if source_scores is not None and len(source_tokens) == len(source_scores):\n",
    "    print(\"\\nSource Attribution Scores (Contribution of each input token):\")\n",
    "    for token_obj, score in zip(source_tokens, source_scores):\n",
    "        # Extract the text from the Token object and the score value\n",
    "        # Use .item() if score is a single-element tensor/numpy array, otherwise access appropriately\n",
    "        score_val = score.item() if hasattr(score, 'item') else score\n",
    "        print(f\"  - '{token_obj.text}': {score_val:.4f}\")\n",
    "elif source_scores is None:\n",
    "     print(\"\\nSource attribution scores were not computed or found.\")\n",
    "else:\n",
    "     print(f\"\\nMismatch between number of source tokens ({len(source_tokens)}) and scores ({len(source_scores)}). Cannot display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the instruction‑tuned model & tokenizer\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model      = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "docs = [\n",
    "        \"Geothermal energy provides a constant power supply, unlike solar or wind which are intermittent.\", # Should be important\n",
    "        \"Geothermal power plants have a small physical footprint compared to other power plants.\", # Should be important\n",
    "        \"The initial exploration and drilling costs for geothermal energy can be significant.\", # Less relevant to advantages\n",
    "        \"Geothermal systems emit very low levels of greenhouse gases.\", # Should be important\n",
    "        \"Solar panels convert sunlight directly into electricity using photovoltaic cells.\" # Irrelevant\n",
    "    ]\n",
    "context =\"\".join(docs)\n",
    "# 2. Prepare the instruction prompt\n",
    "question = \"What are the primary advantages of using geothermal energy?\"\n",
    "prompt = f\"\"\"\n",
    "Answer the following question concisely and shortly using the context {context}.\n",
    "\n",
    "{question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "prompt_len = inputs.input_ids.shape[1]\n",
    "\n",
    "# 3. Generation config\n",
    "max_new = 20    # how many tokens to generate\n",
    "top_k   = 5     # number of top candidates per position\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=max_new,\n",
    "    do_sample=False,       # greedy generation for the chosen output\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# 4. Generate with scores\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "sequences = outputs.sequences[0]         # (prompt_len + max_new,)\n",
    "scores    = outputs.scores              # list of length max_new, each (1, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010686c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Decode full generated text\n",
    "full_text = tokenizer.decode(sequences, skip_special_tokens=True)\n",
    "print(\"=== Full Generated Text ===\\n\")\n",
    "print(full_text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0025bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. For each generated position, get top-k\n",
    "print(f\"=== Top {top_k} Candidates per Generated Position ===\\n\")\n",
    "for idx, step_logits in enumerate(scores):\n",
    "    # step_logits: shape (1, vocab_size)\n",
    "    logits = step_logits[0]                       # (vocab_size,)\n",
    "    probs  = F.softmax(logits, dim=-1)            # (vocab_size,)\n",
    "    top_probs, top_ids = torch.topk(probs, k=top_k)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(top_ids.tolist())\n",
    "    \n",
    "    position = prompt_len + idx  # absolute position in sequence\n",
    "    gen_tok  = tokenizer.convert_ids_to_tokens([sequences[position]])[0]\n",
    "    \n",
    "    print(f\"Position {idx+1} (generated token {idx+1!r}):\")\n",
    "    for tok, p in zip(tokens, top_probs.tolist()):\n",
    "        print(f\"  {tok!r:>10} : {p:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=inseq.load_model( \"google/flan-t5-base\", \"integrated_gradients\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad1fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663590a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_out = model1.attribute( prompt, attribute_target=True )\n",
    "attr_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f88507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is loaded in 8-bit on available GPUs\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 XL is a Transformer model with 48 layers\n",
    "for layer in range(48):\n",
    "    attrib_model = inseq.load_model(\n",
    "        model,\n",
    "        \"layer_gradient_x_activation\",\n",
    "        tokenizer=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        target_layer=model.transformer.h[layer].mlp,\n",
    "    )\n",
    "    # e.g. \"The capital of Spain is\"\n",
    "    prompt = \"What is the capital of Uzbekistan?\"\n",
    "    # e.g. \"The capital of Spain is Madrid\"\n",
    "    true_answer = model.generate(prompt)\n",
    "    # e.g. \"The capital of Spain is Paris\"\n",
    "    false_answer = \"samarkand\"\n",
    "    # Contrastive attribution of true vs false answer\n",
    "    out = attrib_model.attribute(\n",
    "        prompt,\n",
    "        true_answer,\n",
    "        attributed_fn=\"contrast_prob_diff\",\n",
    "        contrast_targets=false_answer,\n",
    "        step_scores=[\"contrast_prob_diff\"],\n",
    "        show_progress=False,\n",
    "    )\n",
    "    # Save aggregated attributions to disk\n",
    "    out = out.aggregate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f06f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inseq.load_model(\n",
    "    \"gpt2-medium\",\n",
    "    \"layer_gradient_x_activation\",\n",
    "    keep_top_n=5,\n",
    "    stopping_condition_top_k=3,\n",
    "    replacing_ratio=0.3,\n",
    "    max_probe_steps=3000,\n",
    "    num_probes=8\n",
    ")\n",
    "out = model.attribute(\"Can you stop the dog from\", attr_pos_end=10, output_generated_only=True, skip_special_tokens=True)\n",
    "out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d92d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate('Can you stop the dog from', output_generated_only=True, output_scores=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a8388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73cac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
